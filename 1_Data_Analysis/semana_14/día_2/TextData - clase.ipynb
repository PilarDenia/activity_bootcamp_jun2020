{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26271,
     "status": "ok",
     "timestamp": 1600932020606,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "1FahTQE7mP6B",
    "outputId": "4c8acd28-b10c-487a-dac1-0750ab5ca06e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2O0nELPy1Gy"
   },
   "source": [
    "#Text Analysis\n",
    "-----\n",
    "\n",
    "## Text Data: Flattening, Filtering, and Chunking\n",
    "\n",
    "What would you do if you were designing an algorithm to analyze the following paragraph of text?\n",
    "\n",
    "    Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “Hello?”\n",
    "\n",
    "The paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n",
    "\n",
    "So, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main characters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n",
    "\n",
    "This chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling technique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2TIF9lMy1Gz"
   },
   "source": [
    "## Bag-of-X: Turning Natural Text into Flat Vectors\n",
    "\n",
    "Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and interpretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n",
    "For text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This technique can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indicator of the topic content of the document.\n",
    "\n",
    "Bag-of-Words\n",
    "\n",
    "In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corresponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZG2s2_51y1G0"
   },
   "source": [
    "<img src=\"imagenes/imagen1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gplms5HDy1G0"
   },
   "source": [
    "Bag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words representation, these words are all equal elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "568KZ9p6y1G1"
   },
   "source": [
    "<img src=\"imagenes/imagen2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F-3oF-ty1G2"
   },
   "source": [
    "What is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n",
    "\n",
    "<img src=\"imagenes/imagen3.png\">\n",
    "\n",
    "Figure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n",
    "\n",
    "<img src=\"imagenes/imagen4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYHN-3OTy1G2"
   },
   "source": [
    "These figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at feature vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vectors. Figure 3-5 shows an example. With bag-of-words featurization for text documents, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.”  As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix transpose of the bag-of-words vectors.\n",
    "\n",
    "<img src=\"imagenes/imagen5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUR9ybpTy1G3"
   },
   "source": [
    "Bag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating negation plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n",
    "\n",
    "## Bag-of-n-Grams\n",
    "\n",
    "Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n",
    "\n",
    "n-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n",
    "\n",
    "To illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26256,
     "status": "ok",
     "timestamp": 1600932020607,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_g2co9boy1G4"
   },
   "outputs": [],
   "source": [
    "# importamos librerías\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34020,
     "status": "ok",
     "timestamp": 1600932028382,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "5oElc6KLy1G7",
    "outputId": "7f1535d5-8d44-4c22-d053-64338e17af6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209393, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "biz_f = open('yelp_academic_dataset_business.json', encoding='latin1')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "biz_df.shape\n",
    "\n",
    "\n",
    "# biz_df = pd.read_csv('biz.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34005,
     "status": "ok",
     "timestamp": 1600932028383,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "KM-_yICVy1G-",
    "outputId": "48f148fa-8b92-44cf-8623-333d02487ea3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "      <th>attributes</th>\n",
       "      <th>categories</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f9NumwFMBDn751xgFiRbNA</td>\n",
       "      <td>The Range At Lake Norman</td>\n",
       "      <td>10913 Bailey Rd</td>\n",
       "      <td>Cornelius</td>\n",
       "      <td>NC</td>\n",
       "      <td>28031</td>\n",
       "      <td>35.462724</td>\n",
       "      <td>-80.852612</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True', 'BikePa...</td>\n",
       "      <td>Active Life, Gun/Rifle Ranges, Guns &amp; Ammo, Sh...</td>\n",
       "      <td>{'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yzvjg0SayhoZgCljUJRF9Q</td>\n",
       "      <td>Carlos Santo, NMD</td>\n",
       "      <td>8880 E Via Linda, Ste 107</td>\n",
       "      <td>Scottsdale</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85258</td>\n",
       "      <td>33.569404</td>\n",
       "      <td>-111.890264</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'GoodForKids': 'True', 'ByAppointmentOnly': '...</td>\n",
       "      <td>Health &amp; Medical, Fitness &amp; Instruction, Yoga,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XNoUzKckATkOD1hP6vghZg</td>\n",
       "      <td>Felinus</td>\n",
       "      <td>3554 Rue Notre-Dame O</td>\n",
       "      <td>Montreal</td>\n",
       "      <td>QC</td>\n",
       "      <td>H4C 1P4</td>\n",
       "      <td>45.479984</td>\n",
       "      <td>-73.580070</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Pets, Pet Services, Pet Groomers</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6OAZjbxqM5ol29BuHsil3w</td>\n",
       "      <td>Nevada House of Hose</td>\n",
       "      <td>1015 Sharp Cir</td>\n",
       "      <td>North Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89030</td>\n",
       "      <td>36.219728</td>\n",
       "      <td>-115.127725</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True', 'ByAppo...</td>\n",
       "      <td>Hardware Stores, Home Services, Building Suppl...</td>\n",
       "      <td>{'Monday': '7:0-16:0', 'Tuesday': '7:0-16:0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51M2Kk903DFYI6gnB5I6SQ</td>\n",
       "      <td>USE MY GUY SERVICES LLC</td>\n",
       "      <td>4827 E Downing Cir</td>\n",
       "      <td>Mesa</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85205</td>\n",
       "      <td>33.428065</td>\n",
       "      <td>-111.726648</td>\n",
       "      <td>4.5</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': 'True', 'ByAppo...</td>\n",
       "      <td>Home Services, Plumbing, Electricians, Handyma...</td>\n",
       "      <td>{'Monday': '0:0-0:0', 'Tuesday': '9:0-16:0', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                      name  \\\n",
       "0  f9NumwFMBDn751xgFiRbNA  The Range At Lake Norman   \n",
       "1  Yzvjg0SayhoZgCljUJRF9Q         Carlos Santo, NMD   \n",
       "2  XNoUzKckATkOD1hP6vghZg                   Felinus   \n",
       "3  6OAZjbxqM5ol29BuHsil3w      Nevada House of Hose   \n",
       "4  51M2Kk903DFYI6gnB5I6SQ   USE MY GUY SERVICES LLC   \n",
       "\n",
       "                     address             city state postal_code   latitude  \\\n",
       "0            10913 Bailey Rd        Cornelius    NC       28031  35.462724   \n",
       "1  8880 E Via Linda, Ste 107       Scottsdale    AZ       85258  33.569404   \n",
       "2      3554 Rue Notre-Dame O         Montreal    QC     H4C 1P4  45.479984   \n",
       "3             1015 Sharp Cir  North Las Vegas    NV       89030  36.219728   \n",
       "4         4827 E Downing Cir             Mesa    AZ       85205  33.428065   \n",
       "\n",
       "    longitude  stars  review_count  is_open  \\\n",
       "0  -80.852612    3.5            36        1   \n",
       "1 -111.890264    5.0             4        1   \n",
       "2  -73.580070    5.0             5        1   \n",
       "3 -115.127725    2.5             3        0   \n",
       "4 -111.726648    4.5            26        1   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  {'BusinessAcceptsCreditCards': 'True', 'BikePa...   \n",
       "1  {'GoodForKids': 'True', 'ByAppointmentOnly': '...   \n",
       "2                                               None   \n",
       "3  {'BusinessAcceptsCreditCards': 'True', 'ByAppo...   \n",
       "4  {'BusinessAcceptsCreditCards': 'True', 'ByAppo...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Active Life, Gun/Rifle Ranges, Guns & Ammo, Sh...   \n",
       "1  Health & Medical, Fitness & Instruction, Yoga,...   \n",
       "2                   Pets, Pet Services, Pet Groomers   \n",
       "3  Hardware Stores, Home Services, Building Suppl...   \n",
       "4  Home Services, Plumbing, Electricians, Handyma...   \n",
       "\n",
       "                                               hours  \n",
       "0  {'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3  {'Monday': '7:0-16:0', 'Tuesday': '7:0-16:0', ...  \n",
       "4  {'Monday': '0:0-0:0', 'Tuesday': '9:0-16:0', '...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35363,
     "status": "ok",
     "timestamp": 1600932029756,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Rf6MS0iOy1HA",
    "outputId": "11ac8ceb-d6e8-4685-9166-c0311cf28577"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the first 10,000 reviews\n",
    "\n",
    "\n",
    "# f = open('drive/My Drive/Colab Notebooks/yelp_academic_dataset_review.json', encoding='latin1')\n",
    "f = open('yelp_academic_dataset_review.json', encoding='latin1')\n",
    "js = []\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "\n",
    "\n",
    "# review_df = pd.read_csv('review.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35344,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "2xZN2wy1y1HD",
    "outputId": "7c508d93-870b-4847-ca41-49b4f2e0872a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>PxLgGV56Hw4txlG6gKvapQ</td>\n",
       "      <td>1IyGLnESYghXsScyn3ltNA</td>\n",
       "      <td>z2JTN5PXemCRGtGbKiOvZw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I came here to get a pedicure. Worst experienc...</td>\n",
       "      <td>2015-10-07 23:45:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3ykur79WxV27JxgIeaUyEw</td>\n",
       "      <td>IivADm5_qbGHYJvxPDDdLA</td>\n",
       "      <td>Jt28TYWanzKrJYYr0Tf1MQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Excellent service and excellent food! We ate b...</td>\n",
       "      <td>2017-06-17 18:25:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>q7vcvqY434k_Rw3X0eEk_g</td>\n",
       "      <td>5-U7VLg1OtAXLGY57npZLQ</td>\n",
       "      <td>90oH6tilpqsCkz7Dhcxejw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In line with and one of the best massages I ha...</td>\n",
       "      <td>2018-05-08 18:48:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>hd6KRCmVtFqwGluq3D3Nsw</td>\n",
       "      <td>9TNITWe9p-7qCdyAC0u5Nw</td>\n",
       "      <td>9IdnNV6Rq1ddFyWMdyAKrQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Virginia has been our vet for 6 years. She has...</td>\n",
       "      <td>2014-09-25 16:57:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>l6pSPefEqPaFSABqncNOCw</td>\n",
       "      <td>XChCfeJ6Yx2NDJIpIgRhyg</td>\n",
       "      <td>Ehq7wmTyxVdTJkv_MoRqmg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If the $5.99 lunch special proves to be as awe...</td>\n",
       "      <td>2012-05-29 01:19:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   review_id                 user_id             business_id  \\\n",
       "0     xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ  -MhfebM0QIsKt87iDN-FNw   \n",
       "1     UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ  lbrU8StCq3yDfr-QMnGrmQ   \n",
       "2     LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q  HQl28KMwrEKHqhFrrDqVNQ   \n",
       "3     i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ  5JxlZaqCnk1MnbgRirs40Q   \n",
       "4     6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg  IS4cv902ykd8wj1TR0N3-A   \n",
       "...                      ...                     ...                     ...   \n",
       "9995  PxLgGV56Hw4txlG6gKvapQ  1IyGLnESYghXsScyn3ltNA  z2JTN5PXemCRGtGbKiOvZw   \n",
       "9996  3ykur79WxV27JxgIeaUyEw  IivADm5_qbGHYJvxPDDdLA  Jt28TYWanzKrJYYr0Tf1MQ   \n",
       "9997  q7vcvqY434k_Rw3X0eEk_g  5-U7VLg1OtAXLGY57npZLQ  90oH6tilpqsCkz7Dhcxejw   \n",
       "9998  hd6KRCmVtFqwGluq3D3Nsw  9TNITWe9p-7qCdyAC0u5Nw  9IdnNV6Rq1ddFyWMdyAKrQ   \n",
       "9999  l6pSPefEqPaFSABqncNOCw  XChCfeJ6Yx2NDJIpIgRhyg  Ehq7wmTyxVdTJkv_MoRqmg   \n",
       "\n",
       "      stars  useful  funny  cool  \\\n",
       "0       2.0       5      0     0   \n",
       "1       1.0       1      1     0   \n",
       "2       5.0       1      0     0   \n",
       "3       1.0       0      0     0   \n",
       "4       4.0       0      0     0   \n",
       "...     ...     ...    ...   ...   \n",
       "9995    1.0       1      0     0   \n",
       "9996    5.0       0      0     0   \n",
       "9997    5.0       0      0     0   \n",
       "9998    5.0       1      0     1   \n",
       "9999    4.0       0      0     0   \n",
       "\n",
       "                                                   text                 date  \n",
       "0     As someone who has worked with many museums, I...  2015-04-15 05:21:16  \n",
       "1     I am actually horrified this place is still in...  2013-12-07 03:16:52  \n",
       "2     I love Deagan's. I do. I really do. The atmosp...  2015-12-05 03:18:11  \n",
       "3     Dismal, lukewarm, defrosted-tasting \"TexMex\" g...  2011-05-27 05:30:52  \n",
       "4     Oh happy day, finally have a Canes near my cas...  2017-01-14 21:56:57  \n",
       "...                                                 ...                  ...  \n",
       "9995  I came here to get a pedicure. Worst experienc...  2015-10-07 23:45:05  \n",
       "9996  Excellent service and excellent food! We ate b...  2017-06-17 18:25:56  \n",
       "9997  In line with and one of the best massages I ha...  2018-05-08 18:48:16  \n",
       "9998  Virginia has been our vet for 6 years. She has...  2014-09-25 16:57:16  \n",
       "9999  If the $5.99 lunch special proves to be as awe...  2012-05-29 01:19:21  \n",
       "\n",
       "[10000 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35328,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "07mKeA-ry1HF",
    "outputId": "6c443116-065e-41b5-c576-0693f0caccff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas. When I saw they would be showing infamous eggs of the House of Faberge from the Virginia Museum of Fine Arts (VMFA), I knew I had to go!\\n\\nTucked away near the gelateria and the garden, the Gallery is pretty much hidden from view. It\\'s what real estate agents would call \"cozy\" or \"charming\" - basically any euphemism for small.\\n\\nThat being said, you can still see wonderful art at a gallery of any size, so why the two *s you ask? Let me tell you:\\n\\n* pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top. For the space and the amount of art you can fit in there, it is a bit much.\\n* it\\'s not kid friendly at all. Seriously, don\\'t bring them.\\n* the security is not trained properly for the show. When the curating and design teams collaborate for exhibitions, there is a definite flow. That means visitors should view the art in a certain sequence, whether it be by historical period or cultural significance (this is how audio guides are usually developed). When I arrived in the gallery I could not tell where to start, and security was certainly not helpful. I was told to \"just look around\" and \"do whatever.\" \\n\\nAt such a *fine* institution, I find the lack of knowledge and respect for the art appalling.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35698,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yFLbM0jry1HI"
   },
   "outputs": [],
   "source": [
    "# uso CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35688,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "937zuVELy1HK"
   },
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "\n",
    "# expresión regular\n",
    "# (?u) es unicode\n",
    "# \\bword\\b\n",
    "# w+ es 'one or more word characters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36593,
     "status": "ok",
     "timestamp": 1600932031068,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "1DX1OjyHy1HM",
    "outputId": "3e98fd9f-0235-4b62-96f3-c854432de8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x26227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 716565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = bow_converter.fit_transform(review_df['text'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36571,
     "status": "ok",
     "timestamp": 1600932031068,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "u_3Xg2O7y1HO",
    "outputId": "babc1b92-d0b5-4ad6-a5d9-893af15baff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26227"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = bow_converter.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36557,
     "status": "ok",
     "timestamp": 1600932031069,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZLDYBINRy1HR",
    "outputId": "3b70182c-0734-4f44-ba58-5b05390b19b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '00', '000', '00a', '00am', '00p', '00pm', '00service', '01', '0100pm']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]\n",
    "# los 10 primeros elementos de la lista, de 0 a 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39682,
     "status": "ok",
     "timestamp": 1600932034204,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rZZTnmvwy1HT"
   },
   "outputs": [],
   "source": [
    "bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern= '(?u)\\\\b\\\\w+\\\\b')\n",
    "x2 = bigram_converter.fit_transform(review_df['text'])\n",
    "# tomamos las palabras solo de 2 en 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39672,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "VEfqqsFny1HV",
    "outputId": "6aabf05f-1d4f-4223-e487-af03ad0cda52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311530"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = bigram_converter.get_feature_names()\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39655,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "R2Rkf4H2y1HX",
    "outputId": "3a1ea27e-5649-49a9-b5b1-9c1aaf6c000f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00am appointment',\n",
       " '00am in',\n",
       " '00am on',\n",
       " '00am who',\n",
       " '00am you',\n",
       " '00p almost',\n",
       " '00p we',\n",
       " '00pm 1',\n",
       " '00pm 10',\n",
       " '00pm and']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[135:145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45154,
     "status": "ok",
     "timestamp": 1600932039720,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "tymksgvRy1HZ"
   },
   "outputs": [],
   "source": [
    "trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern= '(?u)\\\\b\\\\w+\\\\b')\n",
    "x3 = trigram_converter.fit_transform(review_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46275,
     "status": "ok",
     "timestamp": 1600932040853,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "6MwUtblry1Hb",
    "outputId": "58d0f375-5068-4e77-e9e4-c0ebdb9abb15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726525"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = trigram_converter.get_feature_names()\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46259,
     "status": "ok",
     "timestamp": 1600932040854,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_v--lpqsy1He",
    "outputId": "ecfa30c9-feeb-4ccf-ed7f-dc8fd8fb9e85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 00 balance',\n",
       " '0 03 i',\n",
       " '0 3 this',\n",
       " '0 35 ea',\n",
       " '0 5 for',\n",
       " '0 50 difference',\n",
       " '0 50 extra',\n",
       " '0 50 or',\n",
       " '0 75 but',\n",
       " '0 99 charge']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46245,
     "status": "ok",
     "timestamp": 1600932040855,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "lU98Vv6py1Hg",
    "outputId": "59be51c1-0e87-4796-8d24-32bb5551ede8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26227 311530 726525\n"
     ]
    }
   ],
   "source": [
    "print(len(words), len(bigrams), len(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46230,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "uMTx-NPXk35m",
    "outputId": "4e3d4afb-e365-4ce6-a6fd-71684060187f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'apple': 1, 'day': 3, 'keeps': 5, 'the': 6, 'doctor': 4, 'away': 2}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "\n",
    "ejemplo_n_gramas = CountVectorizer(ngram_range=(1,1))\n",
    "print(ejemplo_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)\n",
    "# por defecto, la expresión regular toma 2 caracteres mínimo\n",
    "# devuelve en el valor la posición en orden alfabético\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46215,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DbGhd75Xk35t",
    "outputId": "80a7b7da-8902-4d97-81a2-3ed6f531b13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'apple': 2, 'day': 5, 'keeps': 9, 'the': 11, 'doctor': 7, 'away': 4, 'an apple': 1, 'apple day': 3, 'day keeps': 6, 'keeps the': 10, 'the doctor': 12, 'doctor away': 8}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "\n",
    "ejemplo_n_gramas = CountVectorizer(ngram_range=(1,2))\n",
    "print(ejemplo_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46198,
     "status": "ok",
     "timestamp": 1600932040857,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_IgcngOmk35y",
    "outputId": "7e17edd7-95d5-4456-bf86-a7b9bc16640a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'apple': 3, 'day': 7, 'keeps': 12, 'the': 15, 'doctor': 10, 'away': 6, 'an apple': 1, 'apple day': 4, 'day keeps': 8, 'keeps the': 13, 'the doctor': 16, 'doctor away': 11, 'an apple day': 2, 'apple day keeps': 5, 'day keeps the': 9, 'keeps the doctor': 14, 'the doctor away': 17}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "ejemplo_n_gramas = CountVectorizer(ngram_range=(1,3))\n",
    "print(ejemplo_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46584,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DXeh7ui_y1Hi",
    "outputId": "c500d115-a727-4789-a252-ed3822d0567e"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas.util.testing as tm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46568,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "EWo__Xoxy1Hk",
    "outputId": "15920c9f-3b35-4a62-a957-6c793f923054"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "counts = [len(words), len(bigrams), len(trigrams)]\n",
    "plt.plot(counts, color='cornflowerblue')\n",
    "plt.plot(counts, 'bo')\n",
    "plt.margins(0.1)\n",
    "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Number of ngrams in the first 10,000 reviews of the Yelp dataset', {'fontsize':16})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSEanGNOy1Hl"
   },
   "source": [
    "## Filtering for Cleaner Features\n",
    "\n",
    "With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n",
    "\n",
    "The popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47445,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Qq7BEgiXy1Hm",
    "outputId": "66490902-4a28-44f9-9617-d8382f4393f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alberto.Romero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04kSK7ISy1Ho"
   },
   "source": [
    "Note that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n",
    "\n",
    "## Frequency-Based Filtering\n",
    "\n",
    "Stopword lists are a way of weeding out common words that make for vacuous features. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use frequency statistics here as well.\n",
    "\n",
    "### Frequent words\n",
    "\n",
    "Frequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n",
    "\n",
    "Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, frequency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also contains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization.\n",
    "\n",
    "<img src=\"imagenes/imagen6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgLLbbshy1Ho"
   },
   "source": [
    "In practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no universal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n",
    "\n",
    "### Rare words\n",
    "\n",
    "Depending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful information. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar? Even if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledygook.”\n",
    "\n",
    "Not only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (tokenized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n",
    "\n",
    "Rare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further filtered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n",
    "\n",
    "<img src=\"imagenes/imagen7.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeLaC3qZy1Ho"
   },
   "source": [
    "Since one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n",
    "\n",
    "Since this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely contains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n",
    "\n",
    "\n",
    "## Stemming\n",
    "\n",
    "One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically different tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n",
    "\n",
    "Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tagging and linguistic rules in a process known as lemmatization.\n",
    "\n",
    "Most stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n",
    "\n",
    "Here is an example of running the Porter stemmer through the NLTK Python package. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:\n",
    "\n",
    "<img src=\"imagenes/imagen8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgSOAY6Py1Hp"
   },
   "source": [
    "Stemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n",
    "\n",
    "\n",
    "## Atoms of Meaning: From Words to n-Grams to Phrases\n",
    "\n",
    "The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n",
    "\n",
    "### Parsing and Tokenization\n",
    "\n",
    "Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n",
    "\n",
    "After light parsing, the plain-text portion of the document can go through tokenization. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what characters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n",
    "\n",
    "Sometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n",
    "\n",
    "### Collocation Extraction for Phrase Detection\n",
    "\n",
    "\n",
    "A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things.”\n",
    "\n",
    "Collocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a collocation.\n",
    "\n",
    "Collocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n",
    "\n",
    "Because collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a representation. Bag-of-n-grams is also problematic because it captures too many meaningless sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n",
    "\n",
    "Collocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n",
    "\n",
    "Since the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n",
    "\n",
    "### Frequency-based methods\n",
    "\n",
    "A simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams ( n = 2 ) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n",
    "\n",
    "<img src=\"imagenes/imagen9.png\">\n",
    "\n",
    "### Hypothesis testing for collocation extraction\n",
    "\n",
    "Raw popularity count is too crude of a measure. We have to find more clever statistics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n",
    "\n",
    "Hypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The randomness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n",
    "\n",
    "For example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p-Values.\n",
    "\n",
    "In the context of collocation extraction, many hypothesis tests have been proposed over the years.  One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following question: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n",
    "\n",
    "That is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n",
    "\n",
    "We can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n",
    "\n",
    "The final statistic is the log of the ratio between the two:\n",
    "\n",
    "log λ = log L ( Data; H null ) L ( Data; H alternate ) . #formatear\n",
    "\n",
    "The likelihood function L(Data; H) represents the probability of seeing the word frequencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strategy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probability.\n",
    "\n",
    "The algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n",
    "\n",
    "    Compute occurrence probabilities for all singleton words: P(w).\n",
    "    Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n",
    "    Compute the likelihood ratio log λ for all unique bigrams.\n",
    "    Sort the bigrams based on their likelihood ratio.\n",
    "    Take the bigrams with the smallest likelihood ratio values as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKpifucIy1Hp"
   },
   "source": [
    "There is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text corpora. Hence, it is not commonly used and we will not be demonstrating it here.\n",
    "\n",
    "Note that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filtering. To generate longer phrases, there are other methods such as chunking or combining with part-of-speech (PoS) tagging.\n",
    "\n",
    "\n",
    "### Chunking and part-of-speech tagging\n",
    "\n",
    "Chunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n",
    "\n",
    "For example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple language models available. \n",
    "\n",
    "To  illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47428,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ubFxtO5Yy1Hp",
    "outputId": "3c63dd4f-383c-496b-baaf-1a5054ea189d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the first 10 reviews\n",
    "\n",
    "f = open('yelp_academic_dataset_review.json')\n",
    "js = []\n",
    "for i in range(10):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "\n",
    "# review_df = pd.read_csv('review_10.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "he56IUUfy1Hs"
   },
   "source": [
    "### Using spacy: [Installation instructions for spacy](https://spacy.io/docs/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47418,
     "status": "ok",
     "timestamp": 1600932042154,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Ss9_u5wfy1Hs"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48198,
     "status": "ok",
     "timestamp": 1600932042940,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "z5h0B-wek36Y"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49349,
     "status": "ok",
     "timestamp": 1600932044096,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZhJPJ7rPk36d"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49340,
     "status": "ok",
     "timestamp": 1600932044097,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rdXZbLgMy1Hu",
    "outputId": "eec8243f-5570-494b-c72b-3484ced876d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "===================== Info about model 'en_core_web_sm' =====================\u001b[0m\n",
      "\n",
      "lang             en                            \n",
      "name             core_web_sm                   \n",
      "license          MIT                           \n",
      "author           Explosion                     \n",
      "url              https://explosion.ai          \n",
      "email            contact@explosion.ai          \n",
      "description      English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\n",
      "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}]\n",
      "pipeline         ['tagger', 'parser', 'ner']   \n",
      "version          2.3.1                         \n",
      "spacy_version    >=2.3.0,<2.4.0                \n",
      "parent_package   spacy                         \n",
      "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
      "source           C:\\Users\\Alberto.Romero\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lang': 'en',\n",
       " 'name': 'core_web_sm',\n",
       " 'license': 'MIT',\n",
       " 'author': 'Explosion',\n",
       " 'url': 'https://explosion.ai',\n",
       " 'email': 'contact@explosion.ai',\n",
       " 'description': 'English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.',\n",
       " 'sources': [{'name': 'OntoNotes 5',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC2013T19',\n",
       "   'license': 'commercial (licensed by Explosion)'}],\n",
       " 'pipeline': ['tagger', 'parser', 'ner'],\n",
       " 'version': '2.3.1',\n",
       " 'spacy_version': '>=2.3.0,<2.4.0',\n",
       " 'parent_package': 'spacy',\n",
       " 'accuracy': {'las': 89.7572754092,\n",
       "  'uas': 91.6570115569,\n",
       "  'token_acc': 99.756964111,\n",
       "  'las_per_type': {'advmod': {'p': 85.6065101297,\n",
       "    'r': 84.9512113055,\n",
       "    'f': 85.2776018577},\n",
       "   'aux': {'p': 97.9464841319, 'r': 98.0772654442, 'f': 98.0118311613},\n",
       "   'nsubj': {'p': 95.530627567, 'r': 94.7522887555, 'f': 95.1398662913},\n",
       "   'root': {'p': 89.5162856958, 'r': 91.1692936754, 'f': 90.3352283866},\n",
       "   'compound': {'p': 90.4871122761, 'r': 92.2811316552, 'f': 91.3753170839},\n",
       "   'poss': {'p': 97.0346623923, 'r': 97.4838969404, 'f': 97.2587609198},\n",
       "   'case': {'p': 97.927972373, 'r': 99.3493493493, 'f': 98.6335403727},\n",
       "   'dobj': {'p': 92.4513496547, 'r': 93.8729981675, 'f': 93.1567503459},\n",
       "   'prep': {'p': 85.6642170718, 'r': 86.2427438631, 'f': 85.9525069954},\n",
       "   'pobj': {'p': 96.0694769711, 'r': 96.6428459243, 'f': 96.3553084873},\n",
       "   'relcl': {'p': 76.5768958186, 'r': 78.3538796229, 'f': 77.4551971326},\n",
       "   'det': {'p': 97.7105145232, 'r': 97.7901904024, 'f': 97.7503362269},\n",
       "   'amod': {'p': 91.5748754262, 'r': 90.4891480402, 'f': 91.0287743996},\n",
       "   'attr': {'p': 90.4294478528, 'r': 92.9772918419, 'f': 91.6856728177},\n",
       "   'cc': {'p': 83.8244137102, 'r': 83.3532647692, 'f': 83.5881753313},\n",
       "   'mark': {'p': 90.3421052632, 'r': 90.9644939057, 'f': 90.6522313177},\n",
       "   'nmod': {'p': 76.3772954925, 'r': 55.7586837294, 'f': 64.4593166608},\n",
       "   'conj': {'p': 76.8877867328, 'r': 78.0490874764, 'f': 77.4640849469},\n",
       "   'advcl': {'p': 68.9203354298, 'r': 66.2301687232, 'f': 67.548478233},\n",
       "   'pcomp': {'p': 85.2515506547, 'r': 86.6246498599, 'f': 85.9326154915},\n",
       "   'nummod': {'p': 93.1951089846, 'r': 88.5353535354, 'f': 90.8054908055},\n",
       "   'nsubjpass': {'p': 92.4265842349, 'r': 92.0, 'f': 92.2127987664},\n",
       "   'quantmod': {'p': 85.3463587922, 'r': 78.0666125102, 'f': 81.5443360204},\n",
       "   'auxpass': {'p': 94.7204968944, 'r': 97.2665148064, 'f': 95.9766239604},\n",
       "   'ccomp': {'p': 79.9260844194, 'r': 83.6863543788, 'f': 81.7630086559},\n",
       "   'npadvmod': {'p': 76.8636539204, 'r': 70.6927175844, 'f': 73.6491487787},\n",
       "   'appos': {'p': 70.0960219479, 'r': 66.5075921909, 'f': 68.2546749777},\n",
       "   'neg': {'p': 94.5082376435, 'r': 94.9824385349, 'f': 94.7447447447},\n",
       "   'xcomp': {'p': 88.2854100106, 'r': 89.2677674085, 'f': 88.7738711405},\n",
       "   'predet': {'p': 85.2459016393, 'r': 89.2703862661, 'f': 87.2117400419},\n",
       "   'acomp': {'p': 90.3553299492, 'r': 88.717716357, 'f': 89.529035208},\n",
       "   'acl': {'p': 75.6578947368, 'r': 69.012547736, 'f': 72.182596291},\n",
       "   'oprd': {'p': 81.0810810811, 'r': 71.6417910448, 'f': 76.0697305864},\n",
       "   'dative': {'p': 73.9659367397, 'r': 69.7247706422, 'f': 71.7827626919},\n",
       "   'agent': {'p': 88.5328836425, 'r': 94.0860215054, 'f': 91.2250217202},\n",
       "   'meta': {'p': 94.7368421053, 'r': 34.615384615400004, 'f': 50.7042253521},\n",
       "   'dep': {'p': 40.329218107, 'r': 15.9090909091, 'f': 22.8172293364},\n",
       "   'prt': {'p': 81.9166666667, 'r': 88.082437276, 'f': 84.8877374784},\n",
       "   'expl': {'p': 98.3014861996, 'r': 99.1434689507, 'f': 98.7206823028},\n",
       "   'parataxis': {'p': 63.9240506329, 'r': 43.8177874187, 'f': 51.9948519949},\n",
       "   'intj': {'p': 69.387755102, 'r': 59.7802197802, 'f': 64.2266824085},\n",
       "   'csubj': {'p': 70.5882352941, 'r': 71.0059171598, 'f': 70.796460177},\n",
       "   'preconj': {'p': 57.4468085106, 'r': 62.7906976744, 'f': 60.0},\n",
       "   'csubjpass': {'p': 44.4444444444, 'r': 66.6666666667, 'f': 53.3333333333}},\n",
       "  'tags_acc': 97.056555292,\n",
       "  'ents_f': 85.4306864065,\n",
       "  'ents_p': 85.7239322492,\n",
       "  'ents_r': 85.1394400045,\n",
       "  'ents_per_type': {'ORG': {'p': 83.3194096352,\n",
       "    'r': 82.8808864266,\n",
       "    'f': 83.0995695042},\n",
       "   'CARDINAL': {'p': 83.9554682384, 'r': 86.32996633, 'f': 85.1261620186},\n",
       "   'DATE': {'p': 84.6522781775, 'r': 86.1275705821, 'f': 85.3835521769},\n",
       "   'GPE': {'p': 92.5831202046, 'r': 90.2180685358, 'f': 91.3852950458},\n",
       "   'PERSON': {'p': 88.0239520958, 'r': 92.0908379013, 'f': 90.0114810563},\n",
       "   'MONEY': {'p': 92.9181929182, 'r': 91.4663461538, 'f': 92.1865536039},\n",
       "   'PRODUCT': {'p': 52.1276595745, 'r': 24.1379310345, 'f': 32.9966329966},\n",
       "   'TIME': {'p': 70.1886792453, 'r': 70.9923664122, 'f': 70.5882352941},\n",
       "   'PERCENT': {'p': 91.8566775244, 'r': 88.125, 'f': 89.95215311},\n",
       "   'WORK_OF_ART': {'p': 48.1481481481, 'r': 38.8059701493, 'f': 42.9752066116},\n",
       "   'QUANTITY': {'p': 78.1954887218, 'r': 65.4088050314, 'f': 71.2328767123},\n",
       "   'NORP': {'p': 88.682581786, 'r': 89.2348754448, 'f': 88.9578713969},\n",
       "   'LOC': {'p': 70.8154506438, 'r': 66.0, 'f': 68.3229813665},\n",
       "   'EVENT': {'p': 63.2911392405, 'r': 37.037037037, 'f': 46.7289719626},\n",
       "   'ORDINAL': {'p': 80.0, 'r': 83.9694656489, 'f': 81.9366852886},\n",
       "   'FAC': {'p': 34.8623853211, 'r': 46.9135802469, 'f': 40.0},\n",
       "   'LAW': {'p': 62.962962963, 'r': 56.6666666667, 'f': 59.649122807},\n",
       "   'LANGUAGE': {'p': 75.0, 'r': 65.2173913043, 'f': 69.7674418605}}},\n",
       " 'speed': {'cpu': 6107.3535050376, 'gpu': None, 'nwords': 291315},\n",
       " 'labels': {'tagger': ['$',\n",
       "   \"''\",\n",
       "   ',',\n",
       "   '-LRB-',\n",
       "   '-RRB-',\n",
       "   '.',\n",
       "   ':',\n",
       "   'ADD',\n",
       "   'AFX',\n",
       "   'CC',\n",
       "   'CD',\n",
       "   'DT',\n",
       "   'EX',\n",
       "   'FW',\n",
       "   'HYPH',\n",
       "   'IN',\n",
       "   'JJ',\n",
       "   'JJR',\n",
       "   'JJS',\n",
       "   'LS',\n",
       "   'MD',\n",
       "   'NFP',\n",
       "   'NN',\n",
       "   'NNP',\n",
       "   'NNPS',\n",
       "   'NNS',\n",
       "   'PDT',\n",
       "   'POS',\n",
       "   'PRP',\n",
       "   'PRP$',\n",
       "   'RB',\n",
       "   'RBR',\n",
       "   'RBS',\n",
       "   'RP',\n",
       "   'SYM',\n",
       "   'TO',\n",
       "   'UH',\n",
       "   'VB',\n",
       "   'VBD',\n",
       "   'VBG',\n",
       "   'VBN',\n",
       "   'VBP',\n",
       "   'VBZ',\n",
       "   'WDT',\n",
       "   'WP',\n",
       "   'WP$',\n",
       "   'WRB',\n",
       "   'XX',\n",
       "   '_SP',\n",
       "   '``'],\n",
       "  'parser': ['ROOT',\n",
       "   'acl',\n",
       "   'acomp',\n",
       "   'advcl',\n",
       "   'advmod',\n",
       "   'agent',\n",
       "   'amod',\n",
       "   'appos',\n",
       "   'attr',\n",
       "   'aux',\n",
       "   'auxpass',\n",
       "   'case',\n",
       "   'cc',\n",
       "   'ccomp',\n",
       "   'compound',\n",
       "   'conj',\n",
       "   'csubj',\n",
       "   'csubjpass',\n",
       "   'dative',\n",
       "   'dep',\n",
       "   'det',\n",
       "   'dobj',\n",
       "   'expl',\n",
       "   'intj',\n",
       "   'mark',\n",
       "   'meta',\n",
       "   'neg',\n",
       "   'nmod',\n",
       "   'npadvmod',\n",
       "   'nsubj',\n",
       "   'nsubjpass',\n",
       "   'nummod',\n",
       "   'oprd',\n",
       "   'parataxis',\n",
       "   'pcomp',\n",
       "   'pobj',\n",
       "   'poss',\n",
       "   'preconj',\n",
       "   'predet',\n",
       "   'prep',\n",
       "   'prt',\n",
       "   'punct',\n",
       "   'quantmod',\n",
       "   'relcl',\n",
       "   'xcomp'],\n",
       "  'ner': ['CARDINAL',\n",
       "   'DATE',\n",
       "   'EVENT',\n",
       "   'FAC',\n",
       "   'GPE',\n",
       "   'LANGUAGE',\n",
       "   'LAW',\n",
       "   'LOC',\n",
       "   'MONEY',\n",
       "   'NORP',\n",
       "   'ORDINAL',\n",
       "   'ORG',\n",
       "   'PERCENT',\n",
       "   'PERSON',\n",
       "   'PRODUCT',\n",
       "   'QUANTITY',\n",
       "   'TIME',\n",
       "   'WORK_OF_ART']},\n",
       " 'source': 'C:\\\\Users\\\\Alberto.Romero\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\site-packages\\\\en_core_web_sm'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model meta data\n",
    "spacy.info('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49325,
     "status": "ok",
     "timestamp": 1600932044102,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Crcw9hW2y1Hy",
    "outputId": "80f605e5-5c7a-4fa5-ec1a-fe7cecb0e062"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping it in a pandas dataframe\n",
    "doc_df = review_df['text'].apply(nlp)\n",
    "\n",
    "type(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49693,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ptcmeIO-y1Hz",
    "outputId": "ccd56103-25c4-4066-897c-81f4bdfaddee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49681,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "zBOS4XxBk362",
    "outputId": "819e7428-794f-4a51-ec54-cfa7826227d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Oh happy day, finally have a Canes near my casa. Yes just as others are griping about the Drive thru is packed just like most of the other canes in the area but I like to go sit down to enjoy my chicken. The cashiers are pleasant and as far as food wise i have yet to receive any funky chicken. The clean up crew zips around the dining area constantly so it's usually well kept. My only gripe is the one fella with Red hair he makes the rounds while cleaning but no smile or personality a few nights ago he tossed the napkins i just put on the table to help go with my meal. After I was done he just reached for my tray no \"excuse me or are you done with that?\"  I realize he's trying to do his job quickly but a little table manners goes along way. That being said still like to grub here and glad that there's finally a Cane's close to me."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49670,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yBh2DQmny1H4",
    "outputId": "1cfd36cd-2b67-45be-d7e5-ed339147013d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh INTJ UH\n",
      "happy ADJ JJ\n",
      "day NOUN NN\n",
      ", PUNCT ,\n",
      "finally ADV RB\n",
      "have AUX VBP\n",
      "a DET DT\n",
      "Canes PROPN NNPS\n",
      "near SCONJ IN\n",
      "my DET PRP$\n",
      "casa NOUN NN\n",
      ". PUNCT .\n",
      "Yes INTJ UH\n",
      "just ADV RB\n",
      "as SCONJ IN\n",
      "others NOUN NNS\n",
      "are AUX VBP\n",
      "griping VERB VBG\n",
      "about ADP IN\n",
      "the DET DT\n",
      "Drive NOUN NN\n",
      "thru NOUN NN\n",
      "is AUX VBZ\n",
      "packed VERB VBN\n",
      "just ADV RB\n",
      "like SCONJ IN\n",
      "most ADJ JJS\n",
      "of ADP IN\n",
      "the DET DT\n",
      "other ADJ JJ\n",
      "canes NOUN NNS\n",
      "in ADP IN\n",
      "the DET DT\n",
      "area NOUN NN\n",
      "but CCONJ CC\n",
      "I PRON PRP\n",
      "like VERB VBP\n",
      "to PART TO\n",
      "go VERB VB\n",
      "sit VERB VB\n",
      "down ADP RP\n",
      "to PART TO\n",
      "enjoy VERB VB\n",
      "my DET PRP$\n",
      "chicken NOUN NN\n",
      ". PUNCT .\n",
      "The DET DT\n",
      "cashiers NOUN NNS\n",
      "are AUX VBP\n",
      "pleasant ADJ JJ\n",
      "and CCONJ CC\n",
      "as ADV RB\n",
      "far ADV RB\n",
      "as SCONJ IN\n",
      "food NOUN NN\n",
      "wise ADJ JJ\n",
      "i PRON PRP\n",
      "have AUX VBP\n",
      "yet ADV RB\n",
      "to PART TO\n",
      "receive VERB VB\n",
      "any DET DT\n",
      "funky ADJ JJ\n",
      "chicken NOUN NN\n",
      ". PUNCT .\n",
      "The DET DT\n",
      "clean ADJ JJ\n",
      "up ADP RP\n",
      "crew NOUN NN\n",
      "zips NOUN NNS\n",
      "around ADP IN\n",
      "the DET DT\n",
      "dining NOUN NN\n",
      "area NOUN NN\n",
      "constantly ADV RB\n",
      "so SCONJ IN\n",
      "it PRON PRP\n",
      "'s AUX VBZ\n",
      "usually ADV RB\n",
      "well ADV RB\n",
      "kept VERB VBN\n",
      ". PUNCT .\n",
      "My DET PRP$\n",
      "only ADJ JJ\n",
      "gripe NOUN NN\n",
      "is AUX VBZ\n",
      "the DET DT\n",
      "one NUM CD\n",
      "fella NOUN NN\n",
      "with ADP IN\n",
      "Red ADJ JJ\n",
      "hair NOUN NN\n",
      "he PRON PRP\n",
      "makes VERB VBZ\n",
      "the DET DT\n",
      "rounds NOUN NNS\n",
      "while SCONJ IN\n",
      "cleaning VERB VBG\n",
      "but CCONJ CC\n",
      "no DET DT\n",
      "smile NOUN NN\n",
      "or CCONJ CC\n",
      "personality NOUN NN\n",
      "a DET DT\n",
      "few ADJ JJ\n",
      "nights NOUN NNS\n",
      "ago ADV RB\n",
      "he PRON PRP\n",
      "tossed VERB VBD\n",
      "the DET DT\n",
      "napkins NOUN NNS\n",
      "i PRON PRP\n",
      "just ADV RB\n",
      "put VERB VBD\n",
      "on ADP IN\n",
      "the DET DT\n",
      "table NOUN NN\n",
      "to PART TO\n",
      "help VERB VB\n",
      "go VERB VB\n",
      "with ADP IN\n",
      "my DET PRP$\n",
      "meal NOUN NN\n",
      ". PUNCT .\n",
      "After ADP IN\n",
      "I PRON PRP\n",
      "was AUX VBD\n",
      "done VERB VBN\n",
      "he PRON PRP\n",
      "just ADV RB\n",
      "reached VERB VBN\n",
      "for ADP IN\n",
      "my DET PRP$\n",
      "tray NOUN NN\n",
      "no INTJ UH\n",
      "\" PUNCT ``\n",
      "excuse VERB VB\n",
      "me PRON PRP\n",
      "or CCONJ CC\n",
      "are AUX VBP\n",
      "you PRON PRP\n",
      "done VERB VBN\n",
      "with ADP IN\n",
      "that DET DT\n",
      "? PUNCT .\n",
      "\" PUNCT ''\n",
      "  SPACE _SP\n",
      "I PRON PRP\n",
      "realize VERB VBP\n",
      "he PRON PRP\n",
      "'s AUX VBZ\n",
      "trying VERB VBG\n",
      "to PART TO\n",
      "do AUX VB\n",
      "his DET PRP$\n",
      "job NOUN NN\n",
      "quickly ADV RB\n",
      "but CCONJ CC\n",
      "a DET DT\n",
      "little ADJ JJ\n",
      "table NOUN NN\n",
      "manners NOUN NNS\n",
      "goes VERB VBZ\n",
      "along ADP IN\n",
      "way NOUN NN\n",
      ". PUNCT .\n",
      "That DET DT\n",
      "being AUX VBG\n",
      "said VERB VBN\n",
      "still ADV RB\n",
      "like INTJ UH\n",
      "to PART TO\n",
      "grub PROPN NNP\n",
      "here ADV RB\n",
      "and CCONJ CC\n",
      "glad ADJ JJ\n",
      "that SCONJ IN\n",
      "there PRON EX\n",
      "'s AUX VBZ\n",
      "finally ADV RB\n",
      "a DET DT\n",
      "Cane PROPN NNP\n",
      "'s PART POS\n",
      "close NOUN NN\n",
      "to ADP IN\n",
      "me PRON PRP\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# spacy gives you both fine grained (.pos_) + coarse grained (.tag_) parts of speech    \n",
    "for doc in doc_df[4]:\n",
    "    print(doc.text, doc.pos_, doc.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49657,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "FaC3G5Hck37F",
    "outputId": "ec23e333-eda5-489d-b807-bbd5950962cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interjection'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para buscar a qué se refiere\n",
    "spacy.explain('UH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49645,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Hqvy2dtby1H5",
    "outputId": "acb58a04-6810-4298-90d2-92b1e3e715eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a Canes, my casa, others, the Drive thru, the other canes, the area, I, my chicken, The cashiers, food, i, any funky chicken, The clean up crew zips, the dining area, it, My only gripe, the one fella, Red hair, he, the rounds, no smile, personality, he, the napkins, i, the table, my meal, I, he, my tray, me, you, I, he, his job, a little table manners, way, grub, a Cane, me]\n"
     ]
    }
   ],
   "source": [
    "# spaCy also does noun chunking for us\n",
    "\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpFwoavpy1H7"
   },
   "source": [
    "### Using [Textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49636,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "vZk8yu9Mk37P"
   },
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49630,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "RTCz0Q_Ty1H7"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpIJX8L-y1H8"
   },
   "source": [
    "The default tagger in TextBlob uses the PatternTagger, the same as [pattern](https://www.clips.uantwerpen.be/pattern), which is fine for our example. To use the NLTK tagger, we can specify the pos_tagger when we call TextBlob. More [here](http://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49622,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ph5GAFaIy1H9",
    "outputId": "26d339d0-f3a1-4110-ad19-d5d66097ce4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_df = review_df['text'].apply(TextBlob)\n",
    "type(blob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49609,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "bNTIxc1_y1H-",
    "outputId": "e56603d3-8989-4f40-dc27-e6a7477206e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blob_df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50210,
     "status": "ok",
     "timestamp": 1600932045099,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "I7RvQOyrBsz3",
    "outputId": "e9893181-c68d-4acb-fd22-a6ad9d9cc385"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alberto.Romero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Alberto.Romero\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50606,
     "status": "ok",
     "timestamp": 1600932045508,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "i2vgNTXHy1IA",
    "outputId": "ed653587-1a0d-4211-fa6a-d17abb0f0d42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Oh', 'UH'),\n",
       " ('happy', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('finally', 'RB'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Canes', 'NNP'),\n",
       " ('near', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('casa', 'NN'),\n",
       " ('Yes', 'RB'),\n",
       " ('just', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('others', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('griping', 'VBG'),\n",
       " ('about', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Drive', 'NNP'),\n",
       " ('thru', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('packed', 'VBN'),\n",
       " ('just', 'RB'),\n",
       " ('like', 'IN'),\n",
       " ('most', 'JJS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('canes', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('area', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('go', 'VB'),\n",
       " ('sit', 'RB'),\n",
       " ('down', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('enjoy', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('chicken', 'NN'),\n",
       " ('The', 'DT'),\n",
       " ('cashiers', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('as', 'RB'),\n",
       " ('far', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('food', 'NN'),\n",
       " ('wise', 'NN'),\n",
       " ('i', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('yet', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('receive', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('funky', 'JJ'),\n",
       " ('chicken', 'NN'),\n",
       " ('The', 'DT'),\n",
       " ('clean', 'JJ'),\n",
       " ('up', 'RP'),\n",
       " ('crew', 'JJ'),\n",
       " ('zips', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('dining', 'VBG'),\n",
       " ('area', 'NN'),\n",
       " ('constantly', 'RB'),\n",
       " ('so', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('usually', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('kept', 'VB'),\n",
       " ('My', 'PRP$'),\n",
       " ('only', 'JJ'),\n",
       " ('gripe', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('one', 'CD'),\n",
       " ('fella', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('Red', 'NNP'),\n",
       " ('hair', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('makes', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('rounds', 'NNS'),\n",
       " ('while', 'IN'),\n",
       " ('cleaning', 'VBG'),\n",
       " ('but', 'CC'),\n",
       " ('no', 'DT'),\n",
       " ('smile', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('personality', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('few', 'JJ'),\n",
       " ('nights', 'NNS'),\n",
       " ('ago', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('tossed', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('napkins', 'NNS'),\n",
       " ('i', 'VB'),\n",
       " ('just', 'RB'),\n",
       " ('put', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('table', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('help', 'VB'),\n",
       " ('go', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('meal', 'NN'),\n",
       " ('After', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('done', 'VBN'),\n",
       " ('he', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('reached', 'VBD'),\n",
       " ('for', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('tray', 'NN'),\n",
       " ('no', 'DT'),\n",
       " ('excuse', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('or', 'CC'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('done', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('I', 'PRP'),\n",
       " ('realize', 'VBP'),\n",
       " ('he', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('trying', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('do', 'VB'),\n",
       " ('his', 'PRP$'),\n",
       " ('job', 'NN'),\n",
       " ('quickly', 'RB'),\n",
       " ('but', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('table', 'JJ'),\n",
       " ('manners', 'NNS'),\n",
       " ('goes', 'VBZ'),\n",
       " ('along', 'JJ'),\n",
       " ('way', 'NN'),\n",
       " ('That', 'DT'),\n",
       " ('being', 'VBG'),\n",
       " ('said', 'VBD'),\n",
       " ('still', 'RB'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('grub', 'VB'),\n",
       " ('here', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('glad', 'VBP'),\n",
       " ('that', 'IN'),\n",
       " ('there', 'EX'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('finally', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('Cane', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('close', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('me', 'PRP')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_df[4].tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d0U14jiy1IC"
   },
   "source": [
    "# The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n",
    "\n",
    "A bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n",
    "\n",
    "It would also be nice to pick out words such as “magnificently,” “gleamed,” “intimidated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n",
    "Tf-Idf : A Simple Twist on Bag-of-Words\n",
    "\n",
    "Tf-Idf: Term frequency Inverse document frequency\n",
    "\n",
    "Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency–inverse document frequency.  Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n",
    "\n",
    "bow(w, d) = # times word w appears in document d\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n",
    "\n",
    "N is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n",
    "\n",
    "Alternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n",
    "\n",
    "If we define tf-idf as:\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n",
    "\n",
    "then a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n",
    "\n",
    "Let’s look at some pictures to understand what it’s all about. Figure 4-1 shows a simple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n",
    "\n",
    "<img src=\"imagenes/imagen10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30-014awy1IC"
   },
   "source": [
    "Now let’s look at the same four sentences in tf-idf representation using the log transform for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filtering methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TdYrbPGy1IC"
   },
   "source": [
    "The GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n",
    "\n",
    "<img src=\"imagenes/imagen11.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://tfidf.com/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB_PROFE_TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
