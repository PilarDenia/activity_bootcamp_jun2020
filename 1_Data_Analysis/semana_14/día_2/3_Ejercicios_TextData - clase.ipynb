{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRF_zpUrc02R"
   },
   "source": [
    "## Cleaning Text\n",
    "\n",
    "Clean white spaces in the text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Ydk_l4olc02S",
    "outputId": "cd336613-f2ab-4a45-bd3e-42adec4b9cd4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Interrobang. By aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "text_data = [\"  Interrobang. By aishwarya Henriette    \",\n",
    "            \"Parking And Going. By Karl Gautier\",\n",
    "            \"    Today Is The night. By Jarek Prakash\"]\n",
    "\n",
    "# strip whitespaces\n",
    "[string.strip() for string in  text_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo-z8BKac02X"
   },
   "outputs": [],
   "source": [
    "# remove periods\n",
    "text_data_nopoints = [string.replace('.','') for string in text_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['  Interrobang By aishwarya Henriette    ',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " '    Today Is The night By Jarek Prakash']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "[string.replace('.','') for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoEWZC8wc02Z"
   },
   "source": [
    "Capitalize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "0MiMFdpJc02c",
    "outputId": "24989f0a-b139-48bb-85dc-408f8f3a8787"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['  INTERROBANG. BY AISHWARYA HENRIETTE    ',\n",
       " 'PARKING AND GOING. BY KARL GAUTIER',\n",
       " '    TODAY IS THE NIGHT. BY JAREK PRAKASH']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "[string.upper() for string in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROWeB0AMc02e"
   },
   "source": [
    "### See Also\n",
    "* Beginners Tutorial for Regular Expressions in Python (https://www.analyticsvidhya.com/blog/2015/06/regular-expression-python/)\n",
    "\n",
    "## Parsing and Cleaning HTML\n",
    "Use Beautiful Soup to get the full name in the provided html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Sbc-P4lrc02f",
    "outputId": "de418e43-13e3-4acf-be1a-24bf7224bebd"
   },
   "outputs": [],
   "source": [
    "# beautiful soup es una biblioteca de Python que analiza HTML. Es Ãºtil para web scraping\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <div class='full_name'><span style='font-weight:bold'>Yan</span> Chin</div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6QtuHs8gR1zw",
    "outputId": "f4f181fd-a0c2-433e-f676-b97058f0254e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<div class=\"full_name\"><span style=\"font-weight:bold\">Yan</span> Chin</div>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<span style=\"font-weight:bold\">Yan</span>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(soup.find(\"div\", {\"class\":\"full_name\"}))\n",
    "soup.find(\"span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nYan Chin\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIWpbryEc02h"
   },
   "source": [
    "### See Also\n",
    "* Beautiful Soup documentation (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "## Removing Punctuation\n",
    "Remove punctuation of the text provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "awi3_C0oc02i",
    "outputId": "fd6a7dd1-6495-40d2-f5e8-cbbe0cc0bbd8"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hi i love this song  agree loveit right'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "import re\n",
    "\n",
    "text_data = ['Hi! I. Love. This. Song.....', '10000% Agree!!!! #LoveIT', 'Right?!?!']\n",
    "\n",
    "#\\w son palabras, \\s son espacios en blanco\n",
    "text_sin = re.sub(r\"[^a-zA-z\\s]\", \"\",\" \".join(text_data)).lower()\n",
    "#text_sin = re.sub(r\"[^\\w\\s]\", \"\",\" \".join(text_data)).lower()\n",
    "text_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\n",
    "text_sin_pts = [re.sub(r\"[^\\w\\s]\", \"\", str) for str in text_data]\n",
    "text_sin_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFCwwskAc02k"
   },
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-yH6wBAc02l",
    "outputId": "5f286d0f-86ed-4f89-823f-6735d2999464"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "\n",
    "#tokenize words\n",
    "\n",
    "l = word_tokenize(string)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mabKWS4qc02o"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today\"\n",
    "\n",
    "sent_tokenize (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF_ehH1gc02q"
   },
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\pilar\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenized_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3lJDgR_c02s"
   },
   "source": [
    "## Stemming Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjO9PH7Lc02t"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('i', 'i'),\n",
       " ('am', 'am'),\n",
       " ('humbled', 'humbl'),\n",
       " ('by', 'by'),\n",
       " ('this', 'thi'),\n",
       " ('traditional', 'tradit'),\n",
       " ('meeting', 'meet')]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "stemmer = PorterStemmer()\n",
    "stem_words = [(word,stemmer.stem(word)) for word in tokenized_words]\n",
    "stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Programers  :  program\nprogram  :  program\nwith  :  with\nprograming  :  program\nlanguages  :  languag\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# importing modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "   \n",
    "sentence = \"Programers program with programing languages\"\n",
    "words = word_tokenize(sentence) \n",
    "   \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bfXNBOMEc02v"
   },
   "source": [
    "### See Also\n",
    "* Porter Stemming Algorithm (https://tartarus.org/martin/PorterStemmer/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVwR8z7Nc02w"
   },
   "source": [
    "\n",
    "\n",
    "## Encoding Text as a Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vcIO4bgc02w",
    "outputId": "d83877b9-e5a9-495d-c7e5-6487e94829e3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Gremany beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTN7IIFBc02y",
    "outputId": "c867c696-5c75-4842-eac4-dd161d2518f9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Convert to array and show the result\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(text_data)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['I love Brazil. Brazil!', 'Sweden is best', 'Gremany beats both'],\n",
       "      dtype='<U22')"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmHD5OYAc021",
    "outputId": "3c6b6f2c-94bc-4725-cfd7-3c92b609254a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'gremany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# show the feature names\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'gremany', 'is', 'love', 'sweden']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tCn8LN_c023"
   },
   "source": [
    "\n",
    "\n",
    "## Weighting Word Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rH571Vcc023",
    "outputId": "7324e952-fd49-4a5a-d3ae-fc78837eacd6"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'])\n",
    "\n",
    "# create the tf-idf feature matrix\n",
    "v = TfidfVectorizer()\n",
    "v_matrix = v.fit_transform(text_data) #Matriz DISPERSA\n",
    "v.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHzH-iuUc025",
    "outputId": "c82e2b5a-cd69-4dbf-af9c-fbe2d6ac7aee"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# Convert to array and show the result\n",
    "v_matrix = v_matrix.toarray()\n",
    "v_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlomIjaQc027",
    "outputId": "7f67847a-d3d6-49e3-9132-4905877867ee"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# show the vocabulary\n",
    "v.get_feature_names()\n",
    "v.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfIiK37cc02-"
   },
   "source": [
    "$$\n",
    "tfidf(t, d) = tf(t,d) * idf(t)\n",
    "$$\n",
    "\n",
    "where $t$ is a word\n",
    "\n",
    "$d$ is a document\n",
    "\n",
    "$$\n",
    "idf(t) = log(\\frac{1 + n_d}{1 + df(d, t}) +1\n",
    "$$\n",
    "\n",
    "where $n_d$ is the number of documents and \n",
    "\n",
    "$df(d,t)$ is term, $t$'s document frequency (i.e. number of documents where the term appears)\n",
    "\n",
    "### See Also\n",
    "* scikit-learn documentation: tf-idf term weighting (http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OqkDC6wc02-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3-Ejercicios-TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}