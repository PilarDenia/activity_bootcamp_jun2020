{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26271,
     "status": "ok",
     "timestamp": 1600932020606,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "1FahTQE7mP6B",
    "outputId": "4c8acd28-b10c-487a-dac1-0750ab5ca06e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2O0nELPy1Gy"
   },
   "source": [
    "#Text Analysis\n",
    "-----\n",
    "\n",
    "## Text Data: Flattening, Filtering, and Chunking\n",
    "\n",
    "What would you do if you were designing an algorithm to analyze the following paragraph of text?\n",
    "\n",
    "    Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “Hello?”\n",
    "\n",
    "The paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n",
    "\n",
    "So, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main characters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n",
    "\n",
    "This chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling technique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2TIF9lMy1Gz"
   },
   "source": [
    "## Bag-of-X: Turning Natural Text into Flat Vectors\n",
    "\n",
    "Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and interpretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n",
    "For text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This technique can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indicator of the topic content of the document.\n",
    "\n",
    "Bag-of-Words\n",
    "\n",
    "In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corresponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZG2s2_51y1G0"
   },
   "source": [
    "![texto alternativo](https://drive.google.com/uc?id=1IbLBEuoCaN8E2ct7788gNOqskefXmUQ4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gplms5HDy1G0"
   },
   "source": [
    "Bag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words representation, these words are all equal elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "568KZ9p6y1G1"
   },
   "source": [
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1M9wfn4eNsV09yTGNsJIUlYOZv6DyOFss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F-3oF-ty1G2"
   },
   "source": [
    "What is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1V2N1rOUXJIqZndF9bxDkPifxYXkD8Rlp)\n",
    "\n",
    "Figure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1YHNStJX1ru7YAKmh6KYcySvW5B7TCuR_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYHN-3OTy1G2"
   },
   "source": [
    "These figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at feature vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vectors. Figure 3-5 shows an example. With bag-of-words featurization for text documents, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.”  As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix transpose of the bag-of-words vectors.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1-kP58QvDsOB64ho-29yOWUegXzcHXQFy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUR9ybpTy1G3"
   },
   "source": [
    "Bag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating negation plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n",
    "\n",
    "## Bag-of-n-Grams\n",
    "\n",
    "Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n",
    "\n",
    "n-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n",
    "\n",
    "To illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26256,
     "status": "ok",
     "timestamp": 1600932020607,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_g2co9boy1G4"
   },
   "outputs": [],
   "source": [
    "# importamos librerías\n",
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34020,
     "status": "ok",
     "timestamp": 1600932028382,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "5oElc6KLy1G7",
    "outputId": "7f1535d5-8d44-4c22-d053-64338e17af6b"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "biz_f = open('yelp_academic_dataset_business.json', encoding='latin1')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "biz_df.shape\n",
    "'''\n",
    "biz_df = pd.read_csv('./ficheros_clase/biz.csv', encoding='latin1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              business_id                      name  \\\n",
       "0  f9NumwFMBDn751xgFiRbNA  The Range At Lake Norman   \n",
       "1  Yzvjg0SayhoZgCljUJRF9Q         Carlos Santo, NMD   \n",
       "\n",
       "                     address        city state postal_code   latitude  \\\n",
       "0            10913 Bailey Rd   Cornelius    NC       28031  35.462724   \n",
       "1  8880 E Via Linda, Ste 107  Scottsdale    AZ       85258  33.569404   \n",
       "\n",
       "    longitude  stars  review_count  is_open  \\\n",
       "0  -80.852612    3.5            36        1   \n",
       "1 -111.890264    5.0             4        1   \n",
       "\n",
       "                                          attributes  \\\n",
       "0  {'BusinessAcceptsCreditCards': 'True', 'BikePa...   \n",
       "1  {'GoodForKids': 'True', 'ByAppointmentOnly': '...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Active Life, Gun/Rifle Ranges, Guns & Ammo, Sh...   \n",
       "1  Health & Medical, Fitness & Instruction, Yoga,...   \n",
       "\n",
       "                                               hours  \n",
       "0  {'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...  \n",
       "1                                                NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>business_id</th>\n      <th>name</th>\n      <th>address</th>\n      <th>city</th>\n      <th>state</th>\n      <th>postal_code</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>stars</th>\n      <th>review_count</th>\n      <th>is_open</th>\n      <th>attributes</th>\n      <th>categories</th>\n      <th>hours</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>f9NumwFMBDn751xgFiRbNA</td>\n      <td>The Range At Lake Norman</td>\n      <td>10913 Bailey Rd</td>\n      <td>Cornelius</td>\n      <td>NC</td>\n      <td>28031</td>\n      <td>35.462724</td>\n      <td>-80.852612</td>\n      <td>3.5</td>\n      <td>36</td>\n      <td>1</td>\n      <td>{'BusinessAcceptsCreditCards': 'True', 'BikePa...</td>\n      <td>Active Life, Gun/Rifle Ranges, Guns &amp; Ammo, Sh...</td>\n      <td>{'Monday': '10:0-18:0', 'Tuesday': '11:0-20:0'...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Yzvjg0SayhoZgCljUJRF9Q</td>\n      <td>Carlos Santo, NMD</td>\n      <td>8880 E Via Linda, Ste 107</td>\n      <td>Scottsdale</td>\n      <td>AZ</td>\n      <td>85258</td>\n      <td>33.569404</td>\n      <td>-111.890264</td>\n      <td>5.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>{'GoodForKids': 'True', 'ByAppointmentOnly': '...</td>\n      <td>Health &amp; Medical, Fitness &amp; Instruction, Yoga,...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "biz_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34005,
     "status": "ok",
     "timestamp": 1600932028383,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "KM-_yICVy1G-",
    "outputId": "48f148fa-8b92-44cf-8623-333d02487ea3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35363,
     "status": "ok",
     "timestamp": 1600932029756,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Rf6MS0iOy1HA",
    "outputId": "11ac8ceb-d6e8-4685-9166-c0311cf28577"
   },
   "outputs": [],
   "source": [
    "# Load the first 10,000 reviews\n",
    "\n",
    "'''\n",
    "# f = open('drive/My Drive/Colab Notebooks/yelp_academic_dataset_review.json', encoding='latin1')\n",
    "f = open('yelp_academic_dataset_review.json', encoding='latin1')\n",
    "js = []\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "'''\n",
    "\n",
    "review_df = pd.read_csv('./ficheros_clase/review.csv', encoding='latin1', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35344,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "2xZN2wy1y1HD",
    "outputId": "7c508d93-870b-4847-ca41-49b4f2e0872a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ  -MhfebM0QIsKt87iDN-FNw   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ  lbrU8StCq3yDfr-QMnGrmQ   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    2.0       5      0     0   \n",
       "1    1.0       1      1     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  As someone who has worked with many museums, I...  2015-04-15 05:21:16  \n",
       "1  I am actually horrified this place is still in...  2013-12-07 03:16:52  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n      <td>-MhfebM0QIsKt87iDN-FNw</td>\n      <td>2.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>As someone who has worked with many museums, I...</td>\n      <td>2015-04-15 05:21:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>I am actually horrified this place is still in...</td>\n      <td>2013-12-07 03:16:52</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "review_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35328,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "07mKeA-ry1HF",
    "outputId": "6c443116-065e-41b5-c576-0693f0caccff"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas. When I saw they would be showing infamous eggs of the House of Faberge from the Virginia Museum of Fine Arts (VMFA), I knew I had to go!\\n\\nTucked away near the gelateria and the garden, the Gallery is pretty much hidden from view. It\\'s what real estate agents would call \"cozy\" or \"charming\" - basically any euphemism for small.\\n\\nThat being said, you can still see wonderful art at a gallery of any size, so why the two *s you ask? Let me tell you:\\n\\n* pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top. For the space and the amount of art you can fit in there, it is a bit much.\\n* it\\'s not kid friendly at all. Seriously, don\\'t bring them.\\n* the security is not trained properly for the show. When the curating and design teams collaborate for exhibitions, there is a definite flow. That means visitors should view the art in a certain sequence, whether it be by historical period or cultural significance (this is how audio guides are usually developed). When I arrived in the gallery I could not tell where to start, and security was certainly not helpful. I was told to \"just look around\" and \"do whatever.\" \\n\\nAt such a *fine* institution, I find the lack of knowledge and respect for the art appalling.'"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "review_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35698,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yFLbM0jry1HI"
   },
   "outputs": [],
   "source": [
    "# uso CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35688,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "937zuVELy1HK"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b') \n",
    "#RegEx que se indica en token_pattern \n",
    "#(?u) es unicode\n",
    "#\\bword\\b\n",
    "#w+ es 'one or more word characters'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36571,
     "status": "ok",
     "timestamp": 1600932031068,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "u_3Xg2O7y1HO",
    "outputId": "babc1b92-d0b5-4ad6-a5d9-893af15baff8"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<10000x26206 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 716776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x = bow_converter.fit_transform(review_df.text)\n",
    "x #Es un array_Sparse = matriz_disperas, la gran mayoria de elementos son nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26206"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "words = bow_converter.get_feature_names()\n",
    "len(words) #Es una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36557,
     "status": "ok",
     "timestamp": 1600932031069,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZLDYBINRy1HR",
    "outputId": "3b70182c-0734-4f44-ba58-5b05390b19b0"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['0', '00', '000', '00a', '00am', '00p', '00pm', '00service', '01', '0100pm']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# los 10 primeros elementos de la lista, de 0 a 9\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['garnishment',\n",
       " 'garnishments',\n",
       " 'garret',\n",
       " 'gars',\n",
       " 'garten',\n",
       " 'garth',\n",
       " 'gary',\n",
       " 'gas',\n",
       " 'gasket',\n",
       " 'gaskets']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "words[9990:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39682,
     "status": "ok",
     "timestamp": 1600932034204,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rZZTnmvwy1HT"
   },
   "outputs": [],
   "source": [
    "bigram_converter = CountVectorizer(ngram_range=(2,2),token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "x2 = bigram_converter.fit_transform(review_df['text'])\n",
    "# tomamos las palabras solo de 2 en 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39672,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "VEfqqsFny1HV",
    "outputId": "6aabf05f-1d4f-4223-e487-af03ad0cda52"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "311506"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "bigrams = bigram_converter.get_feature_names()\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39655,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "R2Rkf4H2y1HX",
    "outputId": "3a1ea27e-5649-49a9-b5b1-9c1aaf6c000f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['what doesn',\n",
       " 'what dreams',\n",
       " 'what drew',\n",
       " 'what drinks',\n",
       " 'what dubious',\n",
       " 'what dwells',\n",
       " 'what each',\n",
       " 'what else',\n",
       " 'what ever',\n",
       " 'what everybody']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "bigrams[300035:300045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45154,
     "status": "ok",
     "timestamp": 1600932039720,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "tymksgvRy1HZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46275,
     "status": "ok",
     "timestamp": 1600932040853,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "6MwUtblry1Hb",
    "outputId": "58d0f375-5068-4e77-e9e4-c0ebdb9abb15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46259,
     "status": "ok",
     "timestamp": 1600932040854,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_v--lpqsy1He",
    "outputId": "ecfa30c9-feeb-4ccf-ed7f-dc8fd8fb9e85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46245,
     "status": "ok",
     "timestamp": 1600932040855,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "lU98Vv6py1Hg",
    "outputId": "59be51c1-0e87-4796-8d24-32bb5551ede8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46230,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "uMTx-NPXk35m",
    "outputId": "4e3d4afb-e365-4ce6-a6fd-71684060187f"
   },
   "outputs": [],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "triagram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "x3 = trigram_converter.fit_transform(review_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46215,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DbGhd75Xk35t",
    "outputId": "80a7b7da-8902-4d97-81a2-3ed6f531b13a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "726712"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "triagrams = trigram_converter.get_feature_names()\n",
    "len(triagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(26206, 311506, 726712)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(words),len(bigrams),len(triagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['was perfection melt',\n",
       " 'was perfection the',\n",
       " 'was perfection we',\n",
       " 'was perfectly attentive',\n",
       " 'was perfectly balanced',\n",
       " 'was perfectly bitter',\n",
       " 'was perfectly cooked',\n",
       " 'was perfectly done',\n",
       " 'was perfectly executed',\n",
       " 'was perfectly fine']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "triagrams[678900:678910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46198,
     "status": "ok",
     "timestamp": 1600932040857,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_IgcngOmk35y",
    "outputId": "7e17edd7-95d5-4456-bf86-a7b9bc16640a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'an': 0, 'apple': 1, 'day': 3, 'keeps': 5, 'the': 6, 'doctor': 4, 'away': 2}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "example_n_gramas = CountVectorizer(ngram_range=(1,1))\n",
    "print(example_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'an': 0, 'apple': 2, 'day': 5, 'keeps': 9, 'the': 11, 'doctor': 7, 'away': 4, 'an apple': 1, 'apple day': 3, 'day keeps': 6, 'keeps the': 10, 'the doctor': 12, 'doctor away': 8}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "example_n_gramas = CountVectorizer(ngram_range=(1,2))\n",
    "print(example_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'an': 0, 'apple': 3, 'day': 7, 'keeps': 12, 'the': 15, 'doctor': 10, 'away': 6, 'an apple': 1, 'apple day': 4, 'day keeps': 8, 'keeps the': 13, 'the doctor': 16, 'doctor away': 11, 'an apple day': 2, 'apple day keeps': 5, 'day keeps the': 9, 'keeps the doctor': 14, 'the doctor away': 17}\n"
     ]
    }
   ],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "example_n_gramas = CountVectorizer(ngram_range=(1,3))\n",
    "print(example_n_gramas.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46584,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DXeh7ui_y1Hi",
    "outputId": "c500d115-a727-4789-a252-ed3822d0567e"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas.util.testing as tm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46568,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "EWo__Xoxy1Hk",
    "outputId": "15920c9f-3b35-4a62-a957-6c793f923054"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"269.455pt\" version=\"1.1\" viewBox=\"0 0 470.5775 269.455\" width=\"470.5775pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 269.455 \r\nL 470.5775 269.455 \r\nL 470.5775 -0 \r\nL 0 -0 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 67.88875 242.2875 \r\nL 402.68875 242.2875 \r\nL 402.68875 24.8475 \r\nL 67.88875 24.8475 \r\nz\r\n\" style=\"fill:#eaeaf2;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 95.78875 242.2875 \r\nL 95.78875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- unigram -->\r\n      <defs>\r\n       <path d=\"M 40.578125 0 \r\nL 40.578125 7.625 \r\nQ 34.515625 -1.171875 24.125 -1.171875 \r\nQ 19.53125 -1.171875 15.546875 0.578125 \r\nQ 11.578125 2.34375 9.640625 5 \r\nQ 7.71875 7.671875 6.9375 11.53125 \r\nQ 6.390625 14.109375 6.390625 19.734375 \r\nL 6.390625 51.859375 \r\nL 15.1875 51.859375 \r\nL 15.1875 23.09375 \r\nQ 15.1875 16.21875 15.71875 13.8125 \r\nQ 16.546875 10.359375 19.234375 8.375 \r\nQ 21.921875 6.390625 25.875 6.390625 \r\nQ 29.828125 6.390625 33.296875 8.421875 \r\nQ 36.765625 10.453125 38.203125 13.9375 \r\nQ 39.65625 17.4375 39.65625 24.078125 \r\nL 39.65625 51.859375 \r\nL 48.4375 51.859375 \r\nL 48.4375 0 \r\nz\r\n\" id=\"ArialMT-117\"/>\r\n       <path d=\"M 6.59375 0 \r\nL 6.59375 51.859375 \r\nL 14.5 51.859375 \r\nL 14.5 44.484375 \r\nQ 20.21875 53.03125 31 53.03125 \r\nQ 35.6875 53.03125 39.625 51.34375 \r\nQ 43.5625 49.65625 45.515625 46.921875 \r\nQ 47.46875 44.1875 48.25 40.4375 \r\nQ 48.734375 37.984375 48.734375 31.890625 \r\nL 48.734375 0 \r\nL 39.9375 0 \r\nL 39.9375 31.546875 \r\nQ 39.9375 36.921875 38.90625 39.578125 \r\nQ 37.890625 42.234375 35.28125 43.8125 \r\nQ 32.671875 45.40625 29.15625 45.40625 \r\nQ 23.53125 45.40625 19.453125 41.84375 \r\nQ 15.375 38.28125 15.375 28.328125 \r\nL 15.375 0 \r\nz\r\n\" id=\"ArialMT-110\"/>\r\n       <path d=\"M 6.640625 61.46875 \r\nL 6.640625 71.578125 \r\nL 15.4375 71.578125 \r\nL 15.4375 61.46875 \r\nz\r\nM 6.640625 0 \r\nL 6.640625 51.859375 \r\nL 15.4375 51.859375 \r\nL 15.4375 0 \r\nz\r\n\" id=\"ArialMT-105\"/>\r\n       <path d=\"M 4.984375 -4.296875 \r\nL 13.53125 -5.5625 \r\nQ 14.0625 -9.515625 16.5 -11.328125 \r\nQ 19.78125 -13.765625 25.4375 -13.765625 \r\nQ 31.546875 -13.765625 34.859375 -11.328125 \r\nQ 38.1875 -8.890625 39.359375 -4.5 \r\nQ 40.046875 -1.8125 39.984375 6.78125 \r\nQ 34.234375 0 25.640625 0 \r\nQ 14.9375 0 9.078125 7.71875 \r\nQ 3.21875 15.4375 3.21875 26.21875 \r\nQ 3.21875 33.640625 5.90625 39.90625 \r\nQ 8.59375 46.1875 13.6875 49.609375 \r\nQ 18.796875 53.03125 25.6875 53.03125 \r\nQ 34.859375 53.03125 40.828125 45.609375 \r\nL 40.828125 51.859375 \r\nL 48.921875 51.859375 \r\nL 48.921875 7.03125 \r\nQ 48.921875 -5.078125 46.453125 -10.125 \r\nQ 44 -15.1875 38.640625 -18.109375 \r\nQ 33.296875 -21.046875 25.484375 -21.046875 \r\nQ 16.21875 -21.046875 10.5 -16.875 \r\nQ 4.78125 -12.703125 4.984375 -4.296875 \r\nz\r\nM 12.25 26.859375 \r\nQ 12.25 16.65625 16.296875 11.96875 \r\nQ 20.359375 7.28125 26.46875 7.28125 \r\nQ 32.515625 7.28125 36.609375 11.9375 \r\nQ 40.71875 16.609375 40.71875 26.5625 \r\nQ 40.71875 36.078125 36.5 40.90625 \r\nQ 32.28125 45.75 26.3125 45.75 \r\nQ 20.453125 45.75 16.34375 40.984375 \r\nQ 12.25 36.234375 12.25 26.859375 \r\nz\r\n\" id=\"ArialMT-103\"/>\r\n       <path d=\"M 6.5 0 \r\nL 6.5 51.859375 \r\nL 14.40625 51.859375 \r\nL 14.40625 44 \r\nQ 17.4375 49.515625 20 51.265625 \r\nQ 22.5625 53.03125 25.640625 53.03125 \r\nQ 30.078125 53.03125 34.671875 50.203125 \r\nL 31.640625 42.046875 \r\nQ 28.421875 43.953125 25.203125 43.953125 \r\nQ 22.3125 43.953125 20.015625 42.21875 \r\nQ 17.71875 40.484375 16.75 37.40625 \r\nQ 15.28125 32.71875 15.28125 27.15625 \r\nL 15.28125 0 \r\nz\r\n\" id=\"ArialMT-114\"/>\r\n       <path d=\"M 40.4375 6.390625 \r\nQ 35.546875 2.25 31.03125 0.53125 \r\nQ 26.515625 -1.171875 21.34375 -1.171875 \r\nQ 12.796875 -1.171875 8.203125 3 \r\nQ 3.609375 7.171875 3.609375 13.671875 \r\nQ 3.609375 17.484375 5.34375 20.625 \r\nQ 7.078125 23.78125 9.890625 25.6875 \r\nQ 12.703125 27.59375 16.21875 28.5625 \r\nQ 18.796875 29.25 24.03125 29.890625 \r\nQ 34.671875 31.15625 39.703125 32.90625 \r\nQ 39.75 34.71875 39.75 35.203125 \r\nQ 39.75 40.578125 37.25 42.78125 \r\nQ 33.890625 45.75 27.25 45.75 \r\nQ 21.046875 45.75 18.09375 43.578125 \r\nQ 15.140625 41.40625 13.71875 35.890625 \r\nL 5.125 37.0625 \r\nQ 6.296875 42.578125 8.984375 45.96875 \r\nQ 11.671875 49.359375 16.75 51.1875 \r\nQ 21.828125 53.03125 28.515625 53.03125 \r\nQ 35.15625 53.03125 39.296875 51.46875 \r\nQ 43.453125 49.90625 45.40625 47.53125 \r\nQ 47.359375 45.171875 48.140625 41.546875 \r\nQ 48.578125 39.3125 48.578125 33.453125 \r\nL 48.578125 21.734375 \r\nQ 48.578125 9.46875 49.140625 6.21875 \r\nQ 49.703125 2.984375 51.375 0 \r\nL 42.1875 0 \r\nQ 40.828125 2.734375 40.4375 6.390625 \r\nz\r\nM 39.703125 26.03125 \r\nQ 34.90625 24.078125 25.34375 22.703125 \r\nQ 19.921875 21.921875 17.671875 20.9375 \r\nQ 15.4375 19.96875 14.203125 18.09375 \r\nQ 12.984375 16.21875 12.984375 13.921875 \r\nQ 12.984375 10.40625 15.640625 8.0625 \r\nQ 18.3125 5.71875 23.4375 5.71875 \r\nQ 28.515625 5.71875 32.46875 7.9375 \r\nQ 36.421875 10.15625 38.28125 14.015625 \r\nQ 39.703125 17 39.703125 22.796875 \r\nz\r\n\" id=\"ArialMT-97\"/>\r\n       <path d=\"M 6.59375 0 \r\nL 6.59375 51.859375 \r\nL 14.453125 51.859375 \r\nL 14.453125 44.578125 \r\nQ 16.890625 48.390625 20.9375 50.703125 \r\nQ 25 53.03125 30.171875 53.03125 \r\nQ 35.9375 53.03125 39.625 50.640625 \r\nQ 43.3125 48.25 44.828125 43.953125 \r\nQ 50.984375 53.03125 60.84375 53.03125 \r\nQ 68.5625 53.03125 72.703125 48.75 \r\nQ 76.859375 44.484375 76.859375 35.59375 \r\nL 76.859375 0 \r\nL 68.109375 0 \r\nL 68.109375 32.671875 \r\nQ 68.109375 37.9375 67.25 40.25 \r\nQ 66.40625 42.578125 64.15625 43.984375 \r\nQ 61.921875 45.40625 58.890625 45.40625 \r\nQ 53.421875 45.40625 49.796875 41.765625 \r\nQ 46.1875 38.140625 46.1875 30.125 \r\nL 46.1875 0 \r\nL 37.40625 0 \r\nL 37.40625 33.6875 \r\nQ 37.40625 39.546875 35.25 42.46875 \r\nQ 33.109375 45.40625 28.21875 45.40625 \r\nQ 24.515625 45.40625 21.359375 43.453125 \r\nQ 18.21875 41.5 16.796875 37.734375 \r\nQ 15.375 33.984375 15.375 26.90625 \r\nL 15.375 0 \r\nz\r\n\" id=\"ArialMT-109\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(70.50125 259.308437)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-117\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-110\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-105\"/>\r\n       <use x=\"133.447266\" xlink:href=\"#ArialMT-103\"/>\r\n       <use x=\"189.0625\" xlink:href=\"#ArialMT-114\"/>\r\n       <use x=\"222.363281\" xlink:href=\"#ArialMT-97\"/>\r\n       <use x=\"277.978516\" xlink:href=\"#ArialMT-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 235.28875 242.2875 \r\nL 235.28875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- bigram -->\r\n      <defs>\r\n       <path d=\"M 14.703125 0 \r\nL 6.546875 0 \r\nL 6.546875 71.578125 \r\nL 15.328125 71.578125 \r\nL 15.328125 46.046875 \r\nQ 20.90625 53.03125 29.546875 53.03125 \r\nQ 34.328125 53.03125 38.59375 51.09375 \r\nQ 42.875 49.171875 45.625 45.671875 \r\nQ 48.390625 42.1875 49.953125 37.25 \r\nQ 51.515625 32.328125 51.515625 26.703125 \r\nQ 51.515625 13.375 44.921875 6.09375 \r\nQ 38.328125 -1.171875 29.109375 -1.171875 \r\nQ 19.921875 -1.171875 14.703125 6.5 \r\nz\r\nM 14.59375 26.3125 \r\nQ 14.59375 17 17.140625 12.84375 \r\nQ 21.296875 6.0625 28.375 6.0625 \r\nQ 34.125 6.0625 38.328125 11.0625 \r\nQ 42.53125 16.0625 42.53125 25.984375 \r\nQ 42.53125 36.140625 38.5 40.96875 \r\nQ 34.46875 45.796875 28.765625 45.796875 \r\nQ 23 45.796875 18.796875 40.796875 \r\nQ 14.59375 35.796875 14.59375 26.3125 \r\nz\r\n\" id=\"ArialMT-98\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(213.893906 259.308437)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-98\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-105\"/>\r\n       <use x=\"77.832031\" xlink:href=\"#ArialMT-103\"/>\r\n       <use x=\"133.447266\" xlink:href=\"#ArialMT-114\"/>\r\n       <use x=\"166.748047\" xlink:href=\"#ArialMT-97\"/>\r\n       <use x=\"222.363281\" xlink:href=\"#ArialMT-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 374.78875 242.2875 \r\nL 374.78875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- trigram -->\r\n      <defs>\r\n       <path d=\"M 25.78125 7.859375 \r\nL 27.046875 0.09375 \r\nQ 23.34375 -0.6875 20.40625 -0.6875 \r\nQ 15.625 -0.6875 12.984375 0.828125 \r\nQ 10.359375 2.34375 9.28125 4.8125 \r\nQ 8.203125 7.28125 8.203125 15.1875 \r\nL 8.203125 45.015625 \r\nL 1.765625 45.015625 \r\nL 1.765625 51.859375 \r\nL 8.203125 51.859375 \r\nL 8.203125 64.703125 \r\nL 16.9375 69.96875 \r\nL 16.9375 51.859375 \r\nL 25.78125 51.859375 \r\nL 25.78125 45.015625 \r\nL 16.9375 45.015625 \r\nL 16.9375 14.703125 \r\nQ 16.9375 10.9375 17.40625 9.859375 \r\nQ 17.875 8.796875 18.921875 8.15625 \r\nQ 19.96875 7.515625 21.921875 7.515625 \r\nQ 23.390625 7.515625 25.78125 7.859375 \r\nz\r\n\" id=\"ArialMT-116\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(353.011094 259.308437)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-116\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#ArialMT-114\"/>\r\n       <use x=\"61.083984\" xlink:href=\"#ArialMT-105\"/>\r\n       <use x=\"83.300781\" xlink:href=\"#ArialMT-103\"/>\r\n       <use x=\"138.916016\" xlink:href=\"#ArialMT-114\"/>\r\n       <use x=\"172.216797\" xlink:href=\"#ArialMT-97\"/>\r\n       <use x=\"227.832031\" xlink:href=\"#ArialMT-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_4\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 67.88875 230.94621 \r\nL 402.68875 230.94621 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 4.15625 35.296875 \r\nQ 4.15625 48 6.765625 55.734375 \r\nQ 9.375 63.484375 14.515625 67.671875 \r\nQ 19.671875 71.875 27.484375 71.875 \r\nQ 33.25 71.875 37.59375 69.546875 \r\nQ 41.9375 67.234375 44.765625 62.859375 \r\nQ 47.609375 58.5 49.21875 52.21875 \r\nQ 50.828125 45.953125 50.828125 35.296875 \r\nQ 50.828125 22.703125 48.234375 14.96875 \r\nQ 45.65625 7.234375 40.5 3 \r\nQ 35.359375 -1.21875 27.484375 -1.21875 \r\nQ 17.140625 -1.21875 11.234375 6.203125 \r\nQ 4.15625 15.140625 4.15625 35.296875 \r\nz\r\nM 13.1875 35.296875 \r\nQ 13.1875 17.671875 17.3125 11.828125 \r\nQ 21.4375 6 27.484375 6 \r\nQ 33.546875 6 37.671875 11.859375 \r\nQ 41.796875 17.71875 41.796875 35.296875 \r\nQ 41.796875 52.984375 37.671875 58.78125 \r\nQ 33.546875 64.59375 27.390625 64.59375 \r\nQ 21.34375 64.59375 17.71875 59.46875 \r\nQ 13.1875 52.9375 13.1875 35.296875 \r\nz\r\n\" id=\"ArialMT-48\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(53.103437 235.956679)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 67.88875 179.212178 \r\nL 402.68875 179.212178 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200000 -->\r\n      <defs>\r\n       <path d=\"M 50.34375 8.453125 \r\nL 50.34375 0 \r\nL 3.03125 0 \r\nQ 2.9375 3.171875 4.046875 6.109375 \r\nQ 5.859375 10.9375 9.828125 15.625 \r\nQ 13.8125 20.3125 21.34375 26.46875 \r\nQ 33.015625 36.03125 37.109375 41.625 \r\nQ 41.21875 47.21875 41.21875 52.203125 \r\nQ 41.21875 57.421875 37.46875 61 \r\nQ 33.734375 64.59375 27.734375 64.59375 \r\nQ 21.390625 64.59375 17.578125 60.78125 \r\nQ 13.765625 56.984375 13.71875 50.25 \r\nL 4.6875 51.171875 \r\nQ 5.609375 61.28125 11.65625 66.578125 \r\nQ 17.71875 71.875 27.9375 71.875 \r\nQ 38.234375 71.875 44.234375 66.15625 \r\nQ 50.25 60.453125 50.25 52 \r\nQ 50.25 47.703125 48.484375 43.546875 \r\nQ 46.734375 39.40625 42.65625 34.8125 \r\nQ 38.578125 30.21875 29.109375 22.21875 \r\nQ 21.1875 15.578125 18.9375 13.203125 \r\nQ 16.703125 10.84375 15.234375 8.453125 \r\nz\r\n\" id=\"ArialMT-50\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(14.176875 184.222647)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-50\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"222.460938\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"278.076172\" xlink:href=\"#ArialMT-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_6\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 67.88875 127.478146 \r\nL 402.68875 127.478146 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 400000 -->\r\n      <defs>\r\n       <path d=\"M 32.328125 0 \r\nL 32.328125 17.140625 \r\nL 1.265625 17.140625 \r\nL 1.265625 25.203125 \r\nL 33.9375 71.578125 \r\nL 41.109375 71.578125 \r\nL 41.109375 25.203125 \r\nL 50.78125 25.203125 \r\nL 50.78125 17.140625 \r\nL 41.109375 17.140625 \r\nL 41.109375 0 \r\nz\r\nM 32.328125 25.203125 \r\nL 32.328125 57.46875 \r\nL 9.90625 25.203125 \r\nz\r\n\" id=\"ArialMT-52\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(14.176875 132.488614)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-52\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"222.460938\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"278.076172\" xlink:href=\"#ArialMT-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#pbba347cfbd)\" d=\"M 67.88875 75.744113 \r\nL 402.68875 75.744113 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 600000 -->\r\n      <defs>\r\n       <path d=\"M 49.75 54.046875 \r\nL 41.015625 53.375 \r\nQ 39.84375 58.546875 37.703125 60.890625 \r\nQ 34.125 64.65625 28.90625 64.65625 \r\nQ 24.703125 64.65625 21.53125 62.3125 \r\nQ 17.390625 59.28125 14.984375 53.46875 \r\nQ 12.59375 47.65625 12.5 36.921875 \r\nQ 15.671875 41.75 20.265625 44.09375 \r\nQ 24.859375 46.4375 29.890625 46.4375 \r\nQ 38.671875 46.4375 44.84375 39.96875 \r\nQ 51.03125 33.5 51.03125 23.25 \r\nQ 51.03125 16.5 48.125 10.71875 \r\nQ 45.21875 4.9375 40.140625 1.859375 \r\nQ 35.0625 -1.21875 28.609375 -1.21875 \r\nQ 17.625 -1.21875 10.6875 6.859375 \r\nQ 3.765625 14.9375 3.765625 33.5 \r\nQ 3.765625 54.25 11.421875 63.671875 \r\nQ 18.109375 71.875 29.4375 71.875 \r\nQ 37.890625 71.875 43.28125 67.140625 \r\nQ 48.6875 62.40625 49.75 54.046875 \r\nz\r\nM 13.875 23.1875 \r\nQ 13.875 18.65625 15.796875 14.5 \r\nQ 17.71875 10.359375 21.1875 8.171875 \r\nQ 24.65625 6 28.46875 6 \r\nQ 34.03125 6 38.03125 10.484375 \r\nQ 42.046875 14.984375 42.046875 22.703125 \r\nQ 42.046875 30.125 38.078125 34.390625 \r\nQ 34.125 38.671875 28.125 38.671875 \r\nQ 22.171875 38.671875 18.015625 34.390625 \r\nQ 13.875 30.125 13.875 23.1875 \r\nz\r\n\" id=\"ArialMT-54\"/>\r\n      </defs>\r\n      <g style=\"fill:#262626;\" transform=\"translate(14.176875 80.754582)scale(0.14 -0.14)\">\r\n       <use xlink:href=\"#ArialMT-54\"/>\r\n       <use x=\"55.615234\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"111.230469\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"222.460938\" xlink:href=\"#ArialMT-48\"/>\r\n       <use x=\"278.076172\" xlink:href=\"#ArialMT-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_8\">\r\n    <path clip-path=\"url(#pbba347cfbd)\" d=\"M 95.78875 224.1675 \r\nL 235.28875 150.368903 \r\nL 374.78875 42.9675 \r\n\" style=\"fill:none;stroke:#6495ed;stroke-linecap:round;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_9\">\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m6e197384da\" style=\"stroke:#0000ff;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pbba347cfbd)\">\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"95.78875\" xlink:href=\"#m6e197384da\" y=\"224.1675\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"235.28875\" xlink:href=\"#m6e197384da\" y=\"150.368903\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"374.78875\" xlink:href=\"#m6e197384da\" y=\"42.9675\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 67.88875 242.2875 \r\nL 67.88875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 402.68875 242.2875 \r\nL 402.68875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 67.88875 242.2875 \r\nL 402.68875 242.2875 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 67.88875 24.8475 \r\nL 402.68875 24.8475 \r\n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_8\">\r\n    <!-- Number of ngrams in the first 10,000 reviews of the Yelp dataset -->\r\n    <defs>\r\n     <path d=\"M 7.625 0 \r\nL 7.625 71.578125 \r\nL 17.328125 71.578125 \r\nL 54.9375 15.375 \r\nL 54.9375 71.578125 \r\nL 64.015625 71.578125 \r\nL 64.015625 0 \r\nL 54.296875 0 \r\nL 16.703125 56.25 \r\nL 16.703125 0 \r\nz\r\n\" id=\"ArialMT-78\"/>\r\n     <path d=\"M 42.09375 16.703125 \r\nL 51.171875 15.578125 \r\nQ 49.03125 7.625 43.21875 3.21875 \r\nQ 37.40625 -1.171875 28.375 -1.171875 \r\nQ 17 -1.171875 10.328125 5.828125 \r\nQ 3.65625 12.84375 3.65625 25.484375 \r\nQ 3.65625 38.578125 10.390625 45.796875 \r\nQ 17.140625 53.03125 27.875 53.03125 \r\nQ 38.28125 53.03125 44.875 45.953125 \r\nQ 51.46875 38.875 51.46875 26.03125 \r\nQ 51.46875 25.25 51.421875 23.6875 \r\nL 12.75 23.6875 \r\nQ 13.234375 15.140625 17.578125 10.59375 \r\nQ 21.921875 6.0625 28.421875 6.0625 \r\nQ 33.25 6.0625 36.671875 8.59375 \r\nQ 40.09375 11.140625 42.09375 16.703125 \r\nz\r\nM 13.234375 30.90625 \r\nL 42.1875 30.90625 \r\nQ 41.609375 37.453125 38.875 40.71875 \r\nQ 34.671875 45.796875 27.984375 45.796875 \r\nQ 21.921875 45.796875 17.796875 41.75 \r\nQ 13.671875 37.703125 13.234375 30.90625 \r\nz\r\n\" id=\"ArialMT-101\"/>\r\n     <path id=\"ArialMT-32\"/>\r\n     <path d=\"M 3.328125 25.921875 \r\nQ 3.328125 40.328125 11.328125 47.265625 \r\nQ 18.015625 53.03125 27.640625 53.03125 \r\nQ 38.328125 53.03125 45.109375 46.015625 \r\nQ 51.90625 39.015625 51.90625 26.65625 \r\nQ 51.90625 16.65625 48.90625 10.90625 \r\nQ 45.90625 5.171875 40.15625 2 \r\nQ 34.421875 -1.171875 27.640625 -1.171875 \r\nQ 16.75 -1.171875 10.03125 5.8125 \r\nQ 3.328125 12.796875 3.328125 25.921875 \r\nz\r\nM 12.359375 25.921875 \r\nQ 12.359375 15.96875 16.703125 11.015625 \r\nQ 21.046875 6.0625 27.640625 6.0625 \r\nQ 34.1875 6.0625 38.53125 11.03125 \r\nQ 42.875 16.015625 42.875 26.21875 \r\nQ 42.875 35.84375 38.5 40.796875 \r\nQ 34.125 45.75 27.640625 45.75 \r\nQ 21.046875 45.75 16.703125 40.8125 \r\nQ 12.359375 35.890625 12.359375 25.921875 \r\nz\r\n\" id=\"ArialMT-111\"/>\r\n     <path d=\"M 8.6875 0 \r\nL 8.6875 45.015625 \r\nL 0.921875 45.015625 \r\nL 0.921875 51.859375 \r\nL 8.6875 51.859375 \r\nL 8.6875 57.375 \r\nQ 8.6875 62.59375 9.625 65.140625 \r\nQ 10.890625 68.5625 14.078125 70.671875 \r\nQ 17.28125 72.796875 23.046875 72.796875 \r\nQ 26.765625 72.796875 31.25 71.921875 \r\nL 29.9375 64.265625 \r\nQ 27.203125 64.75 24.75 64.75 \r\nQ 20.75 64.75 19.09375 63.03125 \r\nQ 17.4375 61.328125 17.4375 56.640625 \r\nL 17.4375 51.859375 \r\nL 27.546875 51.859375 \r\nL 27.546875 45.015625 \r\nL 17.4375 45.015625 \r\nL 17.4375 0 \r\nz\r\n\" id=\"ArialMT-102\"/>\r\n     <path d=\"M 3.078125 15.484375 \r\nL 11.765625 16.84375 \r\nQ 12.5 11.625 15.84375 8.84375 \r\nQ 19.1875 6.0625 25.203125 6.0625 \r\nQ 31.25 6.0625 34.171875 8.515625 \r\nQ 37.109375 10.984375 37.109375 14.3125 \r\nQ 37.109375 17.28125 34.515625 19 \r\nQ 32.71875 20.171875 25.53125 21.96875 \r\nQ 15.875 24.421875 12.140625 26.203125 \r\nQ 8.40625 27.984375 6.46875 31.125 \r\nQ 4.546875 34.28125 4.546875 38.09375 \r\nQ 4.546875 41.546875 6.125 44.5 \r\nQ 7.71875 47.46875 10.453125 49.421875 \r\nQ 12.5 50.921875 16.03125 51.96875 \r\nQ 19.578125 53.03125 23.640625 53.03125 \r\nQ 29.734375 53.03125 34.34375 51.265625 \r\nQ 38.96875 49.515625 41.15625 46.5 \r\nQ 43.359375 43.5 44.1875 38.484375 \r\nL 35.59375 37.3125 \r\nQ 35.015625 41.3125 32.203125 43.546875 \r\nQ 29.390625 45.796875 24.265625 45.796875 \r\nQ 18.21875 45.796875 15.625 43.796875 \r\nQ 13.03125 41.796875 13.03125 39.109375 \r\nQ 13.03125 37.40625 14.109375 36.03125 \r\nQ 15.1875 34.625 17.484375 33.6875 \r\nQ 18.796875 33.203125 25.25 31.453125 \r\nQ 34.578125 28.953125 38.25 27.359375 \r\nQ 41.9375 25.78125 44.03125 22.75 \r\nQ 46.140625 19.734375 46.140625 15.234375 \r\nQ 46.140625 10.84375 43.578125 6.953125 \r\nQ 41.015625 3.078125 36.171875 0.953125 \r\nQ 31.34375 -1.171875 25.25 -1.171875 \r\nQ 15.140625 -1.171875 9.84375 3.03125 \r\nQ 4.546875 7.234375 3.078125 15.484375 \r\nz\r\n\" id=\"ArialMT-115\"/>\r\n     <path d=\"M 6.59375 0 \r\nL 6.59375 71.578125 \r\nL 15.375 71.578125 \r\nL 15.375 45.90625 \r\nQ 21.53125 53.03125 30.90625 53.03125 \r\nQ 36.671875 53.03125 40.921875 50.75 \r\nQ 45.171875 48.484375 47 44.484375 \r\nQ 48.828125 40.484375 48.828125 32.859375 \r\nL 48.828125 0 \r\nL 40.046875 0 \r\nL 40.046875 32.859375 \r\nQ 40.046875 39.453125 37.1875 42.453125 \r\nQ 34.328125 45.453125 29.109375 45.453125 \r\nQ 25.203125 45.453125 21.75 43.421875 \r\nQ 18.3125 41.40625 16.84375 37.9375 \r\nQ 15.375 34.46875 15.375 28.375 \r\nL 15.375 0 \r\nz\r\n\" id=\"ArialMT-104\"/>\r\n     <path d=\"M 37.25 0 \r\nL 28.46875 0 \r\nL 28.46875 56 \r\nQ 25.296875 52.984375 20.140625 49.953125 \r\nQ 14.984375 46.921875 10.890625 45.40625 \r\nL 10.890625 53.90625 \r\nQ 18.265625 57.375 23.78125 62.296875 \r\nQ 29.296875 67.234375 31.59375 71.875 \r\nL 37.25 71.875 \r\nz\r\n\" id=\"ArialMT-49\"/>\r\n     <path d=\"M 8.890625 0 \r\nL 8.890625 10.015625 \r\nL 18.890625 10.015625 \r\nL 18.890625 0 \r\nQ 18.890625 -5.515625 16.9375 -8.90625 \r\nQ 14.984375 -12.3125 10.75 -14.15625 \r\nL 8.296875 -10.40625 \r\nQ 11.078125 -9.1875 12.390625 -6.8125 \r\nQ 13.71875 -4.4375 13.875 0 \r\nz\r\n\" id=\"ArialMT-44\"/>\r\n     <path d=\"M 21 0 \r\nL 1.265625 51.859375 \r\nL 10.546875 51.859375 \r\nL 21.6875 20.796875 \r\nQ 23.484375 15.765625 25 10.359375 \r\nQ 26.171875 14.453125 28.265625 20.21875 \r\nL 39.796875 51.859375 \r\nL 48.828125 51.859375 \r\nL 29.203125 0 \r\nz\r\n\" id=\"ArialMT-118\"/>\r\n     <path d=\"M 16.15625 0 \r\nL 0.296875 51.859375 \r\nL 9.375 51.859375 \r\nL 17.625 21.921875 \r\nL 20.703125 10.796875 \r\nQ 20.90625 11.625 23.390625 21.484375 \r\nL 31.640625 51.859375 \r\nL 40.671875 51.859375 \r\nL 48.4375 21.78125 \r\nL 51.03125 11.859375 \r\nL 54 21.875 \r\nL 62.890625 51.859375 \r\nL 71.4375 51.859375 \r\nL 55.21875 0 \r\nL 46.09375 0 \r\nL 37.84375 31.0625 \r\nL 35.84375 39.890625 \r\nL 25.34375 0 \r\nz\r\n\" id=\"ArialMT-119\"/>\r\n     <path d=\"M 27.875 0 \r\nL 27.875 30.328125 \r\nL 0.296875 71.578125 \r\nL 11.8125 71.578125 \r\nL 25.921875 50 \r\nQ 29.828125 43.953125 33.203125 37.890625 \r\nQ 36.421875 43.5 41.015625 50.53125 \r\nL 54.890625 71.578125 \r\nL 65.921875 71.578125 \r\nL 37.359375 30.328125 \r\nL 37.359375 0 \r\nz\r\n\" id=\"ArialMT-89\"/>\r\n     <path d=\"M 6.390625 0 \r\nL 6.390625 71.578125 \r\nL 15.1875 71.578125 \r\nL 15.1875 0 \r\nz\r\n\" id=\"ArialMT-108\"/>\r\n     <path d=\"M 6.59375 -19.875 \r\nL 6.59375 51.859375 \r\nL 14.59375 51.859375 \r\nL 14.59375 45.125 \r\nQ 17.4375 49.078125 21 51.046875 \r\nQ 24.5625 53.03125 29.640625 53.03125 \r\nQ 36.28125 53.03125 41.359375 49.609375 \r\nQ 46.4375 46.1875 49.015625 39.953125 \r\nQ 51.609375 33.734375 51.609375 26.3125 \r\nQ 51.609375 18.359375 48.75 11.984375 \r\nQ 45.90625 5.609375 40.453125 2.21875 \r\nQ 35.015625 -1.171875 29 -1.171875 \r\nQ 24.609375 -1.171875 21.109375 0.6875 \r\nQ 17.625 2.546875 15.375 5.375 \r\nL 15.375 -19.875 \r\nz\r\nM 14.546875 25.640625 \r\nQ 14.546875 15.625 18.59375 10.84375 \r\nQ 22.65625 6.0625 28.421875 6.0625 \r\nQ 34.28125 6.0625 38.453125 11.015625 \r\nQ 42.625 15.96875 42.625 26.375 \r\nQ 42.625 36.28125 38.546875 41.203125 \r\nQ 34.46875 46.140625 28.8125 46.140625 \r\nQ 23.1875 46.140625 18.859375 40.890625 \r\nQ 14.546875 35.640625 14.546875 25.640625 \r\nz\r\n\" id=\"ArialMT-112\"/>\r\n     <path d=\"M 40.234375 0 \r\nL 40.234375 6.546875 \r\nQ 35.296875 -1.171875 25.734375 -1.171875 \r\nQ 19.53125 -1.171875 14.328125 2.25 \r\nQ 9.125 5.671875 6.265625 11.796875 \r\nQ 3.421875 17.921875 3.421875 25.875 \r\nQ 3.421875 33.640625 6 39.96875 \r\nQ 8.59375 46.296875 13.765625 49.65625 \r\nQ 18.953125 53.03125 25.34375 53.03125 \r\nQ 30.03125 53.03125 33.6875 51.046875 \r\nQ 37.359375 49.078125 39.65625 45.90625 \r\nL 39.65625 71.578125 \r\nL 48.390625 71.578125 \r\nL 48.390625 0 \r\nz\r\nM 12.453125 25.875 \r\nQ 12.453125 15.921875 16.640625 10.984375 \r\nQ 20.84375 6.0625 26.5625 6.0625 \r\nQ 32.328125 6.0625 36.34375 10.765625 \r\nQ 40.375 15.484375 40.375 25.140625 \r\nQ 40.375 35.796875 36.265625 40.765625 \r\nQ 32.171875 45.75 26.171875 45.75 \r\nQ 20.3125 45.75 16.375 40.96875 \r\nQ 12.453125 36.1875 12.453125 25.875 \r\nz\r\n\" id=\"ArialMT-100\"/>\r\n    </defs>\r\n    <g style=\"fill:#262626;\" transform=\"translate(7.2 18.8475)scale(0.16 -0.16)\">\r\n     <use xlink:href=\"#ArialMT-78\"/>\r\n     <use x=\"72.216797\" xlink:href=\"#ArialMT-117\"/>\r\n     <use x=\"127.832031\" xlink:href=\"#ArialMT-109\"/>\r\n     <use x=\"211.132812\" xlink:href=\"#ArialMT-98\"/>\r\n     <use x=\"266.748047\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"322.363281\" xlink:href=\"#ArialMT-114\"/>\r\n     <use x=\"355.664062\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"383.447266\" xlink:href=\"#ArialMT-111\"/>\r\n     <use x=\"439.0625\" xlink:href=\"#ArialMT-102\"/>\r\n     <use x=\"466.845703\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"494.628906\" xlink:href=\"#ArialMT-110\"/>\r\n     <use x=\"550.244141\" xlink:href=\"#ArialMT-103\"/>\r\n     <use x=\"605.859375\" xlink:href=\"#ArialMT-114\"/>\r\n     <use x=\"639.160156\" xlink:href=\"#ArialMT-97\"/>\r\n     <use x=\"694.775391\" xlink:href=\"#ArialMT-109\"/>\r\n     <use x=\"778.076172\" xlink:href=\"#ArialMT-115\"/>\r\n     <use x=\"828.076172\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"855.859375\" xlink:href=\"#ArialMT-105\"/>\r\n     <use x=\"878.076172\" xlink:href=\"#ArialMT-110\"/>\r\n     <use x=\"933.691406\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"961.474609\" xlink:href=\"#ArialMT-116\"/>\r\n     <use x=\"989.257812\" xlink:href=\"#ArialMT-104\"/>\r\n     <use x=\"1044.873047\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"1100.488281\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"1128.271484\" xlink:href=\"#ArialMT-102\"/>\r\n     <use x=\"1156.054688\" xlink:href=\"#ArialMT-105\"/>\r\n     <use x=\"1178.271484\" xlink:href=\"#ArialMT-114\"/>\r\n     <use x=\"1211.572266\" xlink:href=\"#ArialMT-115\"/>\r\n     <use x=\"1261.572266\" xlink:href=\"#ArialMT-116\"/>\r\n     <use x=\"1289.355469\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"1317.138672\" xlink:href=\"#ArialMT-49\"/>\r\n     <use x=\"1372.753906\" xlink:href=\"#ArialMT-48\"/>\r\n     <use x=\"1428.369141\" xlink:href=\"#ArialMT-44\"/>\r\n     <use x=\"1456.152344\" xlink:href=\"#ArialMT-48\"/>\r\n     <use x=\"1511.767578\" xlink:href=\"#ArialMT-48\"/>\r\n     <use x=\"1567.382812\" xlink:href=\"#ArialMT-48\"/>\r\n     <use x=\"1622.998047\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"1650.78125\" xlink:href=\"#ArialMT-114\"/>\r\n     <use x=\"1684.082031\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"1739.697266\" xlink:href=\"#ArialMT-118\"/>\r\n     <use x=\"1789.697266\" xlink:href=\"#ArialMT-105\"/>\r\n     <use x=\"1811.914062\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"1867.529297\" xlink:href=\"#ArialMT-119\"/>\r\n     <use x=\"1939.746094\" xlink:href=\"#ArialMT-115\"/>\r\n     <use x=\"1989.746094\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"2017.529297\" xlink:href=\"#ArialMT-111\"/>\r\n     <use x=\"2073.144531\" xlink:href=\"#ArialMT-102\"/>\r\n     <use x=\"2100.927734\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"2128.710938\" xlink:href=\"#ArialMT-116\"/>\r\n     <use x=\"2156.494141\" xlink:href=\"#ArialMT-104\"/>\r\n     <use x=\"2212.109375\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"2267.724609\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"2295.492188\" xlink:href=\"#ArialMT-89\"/>\r\n     <use x=\"2362.050781\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"2417.666016\" xlink:href=\"#ArialMT-108\"/>\r\n     <use x=\"2439.882812\" xlink:href=\"#ArialMT-112\"/>\r\n     <use x=\"2495.498047\" xlink:href=\"#ArialMT-32\"/>\r\n     <use x=\"2523.28125\" xlink:href=\"#ArialMT-100\"/>\r\n     <use x=\"2578.896484\" xlink:href=\"#ArialMT-97\"/>\r\n     <use x=\"2634.511719\" xlink:href=\"#ArialMT-116\"/>\r\n     <use x=\"2662.294922\" xlink:href=\"#ArialMT-97\"/>\r\n     <use x=\"2717.910156\" xlink:href=\"#ArialMT-115\"/>\r\n     <use x=\"2767.910156\" xlink:href=\"#ArialMT-101\"/>\r\n     <use x=\"2823.525391\" xlink:href=\"#ArialMT-116\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pbba347cfbd\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"67.88875\" y=\"24.8475\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "counts = [len(words), len(bigrams), len(trigrams)]\n",
    "plt.plot(counts, color='cornflowerblue')\n",
    "plt.plot(counts, 'bo')\n",
    "plt.margins(0.1)\n",
    "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Number of ngrams in the first 10,000 reviews of the Yelp dataset', {'fontsize':16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSEanGNOy1Hl"
   },
   "source": [
    "## Filtering for Cleaner Features\n",
    "\n",
    "With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n",
    "\n",
    "The popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:"
   ]
  },
  {
   "source": [
    "# NLTK -  Kit de Herramientas de Lenguaje Natural. Procesado de lenguaje Natural."
   ],
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47445,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Qq7BEgiXy1Hm",
    "outputId": "66490902-4a28-44f9-9617-d8382f4393f9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "179\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pilar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#Por defecto las palabra 'stopwords' que se descartan están en MINUSCULAS.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(len(stopwords.words('english')))\n",
    "stopwords.words('english')[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04kSK7ISy1Ho"
   },
   "source": [
    "Note that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n",
    "\n",
    "## Frequency-Based Filtering\n",
    "\n",
    "Stopword lists are a way of weeding out common words that make for vacuous features. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use frequency statistics here as well.\n",
    "\n",
    "### Frequent words\n",
    "\n",
    "Frequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n",
    "\n",
    "Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, frequency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also contains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=10t_e8ghb0288snEPVr1TdWDjBlhiYxjW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgLLbbshy1Ho"
   },
   "source": [
    "In practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no universal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n",
    "\n",
    "### Rare words\n",
    "\n",
    "Depending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful information. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar? Even if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledygook.”\n",
    "\n",
    "Not only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (tokenized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n",
    "\n",
    "Rare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further filtered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1jIU48-yVo2peJryvIvLWdVodVROWxJ5h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeLaC3qZy1Ho"
   },
   "source": [
    "Since one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n",
    "\n",
    "Since this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely contains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n",
    "\n",
    "\n",
    "## Stemming - 'Raiz'\n",
    "\n",
    "One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically different tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n",
    "\n",
    "Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tagging and linguistic rules in a process known as lemmatization.\n",
    "\n",
    "Most stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n",
    "\n",
    "Here is an example of running the Porter stemmer through the NLTK Python package. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=19m3iQWTeyzgVMZ_ncjtwJL2vTOGWzZYi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgSOAY6Py1Hp"
   },
   "source": [
    "Stemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n",
    "\n",
    "\n",
    "## Atoms of Meaning: From Words to n-Grams to Phrases\n",
    "\n",
    "The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n",
    "\n",
    "### Parsing and Tokenization\n",
    "\n",
    "Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n",
    "\n",
    "After light parsing, the plain-text portion of the document can go through tokenization. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what characters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n",
    "\n",
    "Sometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n",
    "\n",
    "### Collocation Extraction for Phrase Detection\n",
    "\n",
    "\n",
    "A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things.”\n",
    "\n",
    "Collocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a collocation.\n",
    "\n",
    "Collocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n",
    "\n",
    "Because collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a representation. Bag-of-n-grams is also problematic because it captures too many meaningless sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n",
    "\n",
    "Collocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n",
    "\n",
    "Since the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n",
    "\n",
    "### Frequency-based methods\n",
    "\n",
    "A simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams ( n = 2 ) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1qKEnujZ3oa6Mph8_CuqB7c1Xb0rbF6Dk)\n",
    "\n",
    "### Hypothesis testing for collocation extraction\n",
    "\n",
    "Raw popularity count is too crude of a measure. We have to find more clever statistics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n",
    "\n",
    "Hypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The randomness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n",
    "\n",
    "For example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p-Values.\n",
    "\n",
    "In the context of collocation extraction, many hypothesis tests have been proposed over the years.  One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following question: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n",
    "\n",
    "That is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n",
    "\n",
    "We can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n",
    "\n",
    "The final statistic is the log of the ratio between the two:\n",
    "\n",
    "log λ = log L ( Data; H null ) L ( Data; H alternate ) . #formatear\n",
    "\n",
    "The likelihood function L(Data; H) represents the probability of seeing the word frequencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strategy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probability.\n",
    "\n",
    "The algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n",
    "\n",
    "    Compute occurrence probabilities for all singleton words: P(w).\n",
    "    Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n",
    "    Compute the likelihood ratio log λ for all unique bigrams.\n",
    "    Sort the bigrams based on their likelihood ratio.\n",
    "    Take the bigrams with the smallest likelihood ratio values as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKpifucIy1Hp"
   },
   "source": [
    "There is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text corpora. Hence, it is not commonly used and we will not be demonstrating it here.\n",
    "\n",
    "Note that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filtering. To generate longer phrases, there are other methods such as chunking or combining with part-of-speech (PoS) tagging.\n",
    "\n",
    "\n",
    "### Chunking and part-of-speech tagging\n",
    "\n",
    "Chunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n",
    "\n",
    "For example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple language models available. \n",
    "\n",
    "To  illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47428,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ubFxtO5Yy1Hp",
    "outputId": "3c63dd4f-383c-496b-baaf-1a5054ea189d"
   },
   "outputs": [],
   "source": [
    "# Load the first 10 reviews\n",
    "'''\n",
    "f = open('yelp_academic_dataset_review.json')\n",
    "js = []\n",
    "for i in range(10):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "'''\n",
    "review_df = pd.read_csv('./ficheros_clase/review_10.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ  OwjRMXRC0KyPrIlcjaXeFQ  -MhfebM0QIsKt87iDN-FNw   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA  nIJD_7ZXHq-FX8byPMOkMQ  lbrU8StCq3yDfr-QMnGrmQ   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw  V34qejxNsCbcgD8C0HVk-Q  HQl28KMwrEKHqhFrrDqVNQ   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw  ofKDkJKXSKZXu5xJNGiiBQ  5JxlZaqCnk1MnbgRirs40Q   \n",
       "4  6TdNDKywdbjoTkizeMce8A  UgMW8bLE0QMJDCkQ1Ax5Mg  IS4cv902ykd8wj1TR0N3-A   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0    2.0       5      0     0   \n",
       "1    1.0       1      1     0   \n",
       "2    5.0       1      0     0   \n",
       "3    1.0       0      0     0   \n",
       "4    4.0       0      0     0   \n",
       "\n",
       "                                                text                 date  \n",
       "0  As someone who has worked with many museums, I...  2015-04-15 05:21:16  \n",
       "1  I am actually horrified this place is still in...  2013-12-07 03:16:52  \n",
       "2  I love Deagan's. I do. I really do. The atmosp...  2015-12-05 03:18:11  \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...  2011-05-27 05:30:52  \n",
       "4  Oh happy day, finally have a Canes near my cas...  2017-01-14 21:56:57  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>user_id</th>\n      <th>business_id</th>\n      <th>stars</th>\n      <th>useful</th>\n      <th>funny</th>\n      <th>cool</th>\n      <th>text</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n      <td>-MhfebM0QIsKt87iDN-FNw</td>\n      <td>2.0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>As someone who has worked with many museums, I...</td>\n      <td>2015-04-15 05:21:16</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>I am actually horrified this place is still in...</td>\n      <td>2013-12-07 03:16:52</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n      <td>2015-12-05 03:18:11</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n      <td>2011-05-27 05:30:52</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>6TdNDKywdbjoTkizeMce8A</td>\n      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n      <td>IS4cv902ykd8wj1TR0N3-A</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Oh happy day, finally have a Canes near my cas...</td>\n      <td>2017-01-14 21:56:57</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "he56IUUfy1Hs"
   },
   "source": [
    "### Using spacy: [Installation instructions for spacy](https://spacy.io/docs/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47418,
     "status": "ok",
     "timestamp": 1600932042154,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Ss9_u5wfy1Hs"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting http://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n  Downloading http://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz (12.0MB)\nCollecting spacy>=2.2.0 (from en-core-web-sm==2.2.0)\n  Using cached https://files.pythonhosted.org/packages/9b/ce/ddac37d457ae17152bc7e15164a11bf8236fc4e8a05cabb94d922f58ea23/spacy-2.3.2-cp37-cp37m-win_amd64.whl\nCollecting srsly<1.1.0,>=1.0.2 (from spacy>=2.2.0->en-core-web-sm==2.2.0)\n  Using cached https://files.pythonhosted.org/packages/fd/31/edaf3cdb7fcb644a51c7a568bc62a8c8d7462e61fa79b090f4d12d73d39e/srsly-1.0.2-cp37-cp37m-win_amd64.whl\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.1.3)\nCollecting thinc==7.4.1 (from spacy>=2.2.0->en-core-web-sm==2.2.0)\n  Using cached https://files.pythonhosted.org/packages/8b/6f/7cd666630afeb9b85cbd23c75da76b61580e45a9fe1c19145e3f7675ffc8/thinc-7.4.1-cp37-cp37m-win_amd64.whl\nRequirement already satisfied: numpy>=1.15.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.18.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.22.0)\nCollecting wasabi<1.1.0,>=0.4.0 (from spacy>=2.2.0->en-core-web-sm==2.2.0)\n  Using cached https://files.pythonhosted.org/packages/1b/10/55f3cf6b52cc89107b3e1b88fcf39719392b377a3d78ca61da85934d0d10/wasabi-0.8.0-py3-none-any.whl\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (0.4.1)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.0)\nRequirement already satisfied: setuptools in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (41.4.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (4.50.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (2.0.3)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy>=2.2.0->en-core-web-sm==2.2.0) (1.0.2)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (1.24.2)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.0->en-core-web-sm==2.2.0) (2.8)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.23)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (0.6.0)\nRequirement already satisfied: more-itertools in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.0->en-core-web-sm==2.2.0) (7.2.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py): started\n  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.0-cp37-none-any.whl size=12019129 sha256=a16147a4a8b868c5e4a1ff60e4d874d6ccb629d25d50525345f6cc26cbb65a09\n  Stored in directory: C:\\Users\\pilar\\AppData\\Local\\pip\\Cache\\wheels\\9c\\a3\\7c\\97125b291df1c945c944c69a235c020eb1ce4c26f72f5108ac\nSuccessfully built en-core-web-sm\nInstalling collected packages: srsly, wasabi, thinc, spacy, en-core-web-sm\nSuccessfully installed en-core-web-sm-2.2.0 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "#Comando que se usa en Python para instalar paquetes/modulos\n",
    "!pip install http://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48198,
     "status": "ok",
     "timestamp": 1600932042940,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "z5h0B-wek36Y"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " El volumen de la unidad C es OS\n El n�mero de serie del volumen es: B0E8-DAF5\n\n Directorio de c:\\Users\\pilar\\Edition_Jun2020\\Repositorios_GitHub\\activity_bootcamp_jun2020\\1_Data_Analysis\\semana_13\\d�a_1\n\n06/10/2020  21:13    <DIR>          .\n06/10/2020  21:13    <DIR>          ..\n29/09/2020  13:28    <DIR>          ficheros_clase\n06/10/2020  20:23        28.856.387 ficheros_clase.zip\n06/10/2020  20:23           103.116 SOLUCI�N_2_FeatureEngineering_CategoricalData_ejercicios .ipynb\n13/10/2020  20:46           143.214 TextData - clase.ipynb\n               3 archivos     29.102.717 bytes\n               3 dirs  21.776.674.816 bytes libres\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49349,
     "status": "ok",
     "timestamp": 1600932044096,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZhJPJ7rPk36d"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\u001b[2K✔ Loaded compatibility table\n\u001b[1m\n====================== Installed models (spaCy v2.3.2) ======================\u001b[0m\nℹ spaCy installation: C:\\Users\\pilar\\Anaconda3\\lib\\site-packages\\spacy\n\nTYPE      NAME             MODEL            VERSION    \npackage   en-core-web-sm   en_core_web_sm   2.3.1     ✔\n\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (2.3.1)\nRequirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: thinc==7.4.1 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.50.2)\nRequirement already satisfied: setuptools in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.4.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.22.0)\nRequirement already satisfied: numpy>=1.15.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.1)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.23)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.2)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2019.9.11)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.8)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\nRequirement already satisfied: more-itertools in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.2.0)\n✔ Download and installation successful\nYou can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49340,
     "status": "ok",
     "timestamp": 1600932044097,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rdXZbLgMy1Hu",
    "outputId": "eec8243f-5570-494b-c72b-3484ced876d6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\n===================== Info about model 'en_core_web_sm' =====================\u001b[0m\n\nlang             en                            \nname             core_web_sm                   \nlicense          MIT                           \nauthor           Explosion                     \nurl              https://explosion.ai          \nemail            contact@explosion.ai          \ndescription      English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\nsources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}]\npipeline         ['tagger', 'parser', 'ner']   \nversion          2.3.1                         \nspacy_version    >=2.3.0,<2.4.0                \nparent_package   spacy                         \nlabels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\nsource           C:\\Users\\pilar\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lang': 'en',\n",
       " 'name': 'core_web_sm',\n",
       " 'license': 'MIT',\n",
       " 'author': 'Explosion',\n",
       " 'url': 'https://explosion.ai',\n",
       " 'email': 'contact@explosion.ai',\n",
       " 'description': 'English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.',\n",
       " 'sources': [{'name': 'OntoNotes 5',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC2013T19',\n",
       "   'license': 'commercial (licensed by Explosion)'}],\n",
       " 'pipeline': ['tagger', 'parser', 'ner'],\n",
       " 'version': '2.3.1',\n",
       " 'spacy_version': '>=2.3.0,<2.4.0',\n",
       " 'parent_package': 'spacy',\n",
       " 'accuracy': {'las': 89.7572754092,\n",
       "  'uas': 91.6570115569,\n",
       "  'token_acc': 99.756964111,\n",
       "  'las_per_type': {'advmod': {'p': 85.6065101297,\n",
       "    'r': 84.9512113055,\n",
       "    'f': 85.2776018577},\n",
       "   'aux': {'p': 97.9464841319, 'r': 98.0772654442, 'f': 98.0118311613},\n",
       "   'nsubj': {'p': 95.530627567, 'r': 94.7522887555, 'f': 95.1398662913},\n",
       "   'root': {'p': 89.5162856958, 'r': 91.1692936754, 'f': 90.3352283866},\n",
       "   'compound': {'p': 90.4871122761, 'r': 92.2811316552, 'f': 91.3753170839},\n",
       "   'poss': {'p': 97.0346623923, 'r': 97.4838969404, 'f': 97.2587609198},\n",
       "   'case': {'p': 97.927972373, 'r': 99.3493493493, 'f': 98.6335403727},\n",
       "   'dobj': {'p': 92.4513496547, 'r': 93.8729981675, 'f': 93.1567503459},\n",
       "   'prep': {'p': 85.6642170718, 'r': 86.2427438631, 'f': 85.9525069954},\n",
       "   'pobj': {'p': 96.0694769711, 'r': 96.6428459243, 'f': 96.3553084873},\n",
       "   'relcl': {'p': 76.5768958186, 'r': 78.3538796229, 'f': 77.4551971326},\n",
       "   'det': {'p': 97.7105145232, 'r': 97.7901904024, 'f': 97.7503362269},\n",
       "   'amod': {'p': 91.5748754262, 'r': 90.4891480402, 'f': 91.0287743996},\n",
       "   'attr': {'p': 90.4294478528, 'r': 92.9772918419, 'f': 91.6856728177},\n",
       "   'cc': {'p': 83.8244137102, 'r': 83.3532647692, 'f': 83.5881753313},\n",
       "   'mark': {'p': 90.3421052632, 'r': 90.9644939057, 'f': 90.6522313177},\n",
       "   'nmod': {'p': 76.3772954925, 'r': 55.7586837294, 'f': 64.4593166608},\n",
       "   'conj': {'p': 76.8877867328, 'r': 78.0490874764, 'f': 77.4640849469},\n",
       "   'advcl': {'p': 68.9203354298, 'r': 66.2301687232, 'f': 67.548478233},\n",
       "   'pcomp': {'p': 85.2515506547, 'r': 86.6246498599, 'f': 85.9326154915},\n",
       "   'nummod': {'p': 93.1951089846, 'r': 88.5353535354, 'f': 90.8054908055},\n",
       "   'nsubjpass': {'p': 92.4265842349, 'r': 92.0, 'f': 92.2127987664},\n",
       "   'quantmod': {'p': 85.3463587922, 'r': 78.0666125102, 'f': 81.5443360204},\n",
       "   'auxpass': {'p': 94.7204968944, 'r': 97.2665148064, 'f': 95.9766239604},\n",
       "   'ccomp': {'p': 79.9260844194, 'r': 83.6863543788, 'f': 81.7630086559},\n",
       "   'npadvmod': {'p': 76.8636539204, 'r': 70.6927175844, 'f': 73.6491487787},\n",
       "   'appos': {'p': 70.0960219479, 'r': 66.5075921909, 'f': 68.2546749777},\n",
       "   'neg': {'p': 94.5082376435, 'r': 94.9824385349, 'f': 94.7447447447},\n",
       "   'xcomp': {'p': 88.2854100106, 'r': 89.2677674085, 'f': 88.7738711405},\n",
       "   'predet': {'p': 85.2459016393, 'r': 89.2703862661, 'f': 87.2117400419},\n",
       "   'acomp': {'p': 90.3553299492, 'r': 88.717716357, 'f': 89.529035208},\n",
       "   'acl': {'p': 75.6578947368, 'r': 69.012547736, 'f': 72.182596291},\n",
       "   'oprd': {'p': 81.0810810811, 'r': 71.6417910448, 'f': 76.0697305864},\n",
       "   'dative': {'p': 73.9659367397, 'r': 69.7247706422, 'f': 71.7827626919},\n",
       "   'agent': {'p': 88.5328836425, 'r': 94.0860215054, 'f': 91.2250217202},\n",
       "   'meta': {'p': 94.7368421053, 'r': 34.615384615400004, 'f': 50.7042253521},\n",
       "   'dep': {'p': 40.329218107, 'r': 15.9090909091, 'f': 22.8172293364},\n",
       "   'prt': {'p': 81.9166666667, 'r': 88.082437276, 'f': 84.8877374784},\n",
       "   'expl': {'p': 98.3014861996, 'r': 99.1434689507, 'f': 98.7206823028},\n",
       "   'parataxis': {'p': 63.9240506329, 'r': 43.8177874187, 'f': 51.9948519949},\n",
       "   'intj': {'p': 69.387755102, 'r': 59.7802197802, 'f': 64.2266824085},\n",
       "   'csubj': {'p': 70.5882352941, 'r': 71.0059171598, 'f': 70.796460177},\n",
       "   'preconj': {'p': 57.4468085106, 'r': 62.7906976744, 'f': 60.0},\n",
       "   'csubjpass': {'p': 44.4444444444, 'r': 66.6666666667, 'f': 53.3333333333}},\n",
       "  'tags_acc': 97.056555292,\n",
       "  'ents_f': 85.4306864065,\n",
       "  'ents_p': 85.7239322492,\n",
       "  'ents_r': 85.1394400045,\n",
       "  'ents_per_type': {'ORG': {'p': 83.3194096352,\n",
       "    'r': 82.8808864266,\n",
       "    'f': 83.0995695042},\n",
       "   'CARDINAL': {'p': 83.9554682384, 'r': 86.32996633, 'f': 85.1261620186},\n",
       "   'DATE': {'p': 84.6522781775, 'r': 86.1275705821, 'f': 85.3835521769},\n",
       "   'GPE': {'p': 92.5831202046, 'r': 90.2180685358, 'f': 91.3852950458},\n",
       "   'PERSON': {'p': 88.0239520958, 'r': 92.0908379013, 'f': 90.0114810563},\n",
       "   'MONEY': {'p': 92.9181929182, 'r': 91.4663461538, 'f': 92.1865536039},\n",
       "   'PRODUCT': {'p': 52.1276595745, 'r': 24.1379310345, 'f': 32.9966329966},\n",
       "   'TIME': {'p': 70.1886792453, 'r': 70.9923664122, 'f': 70.5882352941},\n",
       "   'PERCENT': {'p': 91.8566775244, 'r': 88.125, 'f': 89.95215311},\n",
       "   'WORK_OF_ART': {'p': 48.1481481481, 'r': 38.8059701493, 'f': 42.9752066116},\n",
       "   'QUANTITY': {'p': 78.1954887218, 'r': 65.4088050314, 'f': 71.2328767123},\n",
       "   'NORP': {'p': 88.682581786, 'r': 89.2348754448, 'f': 88.9578713969},\n",
       "   'LOC': {'p': 70.8154506438, 'r': 66.0, 'f': 68.3229813665},\n",
       "   'EVENT': {'p': 63.2911392405, 'r': 37.037037037, 'f': 46.7289719626},\n",
       "   'ORDINAL': {'p': 80.0, 'r': 83.9694656489, 'f': 81.9366852886},\n",
       "   'FAC': {'p': 34.8623853211, 'r': 46.9135802469, 'f': 40.0},\n",
       "   'LAW': {'p': 62.962962963, 'r': 56.6666666667, 'f': 59.649122807},\n",
       "   'LANGUAGE': {'p': 75.0, 'r': 65.2173913043, 'f': 69.7674418605}}},\n",
       " 'speed': {'cpu': 6107.3535050376, 'gpu': None, 'nwords': 291315},\n",
       " 'labels': {'tagger': ['$',\n",
       "   \"''\",\n",
       "   ',',\n",
       "   '-LRB-',\n",
       "   '-RRB-',\n",
       "   '.',\n",
       "   ':',\n",
       "   'ADD',\n",
       "   'AFX',\n",
       "   'CC',\n",
       "   'CD',\n",
       "   'DT',\n",
       "   'EX',\n",
       "   'FW',\n",
       "   'HYPH',\n",
       "   'IN',\n",
       "   'JJ',\n",
       "   'JJR',\n",
       "   'JJS',\n",
       "   'LS',\n",
       "   'MD',\n",
       "   'NFP',\n",
       "   'NN',\n",
       "   'NNP',\n",
       "   'NNPS',\n",
       "   'NNS',\n",
       "   'PDT',\n",
       "   'POS',\n",
       "   'PRP',\n",
       "   'PRP$',\n",
       "   'RB',\n",
       "   'RBR',\n",
       "   'RBS',\n",
       "   'RP',\n",
       "   'SYM',\n",
       "   'TO',\n",
       "   'UH',\n",
       "   'VB',\n",
       "   'VBD',\n",
       "   'VBG',\n",
       "   'VBN',\n",
       "   'VBP',\n",
       "   'VBZ',\n",
       "   'WDT',\n",
       "   'WP',\n",
       "   'WP$',\n",
       "   'WRB',\n",
       "   'XX',\n",
       "   '_SP',\n",
       "   '``'],\n",
       "  'parser': ['ROOT',\n",
       "   'acl',\n",
       "   'acomp',\n",
       "   'advcl',\n",
       "   'advmod',\n",
       "   'agent',\n",
       "   'amod',\n",
       "   'appos',\n",
       "   'attr',\n",
       "   'aux',\n",
       "   'auxpass',\n",
       "   'case',\n",
       "   'cc',\n",
       "   'ccomp',\n",
       "   'compound',\n",
       "   'conj',\n",
       "   'csubj',\n",
       "   'csubjpass',\n",
       "   'dative',\n",
       "   'dep',\n",
       "   'det',\n",
       "   'dobj',\n",
       "   'expl',\n",
       "   'intj',\n",
       "   'mark',\n",
       "   'meta',\n",
       "   'neg',\n",
       "   'nmod',\n",
       "   'npadvmod',\n",
       "   'nsubj',\n",
       "   'nsubjpass',\n",
       "   'nummod',\n",
       "   'oprd',\n",
       "   'parataxis',\n",
       "   'pcomp',\n",
       "   'pobj',\n",
       "   'poss',\n",
       "   'preconj',\n",
       "   'predet',\n",
       "   'prep',\n",
       "   'prt',\n",
       "   'punct',\n",
       "   'quantmod',\n",
       "   'relcl',\n",
       "   'xcomp'],\n",
       "  'ner': ['CARDINAL',\n",
       "   'DATE',\n",
       "   'EVENT',\n",
       "   'FAC',\n",
       "   'GPE',\n",
       "   'LANGUAGE',\n",
       "   'LAW',\n",
       "   'LOC',\n",
       "   'MONEY',\n",
       "   'NORP',\n",
       "   'ORDINAL',\n",
       "   'ORG',\n",
       "   'PERCENT',\n",
       "   'PERSON',\n",
       "   'PRODUCT',\n",
       "   'QUANTITY',\n",
       "   'TIME',\n",
       "   'WORK_OF_ART']},\n",
       " 'source': 'C:\\\\Users\\\\pilar\\\\Anaconda3\\\\lib\\\\site-packages\\\\en_core_web_sm'}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# model meta data\n",
    "spacy.info('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49325,
     "status": "ok",
     "timestamp": 1600932044102,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Crcw9hW2y1Hy",
    "outputId": "80f605e5-5c7a-4fa5-ec1a-fe7cecb0e062"
   },
   "outputs": [],
   "source": [
    "# Keeping it in a pandas dataframe\n",
    "doc_df = review_df['text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49693,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ptcmeIO-y1Hz",
    "outputId": "ccd56103-25c4-4066-897c-81f4bdfaddee"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "type(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49681,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "zBOS4XxBk362",
    "outputId": "819e7428-794f-4a51-ec54-cfa7826227d1"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Oh happy day, finally have a Canes near my casa. Yes just as others are griping about the Drive thru is packed just like most of the other canes in the area but I like to go sit down to enjoy my chicken. The cashiers are pleasant and as far as food wise i have yet to receive any funky chicken. The clean up crew zips around the dining area constantly so it's usually well kept. My only gripe is the one fella with Red hair he makes the rounds while cleaning but no smile or personality a few nights ago he tossed the napkins i just put on the table to help go with my meal. After I was done he just reached for my tray no \"excuse me or are you done with that?\"  I realize he's trying to do his job quickly but a little table manners goes along way. That being said still like to grub here and glad that there's finally a Cane's close to me."
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "doc_df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49670,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yBh2DQmny1H4",
    "outputId": "1cfd36cd-2b67-45be-d7e5-ed339147013d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\nOh INTJ UH\nhappy ADJ JJ\nday NOUN NN\n, PUNCT ,\nfinally ADV RB\nhave AUX VBP\na DET DT\nCanes PROPN NNPS\nnear SCONJ IN\nmy DET PRP$\ncasa NOUN NN\n. PUNCT .\nYes INTJ UH\njust ADV RB\nas SCONJ IN\nothers NOUN NNS\nare AUX VBP\ngriping VERB VBG\nabout ADP IN\nthe DET DT\nDrive NOUN NN\nthru NOUN NN\nis AUX VBZ\npacked VERB VBN\njust ADV RB\nlike SCONJ IN\nmost ADJ JJS\nof ADP IN\nthe DET DT\nother ADJ JJ\ncanes NOUN NNS\nin ADP IN\nthe DET DT\narea NOUN NN\nbut CCONJ CC\nI PRON PRP\nlike VERB VBP\nto PART TO\ngo VERB VB\nsit VERB VB\ndown ADP RP\nto PART TO\nenjoy VERB VB\nmy DET PRP$\nchicken NOUN NN\n. PUNCT .\nThe DET DT\ncashiers NOUN NNS\nare AUX VBP\npleasant ADJ JJ\nand CCONJ CC\nas ADV RB\nfar ADV RB\nas SCONJ IN\nfood NOUN NN\nwise ADJ JJ\ni PRON PRP\nhave AUX VBP\nyet ADV RB\nto PART TO\nreceive VERB VB\nany DET DT\nfunky ADJ JJ\nchicken NOUN NN\n. PUNCT .\nThe DET DT\nclean ADJ JJ\nup ADP RP\ncrew NOUN NN\nzips NOUN NNS\naround ADP IN\nthe DET DT\ndining NOUN NN\narea NOUN NN\nconstantly ADV RB\nso SCONJ IN\nit PRON PRP\n's AUX VBZ\nusually ADV RB\nwell ADV RB\nkept VERB VBN\n. PUNCT .\nMy DET PRP$\nonly ADJ JJ\ngripe NOUN NN\nis AUX VBZ\nthe DET DT\none NUM CD\nfella NOUN NN\nwith ADP IN\nRed ADJ JJ\nhair NOUN NN\nhe PRON PRP\nmakes VERB VBZ\nthe DET DT\nrounds NOUN NNS\nwhile SCONJ IN\ncleaning VERB VBG\nbut CCONJ CC\nno DET DT\nsmile NOUN NN\nor CCONJ CC\npersonality NOUN NN\na DET DT\nfew ADJ JJ\nnights NOUN NNS\nago ADV RB\nhe PRON PRP\ntossed VERB VBD\nthe DET DT\nnapkins NOUN NNS\ni PRON PRP\njust ADV RB\nput VERB VBD\non ADP IN\nthe DET DT\ntable NOUN NN\nto PART TO\nhelp VERB VB\ngo VERB VB\nwith ADP IN\nmy DET PRP$\nmeal NOUN NN\n. PUNCT .\nAfter ADP IN\nI PRON PRP\nwas AUX VBD\ndone VERB VBN\nhe PRON PRP\njust ADV RB\nreached VERB VBN\nfor ADP IN\nmy DET PRP$\ntray NOUN NN\nno INTJ UH\n\" PUNCT ``\nexcuse VERB VB\nme PRON PRP\nor CCONJ CC\nare AUX VBP\nyou PRON PRP\ndone VERB VBN\nwith ADP IN\nthat DET DT\n? PUNCT .\n\" PUNCT ''\n  SPACE _SP\nI PRON PRP\nrealize VERB VBP\nhe PRON PRP\n's AUX VBZ\ntrying VERB VBG\nto PART TO\ndo AUX VB\nhis DET PRP$\njob NOUN NN\nquickly ADV RB\nbut CCONJ CC\na DET DT\nlittle ADJ JJ\ntable NOUN NN\nmanners NOUN NNS\ngoes VERB VBZ\nalong ADP IN\nway NOUN NN\n. PUNCT .\nThat DET DT\nbeing AUX VBG\nsaid VERB VBN\nstill ADV RB\nlike INTJ UH\nto PART TO\ngrub PROPN NNP\nhere ADV RB\nand CCONJ CC\nglad ADJ JJ\nthat SCONJ IN\nthere PRON EX\n's AUX VBZ\nfinally ADV RB\na DET DT\nCane PROPN NNP\n's PART POS\nclose NOUN NN\nto ADP IN\nme PRON PRP\n. PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# spacy gives you both fine grained (.pos_) + coarse grained (.tag_) parts of speech  \n",
    "\n",
    "print(type(doc_df[4]))\n",
    "for doc in doc_df[4]:\n",
    "    print(doc.text, doc.pos_ , doc.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49657,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "FaC3G5Hck37F",
    "outputId": "ec23e333-eda5-489d-b807-bbd5950962cb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'interjection'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# para buscar a qué se refiere\n",
    "spacy.explain('UH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49645,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Hqvy2dtby1H5",
    "outputId": "acb58a04-6810-4298-90d2-92b1e3e715eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[a Canes, my casa, others, the Drive thru, the other canes, the area, I, my chicken, The cashiers, food, i, any funky chicken, The clean up crew zips, the dining area, it, My only gripe, the one fella, Red hair, he, the rounds, no smile, personality, he, the napkins, i, the table, my meal, I, he, my tray, me, you, I, he, his job, a little table manners, way, grub, a Cane, me]\n"
     ]
    }
   ],
   "source": [
    "# spaCy also does noun chunking for us\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpFwoavpy1H7"
   },
   "source": [
    "### Using [Textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49636,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "vZk8yu9Mk37P"
   },
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49630,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "RTCz0Q_Ty1H7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpIJX8L-y1H8"
   },
   "source": [
    "The default tagger in TextBlob uses the PatternTagger, the same as [pattern](https://www.clips.uantwerpen.be/pattern), which is fine for our example. To use the NLTK tagger, we can specify the pos_tagger when we call TextBlob. More [here](http://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49622,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ph5GAFaIy1H9",
    "outputId": "26d339d0-f3a1-4110-ad19-d5d66097ce4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49609,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "bNTIxc1_y1H-",
    "outputId": "e56603d3-8989-4f40-dc27-e6a7477206e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50210,
     "status": "ok",
     "timestamp": 1600932045099,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "I7RvQOyrBsz3",
    "outputId": "e9893181-c68d-4acb-fd22-a6ad9d9cc385"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50606,
     "status": "ok",
     "timestamp": 1600932045508,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "i2vgNTXHy1IA",
    "outputId": "ed653587-1a0d-4211-fa6a-d17abb0f0d42"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d0U14jiy1IC"
   },
   "source": [
    "# The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n",
    "\n",
    "A bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n",
    "\n",
    "It would also be nice to pick out words such as “magnificently,” “gleamed,” “intimidated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n",
    "Tf-Idf : A Simple Twist on Bag-of-Words\n",
    "\n",
    "Tf-Idf: Term frequency Inverse document frequency\n",
    "\n",
    "Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency–inverse document frequency.  Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n",
    "\n",
    "bow(w, d) = # times word w appears in document d\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n",
    "\n",
    "N is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n",
    "\n",
    "Alternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n",
    "\n",
    "If we define tf-idf as:\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n",
    "\n",
    "then a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n",
    "\n",
    "Let’s look at some pictures to understand what it’s all about. Figure 4-1 shows a simple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1TRAWrbJu7VHEteoJDD523yKZ7fo_W1AY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30-014awy1IC"
   },
   "source": [
    "Now let’s look at the same four sentences in tf-idf representation using the log transform for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filtering methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TdYrbPGy1IC"
   },
   "source": [
    "The GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1lV4DHIu6tGpt2gXKPQJbdaw8bIj7767L)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB_PROFE_TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}