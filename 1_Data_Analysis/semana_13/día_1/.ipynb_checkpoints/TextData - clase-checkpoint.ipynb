{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26271,
     "status": "ok",
     "timestamp": 1600932020606,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "1FahTQE7mP6B",
    "outputId": "4c8acd28-b10c-487a-dac1-0750ab5ca06e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2O0nELPy1Gy"
   },
   "source": [
    "#Text Analysis\n",
    "-----\n",
    "\n",
    "## Text Data: Flattening, Filtering, and Chunking\n",
    "\n",
    "What would you do if you were designing an algorithm to analyze the following paragraph of text?\n",
    "\n",
    "    Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “Hello?”\n",
    "\n",
    "The paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n",
    "\n",
    "So, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main characters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n",
    "\n",
    "This chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling technique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2TIF9lMy1Gz"
   },
   "source": [
    "## Bag-of-X: Turning Natural Text into Flat Vectors\n",
    "\n",
    "Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and interpretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n",
    "For text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This technique can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indicator of the topic content of the document.\n",
    "\n",
    "Bag-of-Words\n",
    "\n",
    "In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corresponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZG2s2_51y1G0"
   },
   "source": [
    "![texto alternativo](https://drive.google.com/uc?id=1IbLBEuoCaN8E2ct7788gNOqskefXmUQ4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gplms5HDy1G0"
   },
   "source": [
    "Bag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words representation, these words are all equal elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "568KZ9p6y1G1"
   },
   "source": [
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1M9wfn4eNsV09yTGNsJIUlYOZv6DyOFss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F-3oF-ty1G2"
   },
   "source": [
    "What is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1V2N1rOUXJIqZndF9bxDkPifxYXkD8Rlp)\n",
    "\n",
    "Figure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1YHNStJX1ru7YAKmh6KYcySvW5B7TCuR_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYHN-3OTy1G2"
   },
   "source": [
    "These figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at feature vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vectors. Figure 3-5 shows an example. With bag-of-words featurization for text documents, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.”  As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix transpose of the bag-of-words vectors.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1-kP58QvDsOB64ho-29yOWUegXzcHXQFy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUR9ybpTy1G3"
   },
   "source": [
    "Bag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating negation plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n",
    "\n",
    "## Bag-of-n-Grams\n",
    "\n",
    "Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n",
    "\n",
    "n-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n",
    "\n",
    "To illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26256,
     "status": "ok",
     "timestamp": 1600932020607,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_g2co9boy1G4"
   },
   "outputs": [],
   "source": [
    "# importamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34020,
     "status": "ok",
     "timestamp": 1600932028382,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "5oElc6KLy1G7",
    "outputId": "7f1535d5-8d44-4c22-d053-64338e17af6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbiz_f = open('yelp_academic_dataset_business.json', encoding='latin1')\\nbiz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\\nbiz_f.close()\\nbiz_df.shape\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "biz_f = open('yelp_academic_dataset_business.json', encoding='latin1')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "biz_df.shape\n",
    "'''\n",
    "\n",
    "# biz_df = pd.read_csv('biz.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34005,
     "status": "ok",
     "timestamp": 1600932028383,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "KM-_yICVy1G-",
    "outputId": "48f148fa-8b92-44cf-8623-333d02487ea3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35363,
     "status": "ok",
     "timestamp": 1600932029756,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Rf6MS0iOy1HA",
    "outputId": "11ac8ceb-d6e8-4685-9166-c0311cf28577"
   },
   "outputs": [],
   "source": [
    "# Load the first 10,000 reviews\n",
    "\n",
    "'''\n",
    "# f = open('drive/My Drive/Colab Notebooks/yelp_academic_dataset_review.json', encoding='latin1')\n",
    "f = open('yelp_academic_dataset_review.json', encoding='latin1')\n",
    "js = []\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "'''\n",
    "\n",
    "review_df = pd.read_csv('review.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35344,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "2xZN2wy1y1HD",
    "outputId": "7c508d93-870b-4847-ca41-49b4f2e0872a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35328,
     "status": "ok",
     "timestamp": 1600932029757,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "07mKeA-ry1HF",
    "outputId": "6c443116-065e-41b5-c576-0693f0caccff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35698,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yFLbM0jry1HI"
   },
   "outputs": [],
   "source": [
    "# uso CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35688,
     "status": "ok",
     "timestamp": 1600932030138,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "937zuVELy1HK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36593,
     "status": "ok",
     "timestamp": 1600932031068,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "1DX1OjyHy1HM",
    "outputId": "3e98fd9f-0235-4b62-96f3-c854432de8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x26227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 716565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36571,
     "status": "ok",
     "timestamp": 1600932031068,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "u_3Xg2O7y1HO",
    "outputId": "babc1b92-d0b5-4ad6-a5d9-893af15baff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26227"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36557,
     "status": "ok",
     "timestamp": 1600932031069,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZLDYBINRy1HR",
    "outputId": "3b70182c-0734-4f44-ba58-5b05390b19b0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# los 10 primeros elementos de la lista, de 0 a 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39682,
     "status": "ok",
     "timestamp": 1600932034204,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rZZTnmvwy1HT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tomamos las palabras solo de 2 en 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39672,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "VEfqqsFny1HV",
    "outputId": "6aabf05f-1d4f-4223-e487-af03ad0cda52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39655,
     "status": "ok",
     "timestamp": 1600932034206,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "R2Rkf4H2y1HX",
    "outputId": "3a1ea27e-5649-49a9-b5b1-9c1aaf6c000f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45154,
     "status": "ok",
     "timestamp": 1600932039720,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "tymksgvRy1HZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46275,
     "status": "ok",
     "timestamp": 1600932040853,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "6MwUtblry1Hb",
    "outputId": "58d0f375-5068-4e77-e9e4-c0ebdb9abb15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46259,
     "status": "ok",
     "timestamp": 1600932040854,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_v--lpqsy1He",
    "outputId": "ecfa30c9-feeb-4ccf-ed7f-dc8fd8fb9e85"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46245,
     "status": "ok",
     "timestamp": 1600932040855,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "lU98Vv6py1Hg",
    "outputId": "59be51c1-0e87-4796-8d24-32bb5551ede8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46230,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "uMTx-NPXk35m",
    "outputId": "4e3d4afb-e365-4ce6-a6fd-71684060187f"
   },
   "outputs": [],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46215,
     "status": "ok",
     "timestamp": 1600932040856,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DbGhd75Xk35t",
    "outputId": "80a7b7da-8902-4d97-81a2-3ed6f531b13a"
   },
   "outputs": [],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46198,
     "status": "ok",
     "timestamp": 1600932040857,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "_IgcngOmk35y",
    "outputId": "7e17edd7-95d5-4456-bf86-a7b9bc16640a"
   },
   "outputs": [],
   "source": [
    "# Entendiendo bien las palabras, bigramas y trigramas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46584,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "DXeh7ui_y1Hi",
    "outputId": "c500d115-a727-4789-a252-ed3822d0567e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas.util.testing as tm\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46568,
     "status": "ok",
     "timestamp": 1600932041260,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "EWo__Xoxy1Hk",
    "outputId": "15920c9f-3b35-4a62-a957-6c793f923054"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sns.set_style(\"darkgrid\")\n",
    "counts = [len(words), len(bigrams), len(trigrams)]\n",
    "plt.plot(counts, color='cornflowerblue')\n",
    "plt.plot(counts, 'bo')\n",
    "plt.margins(0.1)\n",
    "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Number of ngrams in the first 10,000 reviews of the Yelp dataset', {'fontsize':16})\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSEanGNOy1Hl"
   },
   "source": [
    "## Filtering for Cleaner Features\n",
    "\n",
    "With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n",
    "\n",
    "The popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47445,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Qq7BEgiXy1Hm",
    "outputId": "66490902-4a28-44f9-9617-d8382f4393f9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04kSK7ISy1Ho"
   },
   "source": [
    "Note that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n",
    "\n",
    "## Frequency-Based Filtering\n",
    "\n",
    "Stopword lists are a way of weeding out common words that make for vacuous features. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use frequency statistics here as well.\n",
    "\n",
    "### Frequent words\n",
    "\n",
    "Frequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n",
    "\n",
    "Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, frequency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also contains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=10t_e8ghb0288snEPVr1TdWDjBlhiYxjW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VgLLbbshy1Ho"
   },
   "source": [
    "In practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no universal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n",
    "\n",
    "### Rare words\n",
    "\n",
    "Depending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful information. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar? Even if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledygook.”\n",
    "\n",
    "Not only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (tokenized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n",
    "\n",
    "Rare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further filtered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1jIU48-yVo2peJryvIvLWdVodVROWxJ5h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JeLaC3qZy1Ho"
   },
   "source": [
    "Since one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n",
    "\n",
    "Since this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely contains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n",
    "\n",
    "\n",
    "## Stemming\n",
    "\n",
    "One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically different tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n",
    "\n",
    "Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tagging and linguistic rules in a process known as lemmatization.\n",
    "\n",
    "Most stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n",
    "\n",
    "Here is an example of running the Porter stemmer through the NLTK Python package. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=19m3iQWTeyzgVMZ_ncjtwJL2vTOGWzZYi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgSOAY6Py1Hp"
   },
   "source": [
    "Stemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n",
    "\n",
    "\n",
    "## Atoms of Meaning: From Words to n-Grams to Phrases\n",
    "\n",
    "The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n",
    "\n",
    "### Parsing and Tokenization\n",
    "\n",
    "Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n",
    "\n",
    "After light parsing, the plain-text portion of the document can go through tokenization. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what characters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n",
    "\n",
    "Sometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n",
    "\n",
    "### Collocation Extraction for Phrase Detection\n",
    "\n",
    "\n",
    "A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things.”\n",
    "\n",
    "Collocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a collocation.\n",
    "\n",
    "Collocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n",
    "\n",
    "Because collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a representation. Bag-of-n-grams is also problematic because it captures too many meaningless sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n",
    "\n",
    "Collocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n",
    "\n",
    "Since the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n",
    "\n",
    "### Frequency-based methods\n",
    "\n",
    "A simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams ( n = 2 ) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1qKEnujZ3oa6Mph8_CuqB7c1Xb0rbF6Dk)\n",
    "\n",
    "### Hypothesis testing for collocation extraction\n",
    "\n",
    "Raw popularity count is too crude of a measure. We have to find more clever statistics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n",
    "\n",
    "Hypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The randomness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n",
    "\n",
    "For example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p-Values.\n",
    "\n",
    "In the context of collocation extraction, many hypothesis tests have been proposed over the years.  One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following question: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n",
    "\n",
    "That is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n",
    "\n",
    "We can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n",
    "\n",
    "The final statistic is the log of the ratio between the two:\n",
    "\n",
    "log λ = log L ( Data; H null ) L ( Data; H alternate ) . #formatear\n",
    "\n",
    "The likelihood function L(Data; H) represents the probability of seeing the word frequencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strategy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probability.\n",
    "\n",
    "The algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n",
    "\n",
    "    Compute occurrence probabilities for all singleton words: P(w).\n",
    "    Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n",
    "    Compute the likelihood ratio log λ for all unique bigrams.\n",
    "    Sort the bigrams based on their likelihood ratio.\n",
    "    Take the bigrams with the smallest likelihood ratio values as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKpifucIy1Hp"
   },
   "source": [
    "There is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text corpora. Hence, it is not commonly used and we will not be demonstrating it here.\n",
    "\n",
    "Note that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filtering. To generate longer phrases, there are other methods such as chunking or combining with part-of-speech (PoS) tagging.\n",
    "\n",
    "\n",
    "### Chunking and part-of-speech tagging\n",
    "\n",
    "Chunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n",
    "\n",
    "For example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple language models available. \n",
    "\n",
    "To  illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47428,
     "status": "ok",
     "timestamp": 1600932042153,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ubFxtO5Yy1Hp",
    "outputId": "3c63dd4f-383c-496b-baaf-1a5054ea189d"
   },
   "outputs": [],
   "source": [
    "# Load the first 10 reviews\n",
    "'''\n",
    "f = open('yelp_academic_dataset_review.json')\n",
    "js = []\n",
    "for i in range(10):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape\n",
    "'''\n",
    "# review_df = pd.read_csv('review_10.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "he56IUUfy1Hs"
   },
   "source": [
    "### Using spacy: [Installation instructions for spacy](https://spacy.io/docs/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47418,
     "status": "ok",
     "timestamp": 1600932042154,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Ss9_u5wfy1Hs"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48198,
     "status": "ok",
     "timestamp": 1600932042940,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "z5h0B-wek36Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49349,
     "status": "ok",
     "timestamp": 1600932044096,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ZhJPJ7rPk36d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49340,
     "status": "ok",
     "timestamp": 1600932044097,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "rdXZbLgMy1Hu",
    "outputId": "eec8243f-5570-494b-c72b-3484ced876d6"
   },
   "outputs": [],
   "source": [
    "# model meta data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49325,
     "status": "ok",
     "timestamp": 1600932044102,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Crcw9hW2y1Hy",
    "outputId": "80f605e5-5c7a-4fa5-ec1a-fe7cecb0e062"
   },
   "outputs": [],
   "source": [
    "# Keeping it in a pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49693,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ptcmeIO-y1Hz",
    "outputId": "ccd56103-25c4-4066-897c-81f4bdfaddee"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49681,
     "status": "ok",
     "timestamp": 1600932044483,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "zBOS4XxBk362",
    "outputId": "819e7428-794f-4a51-ec54-cfa7826227d1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49670,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "yBh2DQmny1H4",
    "outputId": "1cfd36cd-2b67-45be-d7e5-ed339147013d"
   },
   "outputs": [],
   "source": [
    "# spacy gives you both fine grained (.pos_) + coarse grained (.tag_) parts of speech    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49657,
     "status": "ok",
     "timestamp": 1600932044484,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "FaC3G5Hck37F",
    "outputId": "ec23e333-eda5-489d-b807-bbd5950962cb"
   },
   "outputs": [],
   "source": [
    "# para buscar a qué se refiere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49645,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "Hqvy2dtby1H5",
    "outputId": "acb58a04-6810-4298-90d2-92b1e3e715eb"
   },
   "outputs": [],
   "source": [
    "# spaCy also does noun chunking for us\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpFwoavpy1H7"
   },
   "source": [
    "### Using [Textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49636,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "vZk8yu9Mk37P"
   },
   "outputs": [],
   "source": [
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49630,
     "status": "ok",
     "timestamp": 1600932044485,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "RTCz0Q_Ty1H7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpIJX8L-y1H8"
   },
   "source": [
    "The default tagger in TextBlob uses the PatternTagger, the same as [pattern](https://www.clips.uantwerpen.be/pattern), which is fine for our example. To use the NLTK tagger, we can specify the pos_tagger when we call TextBlob. More [here](http://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49622,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "ph5GAFaIy1H9",
    "outputId": "26d339d0-f3a1-4110-ad19-d5d66097ce4f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49609,
     "status": "ok",
     "timestamp": 1600932044486,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "bNTIxc1_y1H-",
    "outputId": "e56603d3-8989-4f40-dc27-e6a7477206e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50210,
     "status": "ok",
     "timestamp": 1600932045099,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "I7RvQOyrBsz3",
    "outputId": "e9893181-c68d-4acb-fd22-a6ad9d9cc385"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50606,
     "status": "ok",
     "timestamp": 1600932045508,
     "user": {
      "displayName": "Alberto Romero",
      "photoUrl": "",
      "userId": "13942113647740663414"
     },
     "user_tz": -120
    },
    "id": "i2vgNTXHy1IA",
    "outputId": "ed653587-1a0d-4211-fa6a-d17abb0f0d42"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0d0U14jiy1IC"
   },
   "source": [
    "# The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n",
    "\n",
    "A bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n",
    "\n",
    "It would also be nice to pick out words such as “magnificently,” “gleamed,” “intimidated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n",
    "Tf-Idf : A Simple Twist on Bag-of-Words\n",
    "\n",
    "Tf-Idf: Term frequency Inverse document frequency\n",
    "\n",
    "Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency–inverse document frequency.  Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n",
    "\n",
    "bow(w, d) = # times word w appears in document d\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n",
    "\n",
    "N is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n",
    "\n",
    "Alternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n",
    "\n",
    "If we define tf-idf as:\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n",
    "\n",
    "then a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n",
    "\n",
    "Let’s look at some pictures to understand what it’s all about. Figure 4-1 shows a simple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1TRAWrbJu7VHEteoJDD523yKZ7fo_W1AY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30-014awy1IC"
   },
   "source": [
    "Now let’s look at the same four sentences in tf-idf representation using the log transform for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filtering methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TdYrbPGy1IC"
   },
   "source": [
    "The GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n",
    "\n",
    "![texto alternativo](https://drive.google.com/uc?id=1lV4DHIu6tGpt2gXKPQJbdaw8bIj7767L)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB_PROFE_TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
