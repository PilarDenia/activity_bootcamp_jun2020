{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
    "\n",
    "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out.\n",
    "\n",
    "**Import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = []\n",
    "for line in open('./full_train.txt', 'r', encoding='latin1'):    \n",
    "    reviews_train.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open('./full_test.txt', 'r', encoding='latin1'):\n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See one of the elements in the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "##### CODE #####\n",
    "reviews_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw text is pretty messy for these reviews so before we can do any analytics we need to clean things up\n",
    "\n",
    "\n",
    "**Use Regular expressions to remove the non text characters, and the html tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews] #.sub() sustituir\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"this isn't the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robin's new love of the thriller but this isn't a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until william's character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all it's worth a watch though it's definitely not friday saturday night fare it rates a   from the fiend \""
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "reviews_train_clean[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "In order for this data to make sense to our machine learning algorithm we’ll need to convert each review to a numeric representation, which we call vectorization.\n",
    "\n",
    "The simplest form of this is to create one very large matrix with one column for every unique word in your corpus (where the corpus is all 50k reviews in our case). Then we transform each review into one row containing 0s and 1s, where 1 means that the word in the corpus corresponding to that column appears in that review. That being said, each row of the matrix will be very sparse (mostly zeros). This process is also known as one hot encoding. Use the *CountVectorizer* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos palabra a numeros para posicionarlas en un espacio multidimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE #####\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Si aparece una palabra en una reseña. le pone un 1.CountVectorizer\n",
    "#Da igual que aparezca 100 veces, no cuenta\n",
    "\n",
    "baseline_vectorizer = CountVectorizer(binary= True)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "#reseñas en formato vector de palabras\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bromwell': 9819,\n",
       " 'high': 35211,\n",
       " 'is': 39472,\n",
       " 'cartoon': 11686,\n",
       " 'comedy': 14754,\n",
       " 'it': 39642,\n",
       " 'ran': 61772,\n",
       " 'at': 4537,\n",
       " 'the': 76725,\n",
       " 'same': 66138,\n",
       " 'time': 77626,\n",
       " 'as': 4211,\n",
       " 'some': 71188,\n",
       " 'other': 54861,\n",
       " 'programs': 60156,\n",
       " 'about': 284,\n",
       " 'school': 67025,\n",
       " 'life': 44297,\n",
       " 'such': 74177,\n",
       " 'teachers': 75997,\n",
       " 'my': 51490,\n",
       " 'years': 86260,\n",
       " 'in': 37733,\n",
       " 'teaching': 76000,\n",
       " 'profession': 60088,\n",
       " 'lead': 43605,\n",
       " 'me': 47976,\n",
       " 'to': 77922,\n",
       " 'believe': 6894,\n",
       " 'that': 76671,\n",
       " 'satire': 66462,\n",
       " 'much': 51018,\n",
       " 'closer': 14125,\n",
       " 'reality': 62242,\n",
       " 'than': 76643,\n",
       " 'scramble': 67253,\n",
       " 'survive': 74812,\n",
       " 'financially': 27904,\n",
       " 'insightful': 38615,\n",
       " 'students': 73768,\n",
       " 'who': 84627,\n",
       " 'can': 11132,\n",
       " 'see': 67695,\n",
       " 'right': 64476,\n",
       " 'through': 77369,\n",
       " 'their': 76762,\n",
       " 'pathetic': 56362,\n",
       " 'pomp': 58787,\n",
       " 'pettiness': 57341,\n",
       " 'of': 53843,\n",
       " 'whole': 84639,\n",
       " 'situation': 69959,\n",
       " 'all': 2038,\n",
       " 'remind': 63323,\n",
       " 'schools': 67049,\n",
       " 'knew': 42226,\n",
       " 'and': 2762,\n",
       " 'when': 84450,\n",
       " 'saw': 66588,\n",
       " 'episode': 24853,\n",
       " 'which': 84476,\n",
       " 'student': 73764,\n",
       " 'repeatedly': 63494,\n",
       " 'tried': 79132,\n",
       " 'burn': 10435,\n",
       " 'down': 22050,\n",
       " 'immediately': 37436,\n",
       " 'recalled': 62427,\n",
       " 'classic': 13840,\n",
       " 'line': 44572,\n",
       " 'inspector': 38661,\n",
       " 'here': 34881,\n",
       " 'sack': 65826,\n",
       " 'one': 54223,\n",
       " 'your': 86541,\n",
       " 'welcome': 84135,\n",
       " 'expect': 25952,\n",
       " 'many': 46877,\n",
       " 'adults': 1131,\n",
       " 'age': 1445,\n",
       " 'think': 77056,\n",
       " 'far': 26802,\n",
       " 'fetched': 27446,\n",
       " 'what': 84379,\n",
       " 'pity': 58038,\n",
       " 'isn': 39565,\n",
       " 'homelessness': 35859,\n",
       " 'or': 54556,\n",
       " 'houselessness': 36368,\n",
       " 'george': 30901,\n",
       " 'carlin': 11519,\n",
       " 'stated': 72756,\n",
       " 'has': 34120,\n",
       " 'been': 6660,\n",
       " 'an': 2670,\n",
       " 'issue': 39614,\n",
       " 'for': 28846,\n",
       " 'but': 10574,\n",
       " 'never': 52363,\n",
       " 'plan': 58141,\n",
       " 'help': 34743,\n",
       " 'those': 77231,\n",
       " 'on': 54196,\n",
       " 'street': 73546,\n",
       " 'were': 84257,\n",
       " 'once': 54207,\n",
       " 'considered': 15688,\n",
       " 'human': 36593,\n",
       " 'did': 20215,\n",
       " 'everything': 25523,\n",
       " 'from': 29698,\n",
       " 'going': 31740,\n",
       " 'work': 85510,\n",
       " 'vote': 83129,\n",
       " 'matter': 47565,\n",
       " 'most': 50516,\n",
       " 'people': 56856,\n",
       " 'homeless': 35858,\n",
       " 'just': 41023,\n",
       " 'lost': 45327,\n",
       " 'cause': 12043,\n",
       " 'while': 84492,\n",
       " 'worrying': 85625,\n",
       " 'things': 77032,\n",
       " 'racism': 61457,\n",
       " 'war': 83515,\n",
       " 'iraq': 39359,\n",
       " 'pressuring': 59700,\n",
       " 'kids': 41828,\n",
       " 'succeed': 74135,\n",
       " 'technology': 76091,\n",
       " 'elections': 23680,\n",
       " 'inflation': 38309,\n",
       " 'if': 37183,\n",
       " 'they': 76956,\n",
       " 'll': 44861,\n",
       " 'be': 6399,\n",
       " 'next': 52455,\n",
       " 'end': 24262,\n",
       " 'up': 81499,\n",
       " 'streets': 73557,\n",
       " 'you': 86483,\n",
       " 'given': 31356,\n",
       " 'bet': 7290,\n",
       " 'live': 44794,\n",
       " 'month': 50173,\n",
       " 'without': 85171,\n",
       " 'luxuries': 45842,\n",
       " 'had': 33377,\n",
       " 'home': 35844,\n",
       " 'entertainment': 24667,\n",
       " 'sets': 68267,\n",
       " 'bathroom': 6268,\n",
       " 'pictures': 57717,\n",
       " 'wall': 83402,\n",
       " 'computer': 15211,\n",
       " 'treasure': 78953,\n",
       " 'like': 44416,\n",
       " 'goddard': 31665,\n",
       " 'bolt': 8627,\n",
       " 'lesson': 44051,\n",
       " 'mel': 48272,\n",
       " 'brooks': 9853,\n",
       " 'directs': 20600,\n",
       " 'stars': 72697,\n",
       " 'plays': 58278,\n",
       " 'rich': 64333,\n",
       " 'man': 46595,\n",
       " 'world': 85574,\n",
       " 'until': 81403,\n",
       " 'deciding': 18739,\n",
       " 'make': 46398,\n",
       " 'with': 85140,\n",
       " 'sissy': 69915,\n",
       " 'rival': 64672,\n",
       " 'jeffery': 40268,\n",
       " 'tambor': 75653,\n",
       " 'he': 34360,\n",
       " 'thirty': 77120,\n",
       " 'days': 18432,\n",
       " 'succeeds': 74139,\n",
       " 'do': 21408,\n",
       " 'wants': 83513,\n",
       " 'future': 30031,\n",
       " 'project': 60181,\n",
       " 'making': 46429,\n",
       " 'more': 50319,\n",
       " 'buildings': 10203,\n",
       " 'where': 84457,\n",
       " 'thrown': 77400,\n",
       " 'bracelet': 9221,\n",
       " 'his': 35480,\n",
       " 'leg': 43791,\n",
       " 'monitor': 50057,\n",
       " 'every': 25510,\n",
       " 'move': 50682,\n",
       " 'step': 72949,\n",
       " 'off': 53853,\n",
       " 'sidewalk': 69518,\n",
       " 'nickname': 52543,\n",
       " 'pepto': 56913,\n",
       " 'by': 10691,\n",
       " 'vagrant': 81877,\n",
       " 'after': 1360,\n",
       " 'written': 85840,\n",
       " 'forehead': 28906,\n",
       " 'meets': 48197,\n",
       " 'characters': 12612,\n",
       " 'including': 37879,\n",
       " 'woman': 85298,\n",
       " 'name': 51704,\n",
       " 'molly': 49931,\n",
       " 'lesley': 44036,\n",
       " 'ann': 3025,\n",
       " 'warren': 83612,\n",
       " 'ex': 25627,\n",
       " 'dancer': 18094,\n",
       " 'got': 32084,\n",
       " 'divorce': 21349,\n",
       " 'before': 6701,\n",
       " 'losing': 45322,\n",
       " 'her': 34853,\n",
       " 'pals': 55743,\n",
       " 'sailor': 65978,\n",
       " 'howard': 36404,\n",
       " 'morris': 50429,\n",
       " 'fumes': 29866,\n",
       " 'teddy': 76097,\n",
       " 'wilson': 84909,\n",
       " 'are': 3794,\n",
       " 'already': 2256,\n",
       " 'used': 81711,\n",
       " 're': 62137,\n",
       " 'survivors': 74826,\n",
       " 'not': 53085,\n",
       " 'reaching': 62149,\n",
       " 'mutual': 51469,\n",
       " 'agreements': 1561,\n",
       " 'being': 6826,\n",
       " 'fight': 27612,\n",
       " 'flight': 28404,\n",
       " 'kill': 41873,\n",
       " 'killed': 41878,\n",
       " 'love': 45429,\n",
       " 'connection': 15579,\n",
       " 'between': 7365,\n",
       " 'wasn': 83673,\n",
       " 'necessary': 52066,\n",
       " 'plot': 58391,\n",
       " 'found': 29165,\n",
       " 'stinks': 73159,\n",
       " 'observant': 53634,\n",
       " 'films': 27798,\n",
       " 'prior': 59895,\n",
       " 'shows': 69264,\n",
       " 'tender': 76323,\n",
       " 'side': 69480,\n",
       " 'compared': 15003,\n",
       " 'slapstick': 70221,\n",
       " 'blazing': 8028,\n",
       " 'saddles': 65866,\n",
       " 'young': 86514,\n",
       " 'frankenstein': 29331,\n",
       " 'spaceballs': 71597,\n",
       " 'show': 69221,\n",
       " 'having': 34273,\n",
       " 'something': 71222,\n",
       " 'valuable': 81966,\n",
       " 'day': 18410,\n",
       " 'hand': 33699,\n",
       " 'stupid': 73847,\n",
       " 'don': 21708,\n",
       " 'know': 42286,\n",
       " 'money': 50010,\n",
       " 'maybe': 47676,\n",
       " 'should': 69189,\n",
       " 'give': 31353,\n",
       " 'instead': 38702,\n",
       " 'using': 81743,\n",
       " 'monopoly': 50093,\n",
       " 'this': 77124,\n",
       " 'film': 27688,\n",
       " 'will': 84854,\n",
       " 'inspire': 38671,\n",
       " 'others': 54871,\n",
       " 'brilliant': 9653,\n",
       " 'over': 55140,\n",
       " 'acting': 701,\n",
       " 'best': 7257,\n",
       " 'dramatic': 22219,\n",
       " 'hobo': 35654,\n",
       " 'lady': 42885,\n",
       " 'have': 34253,\n",
       " 'ever': 25473,\n",
       " 'seen': 67745,\n",
       " 'scenes': 66851,\n",
       " 'clothes': 14144,\n",
       " 'warehouse': 83541,\n",
       " 'second': 67602,\n",
       " 'none': 52917,\n",
       " 'corn': 16276,\n",
       " 'face': 26376,\n",
       " 'good': 31862,\n",
       " 'anything': 3323,\n",
       " 'take': 75524,\n",
       " 'lawyers': 43547,\n",
       " 'also': 2267,\n",
       " 'superb': 74496,\n",
       " 'accused': 573,\n",
       " 'turncoat': 79650,\n",
       " 'selling': 67880,\n",
       " 'out': 54934,\n",
       " 'boss': 9016,\n",
       " 'dishonest': 20910,\n",
       " 'lawyer': 43544,\n",
       " 'shrugs': 69345,\n",
       " 'indifferently': 38087,\n",
       " 'says': 66648,\n",
       " 'three': 77306,\n",
       " 'funny': 29934,\n",
       " 'words': 85490,\n",
       " 'jeffrey': 40270,\n",
       " 'favorite': 27073,\n",
       " 'later': 43335,\n",
       " 'larry': 43248,\n",
       " 'sanders': 66221,\n",
       " 'fantastic': 26765,\n",
       " 'too': 78178,\n",
       " 'mad': 46082,\n",
       " 'millionaire': 49123,\n",
       " 'crush': 17438,\n",
       " 'ghetto': 31039,\n",
       " 'character': 12575,\n",
       " 'malevolent': 46488,\n",
       " 'usual': 81760,\n",
       " 'hospital': 36253,\n",
       " 'scene': 66831,\n",
       " 'invade': 39197,\n",
       " 'demolition': 19284,\n",
       " 'site': 69944,\n",
       " 'classics': 13852,\n",
       " 'look': 45171,\n",
       " 'legs': 43844,\n",
       " 'two': 79829,\n",
       " 'big': 7529,\n",
       " 'diggers': 20340,\n",
       " 'fighting': 27621,\n",
       " 'bleeds': 8055,\n",
       " 'movie': 50708,\n",
       " 'gets': 31005,\n",
       " 'better': 7327,\n",
       " 'each': 23040,\n",
       " 'quite': 61361,\n",
       " 'often': 53959,\n",
       " 'easily': 23134,\n",
       " 'underrated': 80490,\n",
       " 'inn': 38496,\n",
       " 'cannon': 11230,\n",
       " 'sure': 74701,\n",
       " 'its': 39760,\n",
       " 'flawed': 28310,\n",
       " 'does': 21526,\n",
       " 'realistic': 62227,\n",
       " 'view': 82635,\n",
       " 'unlike': 80984,\n",
       " 'say': 66614,\n",
       " 'how': 36403,\n",
       " 'citizen': 13703,\n",
       " 'kane': 41231,\n",
       " 'gave': 30600,\n",
       " 'lounge': 45400,\n",
       " 'singers': 69816,\n",
       " 'titanic': 77850,\n",
       " 'italians': 39652,\n",
       " 'idiots': 37148,\n",
       " 'jokes': 40668,\n",
       " 'fall': 26577,\n",
       " 'flat': 28259,\n",
       " 'still': 73100,\n",
       " 'very': 82434,\n",
       " 'lovable': 45426,\n",
       " 'way': 83855,\n",
       " 'comedies': 14748,\n",
       " 'pull': 60756,\n",
       " 'story': 73341,\n",
       " 'traditionally': 78617,\n",
       " 'reviled': 64143,\n",
       " 'members': 48361,\n",
       " 'society': 70981,\n",
       " 'truly': 79392,\n",
       " 'impressive': 37672,\n",
       " 'fisher': 28086,\n",
       " 'king': 41986,\n",
       " 'crap': 16866,\n",
       " 'either': 23591,\n",
       " 'only': 54317,\n",
       " 'complaint': 15087,\n",
       " 'cast': 11820,\n",
       " 'someone': 71200,\n",
       " 'else': 23861,\n",
       " 'director': 20579,\n",
       " 'writer': 85815,\n",
       " 'so': 70920,\n",
       " 'typical': 79884,\n",
       " 'was': 83638,\n",
       " 'less': 44043,\n",
       " 'movies': 50829,\n",
       " 'actually': 855,\n",
       " 'followable': 28733,\n",
       " 'leslie': 44037,\n",
       " 'made': 46110,\n",
       " 'she': 68705,\n",
       " 'under': 80396,\n",
       " 'rated': 61957,\n",
       " 'actress': 833,\n",
       " 'there': 76890,\n",
       " 'moments': 49956,\n",
       " 'could': 16521,\n",
       " 'fleshed': 28358,\n",
       " 'bit': 7772,\n",
       " 'probably': 59962,\n",
       " 'cut': 17774,\n",
       " 'room': 65130,\n",
       " 'worth': 85666,\n",
       " 'price': 59798,\n",
       " 'rent': 63439,\n",
       " 'overall': 55154,\n",
       " 'himself': 35370,\n",
       " 'job': 40537,\n",
       " 'characteristic': 12595,\n",
       " 'speaking': 71740,\n",
       " 'directly': 20577,\n",
       " 'audience': 4783,\n",
       " 'again': 1399,\n",
       " 'actor': 780,\n",
       " 'fume': 29865,\n",
       " 'both': 9037,\n",
       " 'played': 58246,\n",
       " 'parts': 56219,\n",
       " 'well': 84151,\n",
       " 'comedic': 14744,\n",
       " 'robin': 64814,\n",
       " 'williams': 84873,\n",
       " 'nor': 52989,\n",
       " 'quirky': 61355,\n",
       " 'insane': 38570,\n",
       " 'recent': 62460,\n",
       " 'thriller': 77328,\n",
       " 'fame': 26628,\n",
       " 'hybrid': 36886,\n",
       " 'drama': 22201,\n",
       " 'dramatization': 22232,\n",
       " 'mixed': 49686,\n",
       " 'new': 52377,\n",
       " 'per': 56917,\n",
       " 'se': 67482,\n",
       " 'mystery': 51547,\n",
       " 'suspense': 74862,\n",
       " 'vehicle': 82215,\n",
       " 'attempts': 4687,\n",
       " 'locate': 44940,\n",
       " 'sick': 69453,\n",
       " 'boy': 9173,\n",
       " 'keeper': 41529,\n",
       " 'starring': 72693,\n",
       " 'sandra': 66238,\n",
       " 'oh': 53985,\n",
       " 'rory': 65179,\n",
       " 'culkin': 17571,\n",
       " 'pretty': 59753,\n",
       " 'news': 52419,\n",
       " 'report': 63566,\n",
       " 'william': 84871,\n",
       " 'close': 14119,\n",
       " 'achieving': 607,\n",
       " 'goal': 31618,\n",
       " 'must': 51411,\n",
       " 'highly': 35233,\n",
       " 'entertained': 24647,\n",
       " 'though': 77238,\n",
       " 'fails': 26490,\n",
       " 'teach': 75993,\n",
       " 'guide': 33006,\n",
       " 'inspect': 38657,\n",
       " 'amuse': 2656,\n",
       " 'felt': 27310,\n",
       " 'watching': 83742,\n",
       " 'guy': 33230,\n",
       " 'performing': 57030,\n",
       " 'actions': 757,\n",
       " 'third': 77100,\n",
       " 'person': 57183,\n",
       " 'perspective': 57215,\n",
       " 'real': 62209,\n",
       " 'able': 235,\n",
       " 'subscribe': 74033,\n",
       " 'premise': 59569,\n",
       " 'watch': 83719,\n",
       " 'definitely': 18994,\n",
       " 'friday': 29572,\n",
       " 'saturday': 66505,\n",
       " 'night': 52612,\n",
       " 'fare': 26820,\n",
       " 'rates': 61962,\n",
       " 'fiend': 27579,\n",
       " 'yes': 86332,\n",
       " 'art': 4097,\n",
       " 'successfully': 74149,\n",
       " 'slow': 70470,\n",
       " 'paced': 55502,\n",
       " 'unfolds': 80731,\n",
       " 'nice': 52498,\n",
       " 'volumes': 83084,\n",
       " 'even': 25439,\n",
       " 'notice': 53128,\n",
       " 'happening': 33861,\n",
       " 'fine': 27921,\n",
       " 'performance': 56992,\n",
       " 'sexuality': 68378,\n",
       " 'angles': 2926,\n",
       " 'seem': 67728,\n",
       " 'unnecessary': 81064,\n",
       " 'affect': 1257,\n",
       " 'enjoy': 24488,\n",
       " 'however': 36415,\n",
       " 'core': 16248,\n",
       " 'engaging': 24414,\n",
       " 'doesn': 21532,\n",
       " 'rush': 65646,\n",
       " 'onto': 54355,\n",
       " 'grips': 32693,\n",
       " 'enough': 24547,\n",
       " 'keep': 41527,\n",
       " 'wondering': 85359,\n",
       " 'direction': 20560,\n",
       " 'use': 81708,\n",
       " 'lights': 44400,\n",
       " 'achieve': 598,\n",
       " 'desired': 19699,\n",
       " 'affects': 1270,\n",
       " 'unexpectedness': 80672,\n",
       " 'looking': 45182,\n",
       " 'lay': 43553,\n",
       " 'back': 5359,\n",
       " 'hear': 34444,\n",
       " 'thrilling': 77337,\n",
       " 'short': 69132,\n",
       " 'critically': 17210,\n",
       " 'acclaimed': 495,\n",
       " 'psychological': 60623,\n",
       " 'based': 6143,\n",
       " 'true': 79365,\n",
       " 'events': 25456,\n",
       " 'gabriel': 30096,\n",
       " 'celebrated': 12164,\n",
       " 'late': 43323,\n",
       " 'talk': 75597,\n",
       " 'host': 36267,\n",
       " 'becomes': 6593,\n",
       " 'captivated': 11371,\n",
       " 'harrowing': 34081,\n",
       " 'listener': 44714,\n",
       " 'adoptive': 1077,\n",
       " 'mother': 50538,\n",
       " 'toni': 78155,\n",
       " 'collette': 14580,\n",
       " 'troubling': 79314,\n",
       " 'questions': 61267,\n",
       " 'arise': 3879,\n",
       " 'finds': 27919,\n",
       " 'drawn': 22275,\n",
       " 'into': 39115,\n",
       " 'widening': 84736,\n",
       " 'hides': 35190,\n",
       " 'deadly': 18510,\n",
       " 'secretâ': 67641,\n",
       " 'according': 530,\n",
       " 'official': 53900,\n",
       " 'synopsis': 75320,\n",
       " 'really': 62265,\n",
       " 'stop': 73281,\n",
       " 'reading': 62185,\n",
       " 'these': 76937,\n",
       " 'comments': 14879,\n",
       " 'now': 53226,\n",
       " 'lose': 45314,\n",
       " 'ending': 24303,\n",
       " 'ms': 50987,\n",
       " 'planning': 58166,\n",
       " 'chopped': 13293,\n",
       " 'sent': 68019,\n",
       " 'deleted': 19128,\n",
       " 'land': 43073,\n",
       " 'overkill': 55263,\n",
       " 'nature': 51920,\n",
       " 'physical': 57624,\n",
       " 'mental': 48474,\n",
       " 'ailments': 1646,\n",
       " 'obvious': 53691,\n",
       " 'mr': 50948,\n",
       " 'returns': 64042,\n",
       " 'york': 86457,\n",
       " 'possibly': 59098,\n",
       " 'blindness': 8110,\n",
       " 'question': 61253,\n",
       " 'revelation': 64076,\n",
       " 'certain': 12301,\n",
       " 'highway': 35251,\n",
       " 'video': 82575,\n",
       " 'tape': 75755,\n",
       " 'would': 85679,\n",
       " 'benefit': 7031,\n",
       " 'editing': 23318,\n",
       " 'bobby': 8437,\n",
       " 'cannavale': 11210,\n",
       " 'jess': 40372,\n",
       " 'initially': 38453,\n",
       " 'believable': 6887,\n",
       " 'couple': 16620,\n",
       " 'establishing': 25212,\n",
       " 'relationship': 63138,\n",
       " 'might': 48976,\n",
       " 'helped': 34744,\n",
       " 'set': 68256,\n",
       " 'stage': 72472,\n",
       " 'otherwise': 54887,\n",
       " 'exemplary': 25833,\n",
       " 'offers': 53885,\n",
       " 'exceptionally': 25720,\n",
       " 'strong': 73682,\n",
       " 'characterization': 12598,\n",
       " 'gay': 30616,\n",
       " 'impersonation': 37559,\n",
       " 'anna': 3026,\n",
       " 'joe': 40583,\n",
       " 'morton': 50477,\n",
       " 'ashe': 4251,\n",
       " 'pete': 57300,\n",
       " 'logand': 45009,\n",
       " 'perfect': 56960,\n",
       " 'donna': 21755,\n",
       " 'belongs': 6958,\n",
       " 'creepy': 17065,\n",
       " 'hall': 33549,\n",
       " 'correct': 16342,\n",
       " 'saying': 66629,\n",
       " 'psycho': 60606,\n",
       " 'several': 68313,\n",
       " 'organizations': 54663,\n",
       " 'giving': 31364,\n",
       " 'awards': 5121,\n",
       " 'seemed': 67732,\n",
       " 'reach': 62145,\n",
       " 'women': 85324,\n",
       " 'due': 22653,\n",
       " 'slighter': 70362,\n",
       " 'dispersion': 21061,\n",
       " 'roles': 64981,\n",
       " 'certainly': 12302,\n",
       " 'noticed': 53132,\n",
       " 'award': 5117,\n",
       " 'consideration': 15685,\n",
       " 'patrick': 56410,\n",
       " 'stettner': 73022,\n",
       " 'evokes': 25584,\n",
       " 'hitchcock': 35526,\n",
       " 'makes': 46416,\n",
       " 'getting': 31012,\n",
       " 'sandwich': 66251,\n",
       " 'vending': 82254,\n",
       " 'machine': 46013,\n",
       " 'suspenseful': 74868,\n",
       " 'finally': 27894,\n",
       " 'writers': 85819,\n",
       " 'armistead': 3951,\n",
       " 'maupin': 47621,\n",
       " 'terry': 76519,\n",
       " 'anderson': 2788,\n",
       " 'deserve': 19663,\n",
       " 'gratitude': 32392,\n",
       " 'attendants': 4693,\n",
       " 'everywhere': 25539,\n",
       " 'john': 40618,\n",
       " 'cullum': 17580,\n",
       " 'lisa': 44694,\n",
       " 'emery': 24033,\n",
       " 'becky': 6587,\n",
       " 'baker': 5609,\n",
       " 'dir': 20546,\n",
       " 'hitchcockian': 35528,\n",
       " 'suspenser': 74874,\n",
       " 'gives': 31362,\n",
       " 'stand': 72580,\n",
       " 'low': 45508,\n",
       " 'key': 41690,\n",
       " 'celebrities': 12172,\n",
       " 'fans': 26731,\n",
       " 'near': 52032,\n",
       " 'paranoia': 55977,\n",
       " 'associates': 4435,\n",
       " 'why': 84699,\n",
       " 'almost': 2189,\n",
       " 'norm': 53009,\n",
       " 'latest': 43352,\n",
       " 'derange': 19552,\n",
       " 'fan': 26677,\n",
       " 'scenario': 66823,\n",
       " 'no': 52802,\n",
       " 'radio': 61513,\n",
       " 'personality': 57191,\n",
       " 'named': 51706,\n",
       " 'reads': 62194,\n",
       " 'stories': 73317,\n",
       " 'penned': 56813,\n",
       " 'airwaves': 1726,\n",
       " 'accumulated': 556,\n",
       " 'interesting': 38921,\n",
       " 'form': 29012,\n",
       " 'submitted': 74015,\n",
       " 'manuscript': 46869,\n",
       " 'travails': 78905,\n",
       " 'troubled': 79306,\n",
       " 'youth': 86562,\n",
       " 'editor': 23326,\n",
       " 'read': 62171,\n",
       " 'naturally': 51917,\n",
       " 'disturbed': 21265,\n",
       " 'ultimately': 80048,\n",
       " 'intrigued': 39146,\n",
       " 'nightmarish': 52635,\n",
       " 'existence': 25891,\n",
       " 'abducted': 182,\n",
       " 'sexually': 68381,\n",
       " 'abused': 396,\n",
       " 'rescued': 63693,\n",
       " 'nurse': 53411,\n",
       " 'excellent': 25704,\n",
       " 'adopted': 1071,\n",
       " 'correspondence': 16367,\n",
       " 'reveals': 64072,\n",
       " 'dying': 22996,\n",
       " 'aids': 1630,\n",
       " 'meet': 48193,\n",
       " 'suddenly': 74216,\n",
       " 'doubt': 21986,\n",
       " 'devious': 19982,\n",
       " 'ulterior': 80046,\n",
       " 'motives': 50579,\n",
       " 'seed': 67700,\n",
       " 'planted': 58178,\n",
       " 'estranged': 25257,\n",
       " 'lover': 45470,\n",
       " 'whose': 84687,\n",
       " 'sudden': 74215,\n",
       " 'departure': 19459,\n",
       " 'city': 13712,\n",
       " 'apartment': 3390,\n",
       " 'emotional': 24092,\n",
       " 'tailspin': 75492,\n",
       " 'grown': 32841,\n",
       " 'tempest': 76272,\n",
       " 'teacup': 76004,\n",
       " 'decides': 18738,\n",
       " 'investigating': 39248,\n",
       " 'backgrounds': 5388,\n",
       " 'discovering': 20806,\n",
       " 'truths': 79431,\n",
       " 'didn': 20233,\n",
       " 'anticipate': 3200,\n",
       " 'co': 14252,\n",
       " 'wrote': 85871,\n",
       " 'screenplay': 67319,\n",
       " 'former': 29035,\n",
       " 'novice': 53219,\n",
       " 'hoax': 35630,\n",
       " 'run': 65599,\n",
       " 'full': 29845,\n",
       " 'tilt': 77608,\n",
       " 'any': 3294,\n",
       " 'old': 54090,\n",
       " 'fashioned': 26929,\n",
       " 'pot': 59149,\n",
       " 'boiler': 8562,\n",
       " 'helps': 34758,\n",
       " 'conflicted': 15468,\n",
       " 'hearted': 34473,\n",
       " 'genuinely': 30869,\n",
       " 'number': 53362,\n",
       " 'fact': 26418,\n",
       " 'him': 35332,\n",
       " 'thing': 77008,\n",
       " 'escaped': 25094,\n",
       " 'own': 55440,\n",
       " 'unsettling': 81295,\n",
       " 'dreadful': 22285,\n",
       " 'trait': 78683,\n",
       " 'leave': 43698,\n",
       " 'unmentioned': 81035,\n",
       " 'underlines': 80458,\n",
       " 'desperation': 19741,\n",
       " 'rattle': 62010,\n",
       " 'runs': 65618,\n",
       " 'gas': 30515,\n",
       " 'eventually': 25466,\n",
       " 'repetitive': 63524,\n",
       " 'predictable': 59457,\n",
       " 'despite': 19755,\n",
       " 'finely': 27926,\n",
       " 'directed': 20549,\n",
       " 'piece': 57740,\n",
       " 'hoodwink': 36012,\n",
       " 'pays': 56546,\n",
       " 'listen': 44709,\n",
       " 'inner': 38507,\n",
       " 'voice': 83029,\n",
       " 'careful': 11456,\n",
       " 'hope': 36069,\n",
       " 'god': 31648,\n",
       " 'bless': 8078,\n",
       " 'constantly': 15741,\n",
       " 'shooting': 69096,\n",
       " 'foot': 28803,\n",
       " 'lately': 43330,\n",
       " 'dumb': 22725,\n",
       " 'done': 21731,\n",
       " 'decade': 18672,\n",
       " 'perhaps': 57038,\n",
       " 'exception': 25718,\n",
       " 'death': 18575,\n",
       " 'smoochy': 70667,\n",
       " 'bombed': 8654,\n",
       " 'came': 11029,\n",
       " 'cult': 17593,\n",
       " 'dramas': 22214,\n",
       " 'especially': 25159,\n",
       " 'insomnia': 38649,\n",
       " 'hour': 36337,\n",
       " 'photo': 57572,\n",
       " 'mediocre': 48148,\n",
       " 'reviews': 64137,\n",
       " 'quick': 61287,\n",
       " 'dvd': 22928,\n",
       " 'release': 63167,\n",
       " 'among': 2589,\n",
       " 'period': 57050,\n",
       " 'chilling': 13146,\n",
       " 'include': 37874,\n",
       " 'serial': 68161,\n",
       " 'killer': 41885,\n",
       " 'anyone': 3315,\n",
       " 'physically': 57628,\n",
       " 'dangerous': 18132,\n",
       " 'concept': 15267,\n",
       " 'actual': 849,\n",
       " 'case': 11735,\n",
       " 'fraud': 29381,\n",
       " 'yet': 86357,\n",
       " 'officially': 53903,\n",
       " 'confirmed': 15456,\n",
       " 'autobiography': 4952,\n",
       " 'child': 13096,\n",
       " 'anthony': 3178,\n",
       " 'godby': 31657,\n",
       " 'johnson': 40633,\n",
       " 'suffered': 74242,\n",
       " 'horrific': 36168,\n",
       " 'abuse': 394,\n",
       " 'contracted': 15934,\n",
       " 'result': 63915,\n",
       " 'moved': 50686,\n",
       " 'reports': 63578,\n",
       " 'online': 54311,\n",
       " 'may': 47670,\n",
       " 'exist': 25883,\n",
       " 'confused': 15501,\n",
       " 'feelings': 27230,\n",
       " 'brilliantly': 9661,\n",
       " 'portrayed': 59020,\n",
       " 'resurfaced': 63930,\n",
       " 'mind': 49177,\n",
       " 'sociopathic': 70993,\n",
       " 'caretaker': 11474,\n",
       " 'role': 64969,\n",
       " 'cry': 17457,\n",
       " 'little': 44780,\n",
       " 'miss': 49561,\n",
       " 'sunshine': 74481,\n",
       " 'times': 77686,\n",
       " 'looked': 45175,\n",
       " 'camera': 11039,\n",
       " 'thought': 77250,\n",
       " 'staring': 72668,\n",
       " 'takes': 75543,\n",
       " 'play': 58237,\n",
       " 'sort': 71432,\n",
       " 'understated': 80513,\n",
       " 'reviewed': 64130,\n",
       " 'actresses': 837,\n",
       " 'generation': 30764,\n",
       " 'nominated': 52888,\n",
       " 'academy': 428,\n",
       " 'incredible': 37968,\n",
       " 'least': 43674,\n",
       " 'scary': 66784,\n",
       " 'dark': 18225,\n",
       " 'recommend': 62546,\n",
       " 'prepared': 59594,\n",
       " 'unsettled': 81292,\n",
       " 'because': 6560,\n",
       " 'leaves': 43704,\n",
       " 'strange': 73467,\n",
       " 'feeling': 27227,\n",
       " 'liked': 44424,\n",
       " 'action': 741,\n",
       " 'tense': 76369,\n",
       " 'opening': 54422,\n",
       " 'semi': 67908,\n",
       " 'truck': 79345,\n",
       " 'transitional': 78780,\n",
       " 'filmed': 27725,\n",
       " 'ways': 83885,\n",
       " 'lapse': 43197,\n",
       " 'photography': 57589,\n",
       " 'unusual': 81438,\n",
       " 'colors': 14657,\n",
       " 'evil': 25559,\n",
       " 'first': 28060,\n",
       " 'maupins': 47622,\n",
       " 'taken': 75530,\n",
       " 'displayed': 21070,\n",
       " 'cares': 11468,\n",
       " 'loves': 45480,\n",
       " 'said': 65957,\n",
       " 'we': 83914,\n",
       " 'version': 82401,\n",
       " 'expected': 25964,\n",
       " 'past': 56302,\n",
       " 'gloss': 31549,\n",
       " 'hollywood': 35790,\n",
       " 'succeeded': 74136,\n",
       " 'amount': 2604,\n",
       " 'restraint': 63896,\n",
       " 'captures': 11382,\n",
       " 'fragile': 29241,\n",
       " 'essence': 25199,\n",
       " 'lets': 44077,\n",
       " 'us': 81693,\n",
       " 'struggle': 73715,\n",
       " 'issues': 39619,\n",
       " 'trust': 79410,\n",
       " 'personnel': 57212,\n",
       " 'lifejess': 44320,\n",
       " 'around': 4008,\n",
       " 'himdonna': 35340,\n",
       " 'introduced': 39159,\n",
       " 'players': 58253,\n",
       " 'reminded': 63324,\n",
       " 'nothing': 53115,\n",
       " 'seems': 67739,\n",
       " 'smallest': 70549,\n",
       " 'event': 25449,\n",
       " 'change': 12491,\n",
       " 'our': 54924,\n",
       " 'lives': 44816,\n",
       " 'irrevocably': 39449,\n",
       " 'request': 63664,\n",
       " 'review': 64127,\n",
       " 'book': 8778,\n",
       " 'turns': 79662,\n",
       " 'changing': 12507,\n",
       " 'find': 27910,\n",
       " 'strength': 73570,\n",
       " 'within': 85167,\n",
       " 'carry': 11656,\n",
       " 'forward': 29124,\n",
       " 'bad': 5454,\n",
       " 'avoid': 5075,\n",
       " 'average': 5035,\n",
       " 'american': 2512,\n",
       " 'serious': 68186,\n",
       " 'please': 58315,\n",
       " 'chance': 12456,\n",
       " 'touches': 78441,\n",
       " 'darkness': 18246,\n",
       " 'go': 31613,\n",
       " 'ourselves': 54928,\n",
       " 'stepped': 72974,\n",
       " 'another': 3113,\n",
       " 'quality': 61144,\n",
       " 'forget': 28972,\n",
       " 'steals': 72855,\n",
       " 'leading': 43617,\n",
       " 'looks': 45194,\n",
       " 'screen': 67300,\n",
       " 'presence': 59640,\n",
       " 'hacks': 33375,\n",
       " 'opinion': 54463,\n",
       " 'illnesses': 37291,\n",
       " 'born': 8972,\n",
       " 'modern': 49804,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "##### CODE #####\n",
    "print(X_baseline.shape) #tupla (número de reseñas, número de palabras distintas después de limpiar)\n",
    "baseline_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE #####\n",
    "vectorizer_c = CountVectorizer()\n",
    "vectorizer_c.fit(reviews_train_clean)\n",
    "\n",
    "# reseñas en formato vector de palabras\n",
    "X_baseline_c = vectorizer_c.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n87063\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_c.shape)\n",
    "print(len(vectorizer_c.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Baseline Model\n",
    "\n",
    "Train a Logistic Regression model after transforming the data with CountVectorized\n",
    "\n",
    "* They’re easy to interpret\n",
    "* Linear models tend to perform well on sparse datasets like this one\n",
    "* They learn very fast compared to other algorithms.\n",
    "\n",
    "Test models with C values of [0.01, 0.05, 0.25, 0.5, 1] and see wich is the best value for C, and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.86864\n",
      "Accuracy for C=0.05: 0.87584\n",
      "Accuracy for C=0.25: 0.87808\n",
      "Accuracy for C=0.5: 0.8752\n",
      "Accuracy for C=1: 0.87376\n",
      "Final Accuracy: 0.88188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Los comentarios vienen ordenados. los primeros 12500 son negativos\n",
    "# A test le ocurre lo mismo\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "\n",
    "def train_model(X_TRAIN, X_TEST):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_TRAIN,\n",
    "                                                      target,\n",
    "                                                      train_size = 0.75,\n",
    "                                                      random_state=42)\n",
    "\n",
    "    for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "\n",
    "        lr = LogisticRegression(C=c)\n",
    "        lr.fit(X_train, y_train)\n",
    "        print (\"Accuracy for C=%s: %s\" % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "\n",
    "    final_model = LogisticRegression(C=0.05)\n",
    "    final_model.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, final_model.predict(X_TEST)))\n",
    "    \n",
    "train_model(X_baseline, X_test_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words\n",
    "\n",
    "Stop words are the very common words like ‘if’, ‘but’, ‘we’, ‘he’, ‘she’, and ‘they’. We can usually remove these words without changing the semantics of a text and doing so often (but not always) improves the performance of a model. Removing these stop words becomes a lot more useful when we start using longer word sequences as model features (see n-grams below).\n",
    "\n",
    "Before we apply the CountVectorized, lets remove the stopwords, included in nltk.corpus (nltk ==> natural language tool kit)\n",
    "\n",
    "Then apply the CountVectorizer, and train the Logistic regression model and obtain the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "##### CODE #####\n",
    "# Para visualizar las stopwords de inglés\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "##### CODE #####\n",
    "#para visualizar en español\n",
    "stopwords.words('spanish')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "len([word for word in reviews_train_clean[5].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "len([word for word in reviews_train_clean[5].split() if word not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Aplicamos la eliminación de las palabras directamente sobre las reseñas\n",
    "english_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        \n",
    "        # Para cada review elimina las stopwords, y separa todas las palabras por espacio\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.86656\n",
      "Accuracy for C=0.05: 0.87632\n",
      "Accuracy for C=0.25: 0.8792\n",
      "Accuracy for C=0.5: 0.87776\n",
      "Accuracy for C=1: 0.87632\n",
      "Final Accuracy: 0.87936\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "cv = CountVectorizer(binary= True)\n",
    "cv.fit(no_stop_words_train)\n",
    "X = cv.transform(no_stop_words_train)\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n(25000, 87046)\nStop words eliminadas: 17\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.8664\n",
      "Accuracy for C=0.05: 0.876\n",
      "Accuracy for C=0.25: 0.8808\n",
      "Accuracy for C=0.5: 0.87824\n",
      "Accuracy for C=1: 0.87696\n",
      "Final Accuracy: 0.87976\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "#También podemos eliminar las stopwords directamente por parámetro\n",
    "cv = CountVectorizer(binary= True,\n",
    "                     stop_words= english_stop_words) #Existe en la clase un parámetro 'stop_words' para eliminar las palabras que indiques en la lista \n",
    "\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n(25000, 86918)\nStop words eliminadas: 145\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In practice, an easier way to remove stop words is to just use the stop_words argument with any of scikit-learn’s ‘Vectorizer’ classes. If you want to use NLTK’s full list of stop words you can do stop_words='english’. In practice I’ve found that using NLTK’s list actually decreases my performance because its too expansive, so I usually supply my own list of words. For example, stop_words=['in','of','at','a','the'] ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common next step in text preprocessing is to normalize the words in your corpus by trying to convert all of the different forms of a given word into one. Two methods that exist for this are Stemming and Lemmatization.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming is considered to be the more crude/brute-force approach to normalization (although this doesn’t necessarily mean that it will perform worse). There’s several algorithms, but in general they all use basic rules to chop off the ends of words.\n",
    "\n",
    "NLTK has several stemming algorithm implementations. We’ll use the Porter stemmer. Most used:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "Apply a PoterStemmer, vectorize, and train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "\n",
    "\"\"\"\n",
    "El stemmer se aplica sobre cada palabra. Las recortas eliminando plurales y tiempos verbales.\n",
    "\"\"\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted'] #\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(\" \".join(singles))\n",
    "\n",
    "##### CODE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "corr cas play vol vol volv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "##### CODE #####\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "plurals = ['corriendo', 'casas', 'playa', 'volando', 'volar', 'volveré']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.8704\n",
      "Accuracy for C=0.05: 0.87776\n",
      "Accuracy for C=0.25: 0.87744\n",
      "Accuracy for C=0.5: 0.87344\n",
      "Accuracy for C=1: 0.86992\n",
      "Final Accuracy: 0.87736\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "#los stemmer no eliminan palabras sólo quitán sufijos\n",
    "#Y se consigue más número de palabra iguales (coincide la \"raiz\")\n",
    "def get_stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)\n",
    "\n",
    "train_model(X_stem, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n(25000, 66852)\nDiff X normal y X tras stemmer y vectorización: 20211\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X_stem.shape)\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", X_baseline.shape[1] - X_stem.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization works by identifying the part-of-speech of a given word and then applying more complex rules to transform the word into its true root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\pilar\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "##### CODE #####\n",
    "import nltk as nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "caress fly dy mule study died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "##### CODE #####\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.86528\n",
      "Accuracy for C=0.05: 0.8752\n",
      "Accuracy for C=0.25: 0.87392\n",
      "Accuracy for C=0.5: 0.87344\n",
      "Accuracy for C=1: 0.87152\n",
      "Final Accuracy: 0.87956\n"
     ]
    }
   ],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# Lematizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 87063)\n(25000, 80326)\nDiff X normal y X tras lematizador y vectorización: 6737\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "We can potentially add more predictive power to our model by adding two or three word sequences (bigrams or trigrams) as well. For example, if a review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review.\n",
    "\n",
    "The scikit-learn library makes this really easy to play around with. Just use the ngram_range argument with any of the ‘Vectorizer’ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('this', 'is')\n('is', 'foo')\n('foo', 'bar')\n###############\n('this', 'is', 'foo')\n('is', 'foo', 'bar')\n\n[[1 1 1 1 1 1 1 1 1]]\n9\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"this is foo bar\"\n",
    "\n",
    "two = ngrams(sentence.split(),2)\n",
    "three = ngrams(sentence.split(),3)\n",
    "\n",
    "for grams in two:\n",
    "  print(grams)\n",
    "print('###############')\n",
    "for grams in three:\n",
    "  print(grams)\n",
    "\n",
    "##### CODE #####\n",
    "\"\"\"\n",
    "Puede ser bigrams si ngram_range = (2,2), o trigramas...\n",
    "Algunas palabras las elimina, como 'a'. Cuidado con eso a la hora de hacer el \n",
    "conteo ngram_range =(1,3) significa palabras por separado, los bigramas y los trigramas\n",
    "\n",
    "Ojo que esto aumenta muchísimo el espacio de features\n",
    "\"\"\"\n",
    "ngrams_vectorizer = CountVectorizer(binary= True,\n",
    "                                      ngram_range=(1,3))\n",
    "\n",
    "vector = ngrams_vectorizer.fit_transform([sentence]).toarray()\n",
    "print()\n",
    "print(vector)\n",
    "print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for C=0.01: 0.87696\n",
      "Accuracy for C=0.05: 0.88384\n",
      "Accuracy for C=0.25: 0.8872\n",
      "Accuracy for C=0.5: 0.88768\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-18c487dedebc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_test_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-cbd0ee1904f6>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_TRAIN, X_TEST)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy for C=%s: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1414\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1416\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"L-BFGS-B\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"iprint\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gtol\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"maxiter\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m             )\n\u001b[0;32m    763\u001b[0m             n_iter_i = _check_optimize_result(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    352\u001b[0m                        \u001b[0mpgtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miwa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miprint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsave\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsave\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                        isave, dsave, maxls)\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mtask_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'FG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[1;31m# The minimization routine wants f and g at the current x.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                    ngram_range=(1,2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency) for each word, which is a weighting factor that we can use in place of binary or word count representations.\n",
    "\n",
    "There are several ways to do tf-idf transformation but in a nutshell, **tf-idf aims to represent the number of times a given word appears in a document (a movie review in our case) relative to the number of documents in the corpus that the word appears in** — where words that appear in many documents have a value closer to zero and words that appear in less documents have values closer to 1.\n",
    "\n",
    "**Note:** Now that we’ve gone over n-grams, when I refer to ‘words’ I really mean any n-gram (sequence of words) if the model is using an n greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE #####\n",
    "\"\"\"\n",
    "ln(N + 1 / df +1) +1\n",
    "Cuanto más común, menor es el TFIDF. Cuanto más rara, mayor\n",
    "\"\"\"\n",
    "\n",
    "#Número de documentos\n",
    "N = 3\n",
    "\n",
    "#Numero de veces que aparece la palabra\n",
    "df = 2\n",
    "\n",
    "1 + np.log((N + 1)/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CODE #####\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\"\"\"\n",
    "Cuanto más común, más bajo es el TfidfVectorizer\n",
    "\"\"\"\n",
    "sent1 = \"My name is Ralph\"\n",
    "sent2 = \"Ralph is happy\"\n",
    "sent3 = \"Ralph\"\n",
    "\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1,sent2, sent3])\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names())\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "train_model(X, X_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Recall that linear classifiers tend to work well on very sparse datasets (like the one we have). Another algorithm that can produce great results with a quick training time are Support Vector Machines with a linear kernel.\n",
    "\n",
    "Build a model with an n-gram range from 1 to 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM con bigramas\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "    \n",
    "final_svm_ngram = LinearSVC(C=0.01)\n",
    "final_svm_ngram.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_svm_ngram.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "Removing a small set of stop words along with an n-gram range from 1 to 3 and a linear support vector classifier shows the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "##### CODE #####\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary= True,\n",
    "                                   ngram_range=(1,3),\n",
    "                                   stop_words= stop_words)\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "\n",
    "    \n",
    "final = LinearSVC(C=0.01)\n",
    "final.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Postitive and Negative Features\n",
    "\n",
    "Obtain the most important features of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "87063\n"
     ]
    }
   ],
   "source": [
    "##### CODE ####\n",
    "cv = CountVectorizer(binary= True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "log_reg = LogisticRegression(C = 0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "final_model = log_reg\n",
    "\n",
    "#importancia de los coeficientes. En total, todas las palabras vectorizadas\n",
    "print(len(final_model.coef_[0]))\n",
    "\n",
    "#Cada coeficiente va asociado a una palabra\n",
    "cv.get_feature_names()\n",
    "\n",
    "# Montamos un diccionario con palabra -> coeficiente\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "027358317622930116,\n",
       " 'accentcourtesy': 0.03274539513581589,\n",
       " 'accented': 0.021386262450647085,\n",
       " 'accenthe': -0.0032129614449121205,\n",
       " 'accentit': -0.03605147663282439,\n",
       " 'accentlike': 0.03274539513581589,\n",
       " 'accents': -0.24490449936745923,\n",
       " 'accentsomeone': -0.0008323722420750297,\n",
       " 'accentuate': 0.06330820450952748,\n",
       " 'accentuated': 0.06515384935098743,\n",
       " 'accentuates': 0.08460252771205184,\n",
       " 'accentuating': -0.01022260062070583,\n",
       " 'accentuation': 0.0015654923513505404,\n",
       " 'accept': 0.08709325813854711,\n",
       " 'acceptable': -0.15006225756259578,\n",
       " 'acceptablethe': 0.07278614712921161,\n",
       " 'acceptably': 0.16945751143835486,\n",
       " 'acceptance': 0.0834963416907583,\n",
       " 'acceptation': 0.0011710872086697405,\n",
       " 'accepted': -0.3427061036632185,\n",
       " 'acceptence': 0.0007298621491786402,\n",
       " 'accepting': 0.1882251123071673,\n",
       " 'acception': 0.028396929640109817,\n",
       " 'accepts': 0.13696767838444596,\n",
       " 'accesible': -0.0002158234966006485,\n",
       " 'access': 0.12457019526419127,\n",
       " 'accessability': 0.004505378021177838,\n",
       " 'accessed': -0.01326674563509764,\n",
       " 'accessibility': 0.07544454589479774,\n",
       " 'accessible': 0.1930866475202615,\n",
       " 'accessibleit': 0.08575396710299439,\n",
       " 'accession': 9.815293447106928e-05,\n",
       " 'accessories': -0.010741852788256043,\n",
       " 'accessorizing': -0.03417608784745351,\n",
       " 'accessory': 0.019754953758007465,\n",
       " 'accidence': -0.020082977978018467,\n",
       " 'accident': 0.4706497101246467,\n",
       " 'accidentafter': 0.0038769006805238816,\n",
       " 'accidental': 0.073142187862657,\n",
       " 'accidentally': -0.03807028827308643,\n",
       " 'accidentand': -0.0010029799510621429,\n",
       " 'accidentee': -0.0007445813528238536,\n",
       " 'accidenthe': -0.008980362292955974,\n",
       " 'accidentin': 0.08551610070635833,\n",
       " 'accidently': 0.1472984711954253,\n",
       " 'accidentnot': -0.00745558953144591,\n",
       " 'accidents': -0.13399249457921392,\n",
       " 'accidentthat': -0.002870666372509985,\n",
       " 'accidentwell': -0.0032129614449121205,\n",
       " 'acclaim': -0.17930337308162528,\n",
       " 'acclaimed': -0.3720350563567073,\n",
       " 'acclaims': 0.0003042912196731596,\n",
       " 'acclamation': 0.00011167100319470341,\n",
       " 'acclimate': 0.002837592575649099,\n",
       " 'acclimation': -0.06972129970481979,\n",
       " 'accolade': 0.07656168005244562,\n",
       " 'accoladed': -0.00026413746129143095,\n",
       " 'accolades': 0.23564356435903752,\n",
       " 'accoladesit': -0.03640951819293391,\n",
       " 'accommodate': 0.0863985602981225,\n",
       " 'accommodated': 0.051715123487688315,\n",
       " 'accommodates': 0.022623948465480614,\n",
       " 'accommodating': 0.02006920762697869,\n",
       " 'accommodation': 0.09099300858516778,\n",
       " 'accommodations': 0.011694245003692023,\n",
       " 'accompagnied': -0.0029305155353114534,\n",
       " 'accompanied': -0.058967980598184276,\n",
       " 'accompanies': -0.10686206657044305,\n",
       " 'accompaniment': 0.06901408409947883,\n",
       " 'accompany': -0.08387117476937658,\n",
       " 'accompanying': 0.096320624871403,\n",
       " 'accomplice': -0.09678005402954835,\n",
       " 'accomplices': -0.006083808700327249,\n",
       " 'accomplish': 0.04340439181261338,\n",
       " 'accomplished': 0.2410527565991343,\n",
       " 'accomplishedjulie': -0.011179931305241472,\n",
       " 'accomplishes': 0.14025051772967406,\n",
       " 'accomplishing': -0.02959866589273759,\n",
       " 'accomplishment': -0.011911475601679867,\n",
       " 'accomplishments': -0.09575292491885977,\n",
       " 'accomplishthe': 0.0002543639450145014,\n",
       " 'accord': 0.022035612916925136,\n",
       " 'accordance': -0.020441099245286323,\n",
       " 'accorded': 0.031314258139008656,\n",
       " 'accordian': 0.028001143532730803,\n",
       " 'according': 0.11055176416209177,\n",
       " 'accordingly': -0.2488568354272658,\n",
       " 'accordinglyshe': -0.046958222451607454,\n",
       " 'accordion': 0.0816226306707079,\n",
       " 'accords': -0.1144683096578452,\n",
       " 'accorsi': 0.023897508530024272,\n",
       " 'accost': 0.0006068066624525856,\n",
       " 'accosted': 0.019254845797956083,\n",
       " 'accosts': 0.031845057155357875,\n",
       " 'account': -0.07200338983957925,\n",
       " 'accountability': -0.01635368816288668,\n",
       " 'accountable': -0.04018824251575123,\n",
       " 'accountancy': 0.06427527128445086,\n",
       " 'accountant': 0.044688077852821334,\n",
       " 'accountants': -0.00016772119452991593,\n",
       " 'accounted': -0.015382611061920353,\n",
       " 'accounting': 0.0507484023558527,\n",
       " 'accounts': 0.15723627779847466,\n",
       " 'accountsmaybe': -0.029629493648548774,\n",
       " 'accountthe': -0.04218518927312484,\n",
       " 'accouterments': 0.00011451400082746121,\n",
       " 'accredited': 0.07004470918159567,\n",
       " 'accrued': -0.04530566538487833,\n",
       " 'accrutements': -0.002502767327908726,\n",
       " 'acct': 0.0003824243909924739,\n",
       " 'accumulate': 0.016062112052138776,\n",
       " 'accumulated': 0.018584787956431176,\n",
       " 'accumulates': 0.015589066985625677,\n",
       " 'accumulating': 0.00025007269426592524,\n",
       " 'accumulation': -0.08299431401288157,\n",
       " 'accumulator': 0.02548585395697539,\n",
       " 'accuracies': -0.00020113088458448224,\n",
       " 'accuracy': 0.12561693330938023,\n",
       " 'accuracybutin': -0.02796358128860998,\n",
       " 'accurate': 0.5054858061194991,\n",
       " 'accurateif': 0.02790342359124949,\n",
       " 'accurately': -0.013218681856770333,\n",
       " 'accuratelymaybe': -0.0026228106279445576,\n",
       " 'accursed': -0.003713794729905398,\n",
       " 'accusation': 0.15228280783554177,\n",
       " 'accusations': 0.006370467524644854,\n",
       " 'accusatory': 0.0013963234016128494,\n",
       " 'accuse': -0.04981835229884355,\n",
       " 'accused': -0.2514706440486336,\n",
       " 'accuser': -0.05955514658914152,\n",
       " 'accusers': -0.0002519629400271806,\n",
       " 'accuses': 0.041000277280847756,\n",
       " 'accusing': -0.05562649040198777,\n",
       " 'accustomed': 0.2537694968789612,\n",
       " 'acd': 0.026805093691276188,\n",
       " 'ace': 0.3721201968930026,\n",
       " 'aced': 0.00010913765482835628,\n",
       " 'acedemy': 0.09011453141515231,\n",
       " 'acedmy': 0.004877540441824846,\n",
       " 'acerbic': -0.029508318214352108,\n",
       " 'acerbity': 0.00010531565719805837,\n",
       " 'aces': -0.12013482789454132,\n",
       " 'acetylene': -0.0006086484822911544,\n",
       " 'achad': -0.03803964854272808,\n",
       " 'achala': 0.030906021730271495,\n",
       " 'acharya': -0.0756460610465913,\n",
       " 'achcha': 0.0021929325686424485,\n",
       " 'ache': 0.3159529238092403,\n",
       " 'acheaology': 9.692643608509382e-05,\n",
       " 'ached': 0.14846570961989303,\n",
       " 'acheived': 0.30340092293877613,\n",
       " 'achero': -0.04176781820184827,\n",
       " 'achievable': -0.0006192272945302554,\n",
       " 'achieve': 0.24947070201697383,\n",
       " 'achieved': 0.0946533908395115,\n",
       " 'achieveing': 0.0010195533911369727,\n",
       " 'achievement': 0.12201954788686027,\n",
       " 'achievementa': 0.0026775414853089947,\n",
       " 'achievements': -0.17792671363562187,\n",
       " 'achiever': 0.00019092778586377187,\n",
       " 'achievers': 0.01237062430995677,\n",
       " 'achieves': 0.17909192254347323,\n",
       " 'achieving': 0.11115531631683624,\n",
       " 'achile': -0.004447842711000342,\n",
       " 'achilleas': 0.019431668610073814,\n",
       " 'achilles': 0.21476295448090027,\n",
       " 'aching': -0.004554940041082661,\n",
       " 'achingly': 0.046061138707580704,\n",
       " 'achive': -0.0003964251061846144,\n",
       " 'achra': -0.0013246392639431916,\n",
       " 'achterbusch': 0.03690056532746602,\n",
       " 'acid': 0.2498928683064918,\n",
       " 'acidently': -0.0011559538864549287,\n",
       " 'acidic': 0.07316499669304316,\n",
       " 'acidity': 0.11119872962409127,\n",
       " 'acids': 0.012292366202251,\n",
       " 'acidthe': -0.00423072599924601,\n",
       " 'acin': 0.0031896544889722096,\n",
       " 'acing': -0.05788098403204969,\n",
       " 'aciton': 0.00027891513479237536,\n",
       " 'ack': -0.07383173113491007,\n",
       " 'acker': -0.00016244293717228444,\n",
       " 'ackerman': -0.14472794029040478,\n",
       " 'ackland': 0.06606237439494403,\n",
       " 'acknowledge': -0.09500836781509515,\n",
       " 'acknowledged': 0.1686763739216287,\n",
       " 'acknowledgement': 0.07716034195582093,\n",
       " 'acknowledgements': -0.04338790868651296,\n",
       " 'acknowledges': -0.07050252337430453,\n",
       " 'acknowledging': 0.008325075747044915,\n",
       " 'acknowledgment': -0.03909539328690944,\n",
       " 'acknowledgments': 0.018225085415361688,\n",
       " 'ackroyd': 0.0021287484987580906,\n",
       " 'acl': -0.0005168859519660816,\n",
       " 'aclear': 9.584104479532829e-05,\n",
       " 'aclu': -0.013529099312311745,\n",
       " 'acme': -0.03216003280810333,\n",
       " 'acmetropolis': -0.0005944767580488261,\n",
       " 'acne': -0.053679452430520455,\n",
       " 'acolyte': 0.0013543099576343567,\n",
       " 'acolytes': -0.008702794351201797,\n",
       " 'acomplication': 0.04451306955730296,\n",
       " 'acorn': 0.03411374117628545,\n",
       " 'acorns': -0.072090701425716,\n",
       " 'acosta': 0.05368125809862053,\n",
       " 'acoustic': -0.02140665098245067,\n",
       " 'acoustics': -0.025464577838738935,\n",
       " 'acp': 0.024242432673666627,\n",
       " 'acquaint': 0.005217697320041442,\n",
       " 'acquaintaces': -0.010536659010178457,\n",
       " 'acquaintance': 0.22862507620379685,\n",
       " 'acquaintances': 0.002455607004622437,\n",
       " 'acquainted': -0.12061903062121762,\n",
       " 'acquaints': 0.017527173948886955,\n",
       " 'acquart': 0.04567472058541373,\n",
       " 'acquartwho': 0.007155526999551469,\n",
       " 'acquiesce': 0.00016289528180991045,\n",
       " 'acquiescence': 0.02531515520756676,\n",
       " 'acquire': -0.33247632777619873,\n",
       " 'acquired': -0.09399006262776072,\n",
       " 'acquires': 0.049468284582597666,\n",
       " 'acquiring': -0.002974679618915248,\n",
       " 'acquisition': 0.0007706868508828324,\n",
       " 'acquit': -0.18701578621167067,\n",
       " 'acquitane': 0.00014930117007414315,\n",
       " 'acquits': -0.05844087741877204,\n",
       " 'acquittal': 0.044036422019401264,\n",
       " 'acquitted': -0.06376421539850573,\n",
       " 'acre': 0.27317107464153007,\n",
       " 'acres': 0.017627691374776554,\n",
       " 'acrid': 0.0016995005919311372,\n",
       " 'acrimonious': 0.05684568027597086,\n",
       " 'acrimony': -0.06143940804523416,\n",
       " 'acrobat': -0.008876835487186029,\n",
       " 'acrobatic': -0.20002778576679991,\n",
       " 'acrobatics': -0.0154151831179866,\n",
       " 'acronym': -8.79982928750903e-05,\n",
       " 'acronymic': -0.09564010179298195,\n",
       " 'acropolis': 0.00040338265734695766,\n",
       " 'across': -0.14677558936153545,\n",
       " 'acs': -0.04480236966610688,\n",
       " 'act': -0.10833326496179138,\n",
       " 'actally': -0.0001202226607652699,\n",
       " 'acted': 0.007567631456252384,\n",
       " 'actedblah': 0.004958747665575051,\n",
       " 'actedparticularly': 0.00591471351357168,\n",
       " 'actedsummer': -0.048195450577815595,\n",
       " 'actedthe': 0.06657547197049023,\n",
       " 'actedyou': -0.000889255099302332,\n",
       " 'actelone': 0.012347011764569094,\n",
       " 'actess': 0.006462802908880058,\n",
       " 'acteurs': -0.036963766959740456,\n",
       " 'acteurs_': 0.0095572618730638,\n",
       " 'acthe': 0.0015023721143686225,\n",
       " 'actif': 0.008767674386929315,\n",
       " 'actin': -0.0009840272308713719,\n",
       " 'acting': -0.1674190594669947,\n",
       " 'actingand': -0.0890916445035991,\n",
       " 'actingbad': -0.0008888879340783073,\n",
       " 'actingboring': -0.0032180684314313665,\n",
       " 'actingcharacters': -0.00012107908834909326,\n",
       " 'actingde': 0.0011165128301095374,\n",
       " 'actingdirectingeffectsmusic': 0.1208883183614912,\n",
       " 'actingetc': -0.00581622338851688,\n",
       " 'actingexcept': -0.09033209296252193,\n",
       " 'actingfair': -0.002290997078864525,\n",
       " 'actinggenerally': 0.00031922959781590185,\n",
       " 'actinghe': 0.001238168053472074,\n",
       " 'actingi': -0.0006523708969287247,\n",
       " 'actingin': 0.02124925088362454,\n",
       " 'actingit': 0.1424832133538251,\n",
       " 'actingjob': -0.00011276816127118158,\n",
       " 'actingmy': 0.013118162623386317,\n",
       " 'actingnicholas': 0.042329937679987065,\n",
       " 'actingnot': -0.00036987561450065964,\n",
       " 'actingon': -0.00033284001102921963,\n",
       " 'actingrightdoesn': 0.006783418160112194,\n",
       " 'actings': -0.041033767478715524,\n",
       " 'actingsadly': 0.008626453079554794,\n",
       " 'actingshould': -9.73144711485195e-05,\n",
       " 'actingsomewhat': -0.005152113474618089,\n",
       " 'actingsorry': -0.00010930350846658281,\n",
       " 'actingstupid': -0.04731244396339823,\n",
       " 'actingthat': -0.0053924328898033586,\n",
       " 'actingthe': -0.0020765161341599987,\n",
       " 'actingthere': -0.0009064696564729561,\n",
       " 'actingthor': -0.01995720383309291,\n",
       " 'actingwellhmmm': -0.00021515644973329697,\n",
       " 'actingwhich': 0.002305791754945643,\n",
       " 'actingwise': 0.00010490063418502947,\n",
       " 'actingwowit': 0.08672469875469808,\n",
       " 'actingwowthe': -0.003028459631618559,\n",
       " 'actingwriting': -0.004044519842602266,\n",
       " 'actingyet': 0.00031922959781590185,\n",
       " 'actingâ': 0.0021776792588368045,\n",
       " 'actio': 0.0015267569304181063,\n",
       " 'action': 0.23411141712985573,\n",
       " 'actiona': 0.0006915532445492045,\n",
       " 'actionand': -0.005152113474618089,\n",
       " 'actioned': -0.011697803193452485,\n",
       " 'actioneer': 0.054321786370085855,\n",
       " 'actioneers': -0.005870259702916868,\n",
       " 'actioner': -0.13414236342003966,\n",
       " 'actioners': 0.13641764544261065,\n",
       " 'actiongreat': 0.00021348644434911144,\n",
       " 'actionhajjbut': 0.0032027389009842934,\n",
       " 'actionish': -0.0001163482788559705,\n",
       " 'actionless': -0.06166572929906591,\n",
       " 'actionmaybe': -0.23490209687019933,\n",
       " 'actionmost': -0.0010458629126011592,\n",
       " 'actionmovie': 0.003900884272369251,\n",
       " 'actionpacked': 0.006823532054206085,\n",
       " 'actions': 0.23571039313255845,\n",
       " 'actionscenes': -0.07250824933721536,\n",
       " 'actionsome': 0.0007254412192564835,\n",
       " 'actionsstylesongs': 0.03129200015607057,\n",
       " 'actionstunts': 0.023668808210514838,\n",
       " 'actionthere': 0.00380481642939041,\n",
       " 'actionthis': -0.0052546221123021455,\n",
       " 'activate': -0.021617788460303725,\n",
       " 'activated': -0.1710148423214249,\n",
       " 'activates': 0.05458309317106476,\n",
       " 'activating': 0.08960522754428629,\n",
       " 'active': 0.07794210661743145,\n",
       " 'actively': -0.08017212979874414,\n",
       " 'actives': 0.09125759638438753,\n",
       " 'activest': -0.21508458610454942,\n",
       " 'activision': 0.00039537288669374963,\n",
       " 'activism': 0.17955290900345192,\n",
       " 'activist': 0.19465914463363326,\n",
       " 'activists': 0.21111159291175385,\n",
       " 'activities': 0.08159181660021972,\n",
       " 'activitiesupon': -0.029358885784261567,\n",
       " 'activity': 0.19768189963376448,\n",
       " 'activityto': -0.0007424783065504552,\n",
       " 'actor': -0.2789428387935829,\n",
       " 'actora': 0.011695204783302686,\n",
       " 'actoralso': 0.0014655284701011724,\n",
       " 'actorat': -0.0003459282694803781,\n",
       " 'actoratwill': -0.000141924914244888,\n",
       " 'actorbrigitte': 0.04652465145841403,\n",
       " 'actorbut': 0.03243563805077989,\n",
       " 'actordefinently': -0.0263776929148351,\n",
       " 'actores': 0.013365359867668567,\n",
       " 'actorgeorge': -0.0003630213151505088,\n",
       " 'actorgone': 0.00013727413748454217,\n",
       " 'actorhe': -0.0003229999260114706,\n",
       " 'actori': 0.0747743651396417,\n",
       " 'actormichael': -0.0018089234454133005,\n",
       " 'actormr': -0.023665390692049305,\n",
       " 'actorno': -0.0014611485887138238,\n",
       " 'actornobody': -0.03727578135024097,\n",
       " 'actorproducer': -0.0002458201535370726,\n",
       " 'actorrespectively': 0.00031922959781590185,\n",
       " 'actors': -0.08394865077425302,\n",
       " 'actorsa': -0.0015775015435434702,\n",
       " 'actorsadashiv': 0.004411561605854294,\n",
       " 'actorscary': 0.004815648255015155,\n",
       " 'actorscome': 0.000692628923218719,\n",
       " 'actorscott': 0.0007482542201041092,\n",
       " 'actorsegmihaela': -0.0031932208870404876,\n",
       " 'actorselijah': 0.0006350690908670369,\n",
       " 'actorsfrom': 0.0029588830488798783,\n",
       " 'actorsgunga': 0.01671026459120878,\n",
       " 'actorshahahathey': -0.004992116334063655,\n",
       " 'actorshe': -0.03320917073600202,\n",
       " 'actorsi': 0.10299315216087619,\n",
       " 'actorsie': 0.05546948885910435,\n",
       " 'actorsin': 0.009528531708649151,\n",
       " 'actorsit': -0.026903117755094495,\n",
       " 'actorsmaybe': -0.0035620219541384873,\n",
       " 'actorsno': 0.024465075281060223,\n",
       " 'actorsor': -0.01426256346720713,\n",
       " 'actorsplaying': 0.01918654669387086,\n",
       " 'actorsrealistic': 0.006481609367676303,\n",
       " 'actorssome': -0.0001883368321743875,\n",
       " 'actorstake': -0.00020191564647687032,\n",
       " 'actorstallone': 0.00010672879113201103,\n",
       " 'actorsthe': -0.2548552522893877,\n",
       " 'actorstheir': 0.004862344940588297,\n",
       " 'actorsthey': -0.0018943390455550954,\n",
       " 'actorsthough': -0.0018005165910219085,\n",
       " 'actorswelli': -0.0031932208870404876,\n",
       " 'actorthe': -0.003028459631618559,\n",
       " 'actortom': -0.0035620219541384873,\n",
       " 'actorwith': -0.01434731725295276,\n",
       " 'actra': -0.00018076282180097075,\n",
       " 'actreesess': 0.000401085686743707,\n",
       " 'actress': -0.2944471390904157,\n",
       " 'actressan': -0.029629493648548774,\n",
       " 'actressand': -0.0002412875624963722,\n",
       " 'actressbut': -0.00716598318191457,\n",
       " 'actresses': -0.10044307302225429,\n",
       " 'actressesalthough': -0.00013575170627052967,\n",
       " 'actresseslooking': -0.019756004386409953,\n",
       " 'actressesmarina': 0.033408294414833575,\n",
       " 'actressesrises': -0.0455619156176009,\n",
       " 'actressthe': 0.052048408031525425,\n",
       " 'actressâ': -0.0596168661042858,\n",
       " 'actriss': -0.00043030079970579285,\n",
       " 'acts': -0.3472430512562292,\n",
       " 'actshe': -0.0024325453627733985,\n",
       " 'actthe': 0.029621484007781133,\n",
       " 'actthey': -0.00013575170627052967,\n",
       " 'actual': -0.03163995610734151,\n",
       " 'actualities': -0.07946352641181838,\n",
       " 'actuality': -0.13346553942282946,\n",
       " 'actualization': 0.012044516742729416,\n",
       " 'actualize': 0.003776315714444868,\n",
       " 'actuall': 0.00834917554181146,\n",
       " 'actually': 0.13732822932440752,\n",
       " 'actuallyactually': -0.0007626953572841155,\n",
       " 'actuallybut': -0.1224231666739595,\n",
       " 'actuallyi': 0.07894559676848134,\n",
       " 'actuallythere': -0.0017846918087771665,\n",
       " 'actuallywasn': -0.000234769833646029,\n",
       " 'actualy': 0.02142508377382373,\n",
       " 'actuelly': 0.003942959490401148,\n",
       " 'actullly': -0.003256000980236149,\n",
       " 'acturly': -0.0015471778484558315,\n",
       " 'acual': 0.09751508413875273,\n",
       " 'acually': -0.0009921456652713196,\n",
       " 'acuity': 0.007725310575326667,\n",
       " 'acumen': -0.0026112015411372523,\n",
       " 'acupat': -0.0006885490859286708,\n",
       " 'acurately': -0.09839159676748177,\n",
       " 'acus': -0.01873371657098799,\n",
       " 'acute': 0.029135703112150243,\n",
       " 'acutely': 0.04021319589515089,\n",
       " 'ad': -0.32183033565925007,\n",
       " 'ada': 0.01237204338664001,\n",
       " 'adabted': -0.00320583562182374,\n",
       " 'adachi': -0.00012217009414727809,\n",
       " 'adage': -0.07420443978094125,\n",
       " 'adagio': 0.0022783175306588473,\n",
       " 'adair': -0.10195631886434592,\n",
       " 'adalbert': 0.0002618409672526353,\n",
       " 'adam': -0.027509375602554443,\n",
       " 'adama': 0.15265381319800508,\n",
       " 'adamafather': 0.047892167188519866,\n",
       " 'adamant': -0.019332257052233348,\n",
       " 'adamantium': -0.03646096053851941,\n",
       " 'adamantly': -0.05668370898903599,\n",
       " 'adamdanny': -0.030553464353277254,\n",
       " 'adames': 0.03099853598522307,\n",
       " 'adamos': -0.09072404365311214,\n",
       " 'adams': 0.14167181666736753,\n",
       " 'adamson': 0.12709033992788943,\n",
       " 'adapt': 0.13273508144424528,\n",
       " 'adaptable': -0.05555390619079426,\n",
       " 'adaptaion': -0.0012213556824871522,\n",
       " 'adaptation': -0.0025711311705411063,\n",
       " 'adaptations': 0.12563648143867592,\n",
       " 'adapted': 0.12450922265386362,\n",
       " 'adaptedi': 0.16877752946863173,\n",
       " 'adapter': -0.23261741367473296,\n",
       " 'adapters': 0.0027015124560060473,\n",
       " 'adapting': 0.10719118203334545,\n",
       " 'adaption': -0.07805872592684483,\n",
       " 'adaptions': -0.04101966102658282,\n",
       " 'adaptor': -0.00031766872903736824,\n",
       " 'adapts': 0.058798351666187444,\n",
       " 'adcox': -0.08176328401197079,\n",
       " 'add': -0.07880490359207944,\n",
       " 'addam': -0.001490615841322618,\n",
       " 'addams': -0.0016464562700862172,\n",
       " 'addario': -0.034172675015780266,\n",
       " 'added': -0.2591648682225252,\n",
       " 'addendum': 0.07325923429446643,\n",
       " 'adder': 0.019352923885018126,\n",
       " 'addict': 0.02134120695071763,\n",
       " 'addicted': 0.3519422021628545,\n",
       " 'addicting': 0.0009849330405918653,\n",
       " 'addiction': -0.09024569100379264,\n",
       " 'addictions': 0.05144761873904751,\n",
       " 'addictive': 0.4400168383852395,\n",
       " 'addictor': 0.10787293405026747,\n",
       " 'addicts': -0.24563113769611594,\n",
       " 'adding': -0.1512258816226883,\n",
       " 'addio': -0.013184135705971083,\n",
       " 'addison': -0.021556095476655594,\n",
       " 'addition': 0.14799289308102737,\n",
       " 'additional': -0.14809588570134805,\n",
       " 'additionally': -0.207664777135596,\n",
       " 'additions': -0.06173786659115189,\n",
       " 'additive': 0.0031765723563838793,\n",
       " 'additives': -0.11150640033876931,\n",
       " 'addled': -0.08860993833171865,\n",
       " 'addons': -0.0014952401492488675,\n",
       " 'address': 0.05090178493627875,\n",
       " 'addressed': 0.2336157854882217,\n",
       " 'addresses': 0.0321518616423293,\n",
       " 'addressing': -0.0020247687919933838,\n",
       " 'adds': 0.4306571277457378,\n",
       " 'addy': 0.11226970089222439,\n",
       " 'ade': 0.07549303459677628,\n",
       " 'adela': 0.030232976167187564,\n",
       " 'adelade': 0.0013154497920014035,\n",
       " 'adelaide': 0.11160646963786884,\n",
       " 'adele': 0.05738586196513505,\n",
       " 'adelehis': 0.004016338076733107,\n",
       " 'adelin': 0.0001024008259070041,\n",
       " 'adeline': 0.017293696053605423,\n",
       " 'adell': 0.008427109696729041,\n",
       " 'adelle': 0.08971285094907494,\n",
       " 'ademir': 0.005061309957707117,\n",
       " 'aden': 0.000659569363702675,\n",
       " 'adenine': -0.025938066982462857,\n",
       " 'adenoidal': -0.003981981032893677,\n",
       " 'adept': 0.03701188116890704,\n",
       " 'adeptly': 0.03313429907666889,\n",
       " 'adeptness': 0.0008646557068691307,\n",
       " 'adequate': -0.4623287737815852,\n",
       " 'adequateamrish': -0.021786364379380762,\n",
       " 'adequatebut': -0.018802277339098183,\n",
       " 'adequately': 0.16900667009906106,\n",
       " 'adequatenever': -0.007736082582668739,\n",
       " 'adequateâ': 0.010371539475628338,\n",
       " 'adgth': 0.004676669684794504,\n",
       " 'adhd': 0.0002798803383648941,\n",
       " 'adhdor': -0.08325473480414683,\n",
       " 'adhe': 0.004016338076733107,\n",
       " 'adhere': 0.011475786291684891,\n",
       " 'adhered': 0.00460267135691044,\n",
       " 'adherence': 0.0017040068759299335,\n",
       " 'adherent': -0.01986389883094793,\n",
       " 'adherents': -0.01899013600483506,\n",
       " 'adheres': -0.12693113080268648,\n",
       " 'adhering': 0.06148647888590822,\n",
       " 'adhesive': 0.08052227134420231,\n",
       " 'adhura': 0.015296699176988218,\n",
       " 'adi': 0.0010588079024583588,\n",
       " 'adibah': 9.051184633195631e-05,\n",
       " 'adiego': 0.0006267314688256438,\n",
       " 'adien': 0.024979627956378658,\n",
       " 'adieu': 0.005889789180854614,\n",
       " 'adios': -0.010727168229039212,\n",
       " 'aditiya': 0.03800272109489824,\n",
       " 'aditya': 0.07779176318986959,\n",
       " 'adityaakshay': 0.0001806654074406321,\n",
       " 'adj': -0.01523742913989586,\n",
       " 'adjacent': -0.040806169626007485,\n",
       " 'adjani': 0.06260361679046213,\n",
       " 'adjanitrelkovsky': 0.0006925211816098254,\n",
       " 'adjective': 0.042430325842061724,\n",
       " 'adjectives': 0.03161454205009022,\n",
       " 'adjoining': 0.0031959719339171837,\n",
       " 'adjournment': 0.0026312696705051764,\n",
       " 'adjunct': -0.00247707963329738,\n",
       " 'adjuncts': 0.00018654675973390298,\n",
       " 'adjurdubois': -0.08961132798019214,\n",
       " 'adjust': 0.07694798434121645,\n",
       " 'adjusted': 0.20253442630226207,\n",
       " 'adjuster': 0.007004821222267074,\n",
       " 'adjustin': -0.025003563295373135,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "feature_to_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('excellent', 1.3796382341255753)\n('refreshing', 1.2804007428476882)\n('perfect', 1.201765417928455)\n('superb', 1.1359048015765132)\n('appreciated', 1.1338062594644855)\n#############################\n('worst', -2.060193118611848)\n('waste', -1.9166831490720768)\n('disappointment', -1.675687288820553)\n('poorly', -1.6570778362602712)\n('awful', -1.535364979859899)\n"
     ]
    }
   ],
   "source": [
    "##### CODE #####\n",
    "for best_positive in sorted(\n",
    "                            feature_to_coef.items(), \n",
    "                            key= lambda x: x[1], \n",
    "                            reverse= True)[:5]:\n",
    "    print(best_positive)\n",
    "\n",
    "print('#############################')\n",
    "\n",
    "for best_negative in sorted(feature_to_coef.items(), key= lambda x: x[1])[:5]:\n",
    "    print(best_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}