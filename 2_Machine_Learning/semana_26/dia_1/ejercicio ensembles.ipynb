{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios ensembling\n",
    "En este ejercicio vas a realizar prediciones sobre un dataset de ciudadanos indios diabéticos. Se trata de un problema de clasificación en el que intentaremos predecir 1 (diabético) 0 (no diabético). Todas las variables son numércias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga las librerias que consideres comunes al notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lee los datos de [esta direccion](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv)\n",
    "Los nombres de columnas son:\n",
    "```Python\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv', names= names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0       6   148    72    35     0  33.6  0.627   50      1\n",
       "1       1    85    66    29     0  26.6  0.351   31      0\n",
       "2       8   183    64     0     0  23.3  0.672   32      1\n",
       "3       1    89    66    23    94  28.1  0.167   21      0\n",
       "4       0   137    40    35   168  43.1  2.288   33      1\n",
       "..    ...   ...   ...   ...   ...   ...    ...  ...    ...\n",
       "763    10   101    76    48   180  32.9  0.171   63      0\n",
       "764     2   122    70    27     0  36.8  0.340   27      0\n",
       "765     5   121    72    23   112  26.2  0.245   30      0\n",
       "766     1   126    60     0     0  30.1  0.349   47      1\n",
       "767     1    93    70    31     0  30.4  0.315   23      0\n",
       "\n",
       "[768 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preg</th>\n      <th>plas</th>\n      <th>pres</th>\n      <th>skin</th>\n      <th>test</th>\n      <th>mass</th>\n      <th>pedi</th>\n      <th>age</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>10</td>\n      <td>101</td>\n      <td>76</td>\n      <td>48</td>\n      <td>180</td>\n      <td>32.9</td>\n      <td>0.171</td>\n      <td>63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>2</td>\n      <td>122</td>\n      <td>70</td>\n      <td>27</td>\n      <td>0</td>\n      <td>36.8</td>\n      <td>0.340</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>5</td>\n      <td>121</td>\n      <td>72</td>\n      <td>23</td>\n      <td>112</td>\n      <td>26.2</td>\n      <td>0.245</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>1</td>\n      <td>126</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.1</td>\n      <td>0.349</td>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>1</td>\n      <td>93</td>\n      <td>70</td>\n      <td>31</td>\n      <td>0</td>\n      <td>30.4</td>\n      <td>0.315</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   preg    768 non-null    int64  \n 1   plas    768 non-null    int64  \n 2   pres    768 non-null    int64  \n 3   skin    768 non-null    int64  \n 4   test    768 non-null    int64  \n 5   mass    768 non-null    float64\n 6   pedi    768 non-null    float64\n 7   age     768 non-null    int64  \n 8   class   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bagging\n",
    "Para este apartado tendrás que crear un ensemble utilizando la técnica de bagging ([BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)), mediante la cual combinarás 100 [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Recuerda utilizar también [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) con 10 kfolds.\n",
    "\n",
    "**Para este apartado y siguientes, no hace falta que dividas en train/test**, por hacerlo más sencillo. Simplemente divide tus datos en features y target.\n",
    "\n",
    "Establece una semilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_shape (768, 8)\ny_shape (768,)\n"
     ]
    }
   ],
   "source": [
    "#Set X and y_target\n",
    "X = df.drop(columns='class')\n",
    "y = df['class']\n",
    "\n",
    "print (\"X_shape\", X.shape)\n",
    "print (\"y_shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "tree_cls = DecisionTreeClassifier()\n",
    "\n",
    "bagging_cls = BaggingClassifier(base_estimator=tree_cls,\n",
    "                                n_estimators=100,\n",
    "                                random_state= seed)\n",
    "\n",
    "result_bagg = cross_val_score(bagging_cls, X, y, cv= kfold ).mean()\n",
    "#result_bagg = cross_val_score(bagging_cls, X, y, cv= 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.775974025974026"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "result_bagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.7012987 , 0.77922078, 0.79220779, 0.66233766, 0.76623377,\n",
       "       0.79220779, 0.83116883, 0.85714286, 0.72368421, 0.80263158])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "result_bagg = cross_val_score(bagging_cls, X, y, cv= 10 )\n",
    "result_bagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest\n",
    "En este caso entrena un [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) con 100 árboles y un `max_features` de 3. También con validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7642857142857143"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "rand_forest_cls = RandomForestClassifier(n_estimators=100,\n",
    "                                         max_features=3,\n",
    "                                         random_state= seed)\n",
    "\n",
    "result_rand_forest_cls = cross_val_score(rand_forest_cls, X, y, cv= kfold ).mean()\n",
    "result_rand_forest_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AdaBoost\n",
    "Implementa un [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) con 30 árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.695164046479836"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "tree_cls = DecisionTreeClassifier()\n",
    "adaBoost_cls = AdaBoostClassifier(tree_cls,\n",
    "                                  n_estimators=30,\n",
    "                                  random_state= seed)\n",
    "\n",
    "result_adaB_cls = cross_val_score(adaBoost_cls, X, y, cv= kfold ).mean()\n",
    "result_adaB_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GradientBoosting\n",
    "Implementa un [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) con 100 estimadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_boost_cls = GradientBoostingClassifier(n_estimators=100,\n",
    "                                            random_state= seed)\n",
    "resuld_gradBoost_cls = cross_val_score(grad_boost_cls, \n",
    "                                        X,\n",
    "                                        y,\n",
    "                                        cv= kfold).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7642857142857143"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "resuld_gradBoost_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.71428571, 0.81818182, 0.74025974, 0.63636364, 0.80519481,\n",
       "       0.79220779, 0.80519481, 0.83116883, 0.71052632, 0.78947368])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "cross_val_score(grad_boost_cls, X, y, cv= kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XGBoost\n",
    "Para este apartado utiliza un [XGBoostClassifier](https://docs.getml.com/latest/api/getml.predictors.XGBoostClassifier.html) con 100 estimadores. XGBoost no forma parte de la suite de modelos de sklearn, por lo que tendrás que instalarlo con pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\pilar\\anaconda3\\lib\\site-packages (1.3.2)\nRequirement already satisfied: scipy in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from xgboost) (1.5.3)\nRequirement already satisfied: numpy in c:\\users\\pilar\\anaconda3\\lib\\site-packages (from xgboost) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:51:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(n_estimators= 100)\n",
    "\n",
    "resuld_xgdBoost_cls = cross_val_score(model, \n",
    "                                      X,\n",
    "                                      y,\n",
    "                                      cv= 10).mean()\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7357142857142857"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "resuld_xgdBoost_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resultados\n",
    "Crea un series con los resultados y sus algoritmos, ordenándolos de mayor a menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results_models = {'models': ['Bagging_cls','Random_forest_cls','AdaBoost_cls', 'GradientBoosting_cls', 'XGBoost_cls' ],\n",
    "                        'results':[result_bagg, result_rand_forest_cls ,result_adaB_cls, resuld_gradBoost_cls,resuld_xgdBoost_cls]}\n",
    "\n",
    "df_results = pd.DataFrame(dict_results_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 models   results\n",
       "0           Bagging_cls  0.775974\n",
       "1     Random_forest_cls  0.764286\n",
       "2          AdaBoost_cls  0.695164\n",
       "3  GradientBoosting_cls  0.764286\n",
       "4           XGBoost_cls  0.735714"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>models</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bagging_cls</td>\n      <td>0.775974</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random_forest_cls</td>\n      <td>0.764286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AdaBoost_cls</td>\n      <td>0.695164</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GradientBoosting_cls</td>\n      <td>0.764286</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGBoost_cls</td>\n      <td>0.735714</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.sort_values('results', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 models   results\n",
       "0           Bagging_cls  0.775974\n",
       "1     Random_forest_cls  0.764286\n",
       "3  GradientBoosting_cls  0.764286\n",
       "4           XGBoost_cls  0.735714\n",
       "2          AdaBoost_cls  0.695164"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>models</th>\n      <th>results</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bagging_cls</td>\n      <td>0.775974</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random_forest_cls</td>\n      <td>0.764286</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GradientBoosting_cls</td>\n      <td>0.764286</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGBoost_cls</td>\n      <td>0.735714</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AdaBoost_cls</td>\n      <td>0.695164</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}